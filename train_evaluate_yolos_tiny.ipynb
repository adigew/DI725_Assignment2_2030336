{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d7be84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'annotations'])\n",
      "{'image_name': 'frame_20190829091111_x_0001973.jpg', 'image_width:': 1920.0, 'image_height': 1080.0, 'platform': 'Parrot Bebop 2', 'time': {'year': 2019, 'month': 8, 'day': 29, 'hour': 9, 'min': 11, 'sec': 11, 'ms': 394400.0}, 'longtitude': 10.18798203255313, 'latitude': 56.20630134795274, 'altitude': 19921.6, 'linear_x': 0.03130074199289083, 'linear_y': 0.028357808757573367, 'linear_z': 0.0744575835764408, 'angle_phi': -0.06713105738162994, 'angle_theta': 0.06894744634628296, 'angle_psi': 1.1161083340644837, 'bbox': [{'top': 163, 'left': 1098, 'height': 185, 'width': 420, 'class': 1}, {'top': 421, 'left': 1128, 'height': 176, 'width': 393, 'class': 1}, {'top': 927, 'left': 1703, 'height': 153, 'width': 183, 'class': 0}]}\n"
     ]
    }
   ],
   "source": [
    "print(annotations.keys())  # Check top-level keys\n",
    "print(annotations['annotations'][0])  # Check the first annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba437b7",
   "metadata": {},
   "source": [
    " YOLOS-Tiny Object Detection Script (Train + Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c3c219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:f6akk5st) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-train</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/f6akk5st' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/f6akk5st</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250417_163349-f6akk5st\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:f6akk5st). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250417_164215-labp4rbw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/labp4rbw' target=\"_blank\">yolos-tiny-train</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/labp4rbw' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/labp4rbw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6565 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 301\u001b[0m\n\u001b[0;32m    298\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(val_annotations, f)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Run inference on validation set\u001b[39;00m\n\u001b[0;32m    304\u001b[0m pred_json \u001b[38;5;241m=\u001b[39m run_yolos_inference(model, val_dataset, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolos_pred.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[18], line 168\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    165\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    166\u001b[0m labels \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    170\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\yolos\\modeling_yolos.py:869\u001b[0m, in \u001b[0;36mYolosForObjectDetection.forward\u001b[1;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    867\u001b[0m         outputs_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_labels_classifier(intermediate)\n\u001b[0;32m    868\u001b[0m         outputs_coord \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_predictor(intermediate)\u001b[38;5;241m.\u001b[39msigmoid()\n\u001b[1;32m--> 869\u001b[0m     loss, loss_dict, auxiliary_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_coord\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m auxiliary_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:552\u001b[0m, in \u001b[0;36mForObjectDetectionLoss\u001b[1;34m(logits, labels, device, pred_boxes, config, outputs_class, outputs_coord, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m     auxiliary_outputs \u001b[38;5;241m=\u001b[39m _set_aux_loss(outputs_class, outputs_coord)\n\u001b[0;32m    550\u001b[0m     outputs_loss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauxiliary_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auxiliary_outputs\n\u001b[1;32m--> 552\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# Fourth: compute total loss, as a weighted sum of the various losses\u001b[39;00m\n\u001b[0;32m    554\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_ce\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_bbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mbbox_loss_coefficient}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:253\u001b[0m, in \u001b[0;36mImageLoss.forward\u001b[1;34m(self, outputs, targets)\u001b[0m\n\u001b[0;32m    250\u001b[0m outputs_without_aux \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauxiliary_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Retrieve the matching between the outputs of the last layer and the targets\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_without_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Compute the average number of target boxes across all nodes, for normalization purposes\u001b[39;00m\n\u001b[0;32m    256\u001b[0m num_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:341\u001b[0m, in \u001b[0;36mHungarianMatcher.forward\u001b[1;34m(self, outputs, targets)\u001b[0m\n\u001b[0;32m    338\u001b[0m out_bbox \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size * num_queries, 4]\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# Also concat the target labels and boxes\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m target_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[0;32m    342\u001b[0m target_bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Compute the classification cost. Contrary to the loss, we don't use the NLL,\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# but approximate it in 1 - proba[target class].\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# The 1 is a constant that doesn't change the matching, it can be ommitted.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class_labels'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "\n",
    "# 🔧 Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 🟣 Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-train\")\n",
    "\n",
    "# ⚙️ Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model.to(device)\n",
    "\n",
    "# ⚙️ Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"train\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            # COCO-style bbox: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to [x_min, y_min, x_max, y_max] for YOLOS\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Convert to tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image and annotations\n",
    "        # Include \"area\" and \"iscrowd\" for COCO compatibility\n",
    "        processor_annotations = [\n",
    "            {\n",
    "                \"bbox\": [x, y, w, h],  # COCO format [x, y, width, height]\n",
    "                \"category_id\": l,\n",
    "                \"area\": float(w * h),  # Compute area\n",
    "                \"iscrowd\": 0  # Default to 0 (no crowd)\n",
    "            }\n",
    "            for (x, y, w, h), l in zip((ann[\"bbox\"] for ann in anns), labels)\n",
    "        ]\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": processor_annotations},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)  # Remove batch dimension\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "# 📂 Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}  # Map image_id to image metadata\n",
    "coco_annotations = []  # COCO-style annotations\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1  # Assign unique image_id (1-based indexing)\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    # Convert bbox list to COCO-style annotations\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        coco_annotations.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": bbox[\"class\"],\n",
    "            \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "            \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "            \"id\": len(coco_annotations) + 1,  # Unique annotation ID\n",
    "        })\n",
    "\n",
    "# Create pseudo-COCO structure\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(raw_annotations[\"categories\"])],\n",
    "}\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:train_size]\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "train_images = [img for img in annotations[\"images\"] if img[\"id\"] in train_ids]\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "train_annotations = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "train_dataset = AUAIRDataset(train_annotations, img_dir, processor, split=\"train\")\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "# 🧠 Training Loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "            labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": avg_train_loss})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "                labels = [item[0][\"labels\"] for item in batch]\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"val_loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "    processor.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "\n",
    "# 🧠 Inference\n",
    "def run_yolos_inference(model, dataset, output_path=\"yolos_pred.json\", log_images=False):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        inputs, image, img_id, image_name = dataset[idx]\n",
    "        if inputs is None:\n",
    "            continue\n",
    "        inputs = {k: v.unsqueeze(0).to(device) for k, v in inputs.items() if k == \"pixel_values\"}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        width, height = image.size\n",
    "        target_sizes = torch.tensor([[height, width]]).to(device)\n",
    "        \n",
    "        result = processor.post_process_object_detection(\n",
    "            outputs,\n",
    "            target_sizes=target_sizes,\n",
    "            threshold=0.5\n",
    "        )[0]\n",
    "\n",
    "        if log_images and idx % 50 == 0 and len(result[\"boxes\"]) > 0:\n",
    "            boxes = result[\"boxes\"].cpu().tolist()\n",
    "            scores = result[\"scores\"].cpu().tolist()\n",
    "            labels = result[\"labels\"].cpu().tolist()\n",
    "\n",
    "            wandb.log({\n",
    "                \"prediction\": wandb.Image(image, boxes={\n",
    "                    \"predictions\": {\n",
    "                        \"box_data\": [\n",
    "                            {\n",
    "                                \"position\": {\n",
    "                                    \"minX\": b[0] / width,\n",
    "                                    \"minY\": b[1] / height,\n",
    "                                    \"maxX\": b[2] / width,\n",
    "                                    \"maxY\": b[3] / height,\n",
    "                                },\n",
    "                                \"score\": s,\n",
    "                                \"class_id\": l\n",
    "                            }\n",
    "                            for b, s, l in zip(boxes, scores, labels)\n",
    "                        ],\n",
    "                        \"class_labels\": {i: name for i, name in enumerate(raw_annotations[\"categories\"])}\n",
    "                    }\n",
    "                }),\n",
    "                \"step\": idx\n",
    "            })\n",
    "\n",
    "        for box, label, score in zip(result[\"boxes\"], result[\"labels\"], result[\"scores\"]):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": int(label),\n",
    "                \"bbox\": [float(xmin), float(ymin), float(xmax - xmin), float(ymax - ymin)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    return output_path\n",
    "\n",
    "# 📊 mAP Evaluation\n",
    "def evaluate_map(gt_path, pred_path):\n",
    "    coco_gt = COCO(gt_path)\n",
    "    coco_dt = coco_gt.loadRes(pred_path)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"mAP@[0.5:0.95]\": coco_eval.stats[0],\n",
    "        \"AP50\": coco_eval.stats[1],\n",
    "        \"AP75\": coco_eval.stats[2],\n",
    "        \"AP_small\": coco_eval.stats[3],\n",
    "        \"AP_medium\": coco_eval.stats[4],\n",
    "        \"AP_large\": coco_eval.stats[5]\n",
    "    }\n",
    "\n",
    "    precisions = coco_eval.eval[\"precision\"]\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    categories = coco_gt.loadCats(cat_ids)\n",
    "\n",
    "    print(\"\\n📊 Per-category AP (IoU=0.50:0.95):\")\n",
    "    for idx, cat in enumerate(categories):\n",
    "        precision = precisions[:, :, idx, 0, 0]\n",
    "        precision = precision[precision > -1]\n",
    "        ap = precision.mean() if precision.size > 0 else float(\"nan\")\n",
    "        metrics[f\"AP_{cat['name']}\"] = ap\n",
    "        print(f\"  {cat['name']:20s}: {ap:.4f}\")\n",
    "\n",
    "    wandb.log(metrics)\n",
    "    print(\"✅ mAP + per-class AP metrics logged to W&B.\")\n",
    "    return metrics\n",
    "\n",
    "# 🧪 Run Training and Evaluation\n",
    "# Save ground truth annotations to gt.json\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5)\n",
    "\n",
    "# Run inference on validation set\n",
    "pred_json = run_yolos_inference(model, val_dataset, output_path=\"yolos_pred.json\", log_images=True)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_map(\"gt.json\", pred_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb73cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39a8104f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fw8xmppq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-train</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fw8xmppq' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fw8xmppq</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250417_164957-fw8xmppq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fw8xmppq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250417_165554-fynpym4j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fynpym4j' target=\"_blank\">yolos-tiny-train</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fynpym4j' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fynpym4j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6565/6565 [58:53<00:00,  1.86it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [10:40<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 13666.2197, Val Loss: 13905.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6565/6565 [57:54<00:00,  1.89it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [10:39<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 13660.2704, Val Loss: 13905.3763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6565/6565 [58:05<00:00,  1.88it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [10:37<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 13667.5493, Val Loss: 13905.3745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6565/6565 [58:17<00:00,  1.88it/s] \n",
      "Validation: 100%|██████████| 1642/1642 [10:51<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 13667.5583, Val Loss: 13905.3575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6565/6565 [58:00<00:00,  1.89it/s] \n",
      "Validation: 100%|██████████| 1642/1642 [10:18<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 13661.5152, Val Loss: 13905.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6565/6565 [57:31<00:00,  1.90it/s]  \n",
      "Validation: 100%|██████████| 1642/1642 [09:04<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 13672.2140, Val Loss: 13905.3667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 11/6565 [00:05<51:31,  2.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 217\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    215\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(val_annotations, f)\n\u001b[1;32m--> 217\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 180\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    177\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    178\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 180\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m    183\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_train_loss})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 🔧 Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 🟣 Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-train\")\n",
    "\n",
    "# ⚙️ Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model.to(device)\n",
    "\n",
    "# ⚙️ Category ID mapping (AUAIR to COCO)\n",
    "label_map = {0: 1, 1: 3, 2: 8, 3: 7, 4: 4, 5: 2, 6: 6, 7: 10}  # AUAIR to COCO\n",
    "\n",
    "# ⚙️ Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"train\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            # COCO-style bbox: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to [x_min, y_min, x_max, y_max] for YOLOS\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(label_map[ann[\"category_id\"]])  # Map AUAIR to COCO IDs\n",
    "\n",
    "        # Convert to tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image and annotations\n",
    "        # Include \"area\" and \"iscrowd\" for COCO compatibility\n",
    "        processor_annotations = [\n",
    "            {\n",
    "                \"bbox\": [x, y, w, h],  # COCO format [x, y, width, height]\n",
    "                \"category_id\": label_map[l],  # Map AUAIR to COCO IDs\n",
    "                \"area\": float(w * h),  # Compute area\n",
    "                \"iscrowd\": 0  # Default to 0 (no crowd)\n",
    "            }\n",
    "            for (x, y, w, h), l in zip((ann[\"bbox\"] for ann in anns), [ann[\"category_id\"] for ann in anns])\n",
    "        ]\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": processor_annotations},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)  # Remove batch dimension\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "# 📂 Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}  # Map image_id to image metadata\n",
    "coco_annotations = []  # COCO-style annotations\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1  # Assign unique image_id (1-based indexing)\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    # Convert bbox list to COCO-style annotations\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        coco_annotations.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": bbox[\"class\"],\n",
    "            \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "            \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "            \"id\": len(coco_annotations) + 1,  # Unique annotation ID\n",
    "        })\n",
    "\n",
    "# Create pseudo-COCO structure\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(raw_annotations[\"categories\"])],\n",
    "}\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:train_size]\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "train_images = [img for img in annotations[\"images\"] if img[\"id\"] in train_ids]\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "train_annotations = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "train_dataset = AUAIRDataset(train_annotations, img_dir, processor, split=\"train\")\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "# 🧠 Training Loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "            labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "            # Move all tensors in labels to the correct device\n",
    "            labels = [\n",
    "                {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                for label in labels\n",
    "            ]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": avg_train_loss})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "                labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "                # Move all tensors in labels to the correct device\n",
    "                labels = [\n",
    "                    {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                    for label in labels\n",
    "                ]\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"val_loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "    processor.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "\n",
    "# 🧪 Run Training\n",
    "# Save ground truth annotations for evaluation\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb7d2b",
   "metadata": {},
   "source": [
    "Fixed Classifier Head Adjustment:\n",
    "Replaced model.config.d_model with model.config.hidden_size to get the correct input dimension (768 for yolos-tiny).\n",
    "Set class_labels_classifier to a torch.nn.Linear layer with hidden_size input and num_classes (9) output.\n",
    "Added Gradient Clipping:\n",
    "Added torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) before optimizer.step() to stabilize training and address the high loss (~13,660) observed previously.\n",
    "Preserved Improvements:\n",
    "Kept bounding box normalization ([x_min, y_min, x_max, y_max] in [0, 1]).\n",
    "Retained loss component logging (loss_ce, loss_bbox, loss_giou).\n",
    "Maintained debugging for category IDs and sample counts.\n",
    "Kept lr=1e-4 for faster convergence.\n",
    "Category IDs:\n",
    "Used AUAIR IDs (0–7) directly, as the classifier head is adjusted to match AUAIR categories (0–7 for classes, 8 for background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01253cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:u9jo9xgg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-train</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/u9jo9xgg' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/u9jo9xgg</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250419_162537-u9jo9xgg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:u9jo9xgg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250419_162654-84um06jj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/84um06jj' target=\"_blank\">yolos-tiny-train</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/84um06jj' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/84um06jj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model id2label: {0: 'N/A', 1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 12: 'N/A', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 26: 'N/A', 27: 'backpack', 28: 'umbrella', 29: 'N/A', 30: 'N/A', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 45: 'N/A', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 66: 'N/A', 67: 'dining table', 68: 'N/A', 69: 'N/A', 70: 'toilet', 71: 'N/A', 72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 83: 'N/A', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n",
      "AUAIR categories: ['Human', 'Car', 'Truck', 'Van', 'Motorbike', 'Bicycle', 'Bus', 'Trailer']\n",
      "Unique category IDs in raw annotations: {0, 1, 2, 3, 4, 5, 6, 7}\n",
      "invalid_category_count: 0\n",
      "Invalid category_id 10 in image_id 29151, annotation: {'image_id': 29151, 'category_id': 10, 'bbox': [34, 96, 44, 29], 'area': 1276, 'id': 111714}\n",
      "Invalid category_id 10 in image_id 29151, annotation: {'image_id': 29151, 'category_id': 10, 'bbox': [101, 94, 30, 25], 'area': 750, 'id': 111715}\n",
      "Invalid category_id 10 in image_id 29151, annotation: {'image_id': 29151, 'category_id': 10, 'bbox': [77, 100, 20, 21], 'area': 420, 'id': 111716}\n",
      "Invalid category_id 10 in image_id 29151, annotation: {'image_id': 29151, 'category_id': 10, 'bbox': [136, 92, 32, 24], 'area': 768, 'id': 111717}\n",
      "Invalid category_id 10 in image_id 29151, annotation: {'image_id': 29151, 'category_id': 10, 'bbox': [213, 84, 43, 30], 'area': 1290, 'id': 111718}\n",
      "Invalid category_id 8 in image_id 9245, annotation: {'image_id': 9245, 'category_id': 8, 'bbox': [635, 545, 513, 122], 'area': 62586, 'id': 26383}\n",
      "Invalid category_id 8 in image_id 5480, annotation: {'image_id': 5480, 'category_id': 8, 'bbox': [706, 433, 88, 185], 'area': 16280, 'id': 14869}\n",
      "Invalid category_id 8 in image_id 13942, annotation: {'image_id': 13942, 'category_id': 8, 'bbox': [396, 502, 145, 151], 'area': 21895, 'id': 43177}\n",
      "Invalid category_id 8 in image_id 13942, annotation: {'image_id': 13942, 'category_id': 8, 'bbox': [541, 543, 500, 169], 'area': 84500, 'id': 43178}\n",
      "Invalid category_id 8 in image_id 15835, annotation: {'image_id': 15835, 'category_id': 8, 'bbox': [1229, 379, 454, 192], 'area': 87168, 'id': 50344}\n",
      "Invalid category_id 8 in image_id 4023, annotation: {'image_id': 4023, 'category_id': 8, 'bbox': [609, 528, 879, 549], 'area': 482571, 'id': 10534}\n",
      "Invalid category_id 8 in image_id 6884, annotation: {'image_id': 6884, 'category_id': 8, 'bbox': [729, 452, 60, 99], 'area': 5940, 'id': 19152}\n",
      "Invalid category_id 8 in image_id 6884, annotation: {'image_id': 6884, 'category_id': 8, 'bbox': [791, 451, 48, 82], 'area': 3936, 'id': 19153}\n",
      "Invalid category_id 8 in image_id 26600, annotation: {'image_id': 26600, 'category_id': 8, 'bbox': [1044, 369, 148, 121], 'area': 17908, 'id': 98346}\n",
      "Invalid category_id 10 in image_id 3300, annotation: {'image_id': 3300, 'category_id': 10, 'bbox': [251, 378, 432, 112], 'area': 48384, 'id': 8359}\n",
      "Invalid category_id 8 in image_id 13069, annotation: {'image_id': 13069, 'category_id': 8, 'bbox': [1716, 58, 165, 185], 'area': 30525, 'id': 40048}\n",
      "Invalid category_id 8 in image_id 3985, annotation: {'image_id': 3985, 'category_id': 8, 'bbox': [1355, 584, 197, 118], 'area': 23246, 'id': 10417}\n",
      "Invalid category_id 8 in image_id 6093, annotation: {'image_id': 6093, 'category_id': 8, 'bbox': [953, 333, 250, 70], 'area': 17500, 'id': 16788}\n",
      "Invalid category_id 10 in image_id 25206, annotation: {'image_id': 25206, 'category_id': 10, 'bbox': [1385, 342, 200, 136], 'area': 27200, 'id': 90129}\n",
      "Invalid category_id 8 in image_id 27770, annotation: {'image_id': 27770, 'category_id': 8, 'bbox': [117, 658, 578, 422], 'area': 243916, 'id': 104493}\n",
      "Invalid category_id 10 in image_id 28611, annotation: {'image_id': 28611, 'category_id': 10, 'bbox': [989, 277, 160, 69], 'area': 11040, 'id': 108918}\n",
      "Invalid category_id 8 in image_id 24564, annotation: {'image_id': 24564, 'category_id': 8, 'bbox': [767, 452, 307, 226], 'area': 69382, 'id': 87414}\n",
      "Invalid category_id 8 in image_id 21604, annotation: {'image_id': 21604, 'category_id': 8, 'bbox': [1321, 175, 155, 216], 'area': 33480, 'id': 70399}\n",
      "Invalid category_id 8 in image_id 27017, annotation: {'image_id': 27017, 'category_id': 8, 'bbox': [498, 679, 175, 172], 'area': 30100, 'id': 100562}\n",
      "Invalid category_id 8 in image_id 9849, annotation: {'image_id': 9849, 'category_id': 8, 'bbox': [1083, 393, 103, 167], 'area': 17201, 'id': 28436}\n",
      "Invalid category_id 8 in image_id 21611, annotation: {'image_id': 21611, 'category_id': 8, 'bbox': [1671, 3, 31, 41], 'area': 1271, 'id': 70461}\n",
      "Invalid category_id 8 in image_id 16801, annotation: {'image_id': 16801, 'category_id': 8, 'bbox': [1329, 239, 231, 91], 'area': 21021, 'id': 53688}\n",
      "Invalid category_id 8 in image_id 19156, annotation: {'image_id': 19156, 'category_id': 8, 'bbox': [1017, 195, 219, 90], 'area': 19710, 'id': 61384}\n",
      "Invalid category_id 8 in image_id 19156, annotation: {'image_id': 19156, 'category_id': 8, 'bbox': [458, 364, 240, 308], 'area': 73920, 'id': 61386}\n",
      "Invalid category_id 8 in image_id 9468, annotation: {'image_id': 9468, 'category_id': 8, 'bbox': [1067, 233, 113, 79], 'area': 8927, 'id': 27076}\n",
      "Invalid category_id 8 in image_id 18857, annotation: {'image_id': 18857, 'category_id': 8, 'bbox': [672, 623, 235, 249], 'area': 58515, 'id': 60287}\n",
      "Invalid category_id 8 in image_id 31746, annotation: {'image_id': 31746, 'category_id': 8, 'bbox': [321, 281, 417, 81], 'area': 33777, 'id': 126211}\n",
      "Invalid category_id 8 in image_id 12487, annotation: {'image_id': 12487, 'category_id': 8, 'bbox': [229, 258, 555, 211], 'area': 117105, 'id': 37829}\n",
      "Invalid category_id 8 in image_id 22034, annotation: {'image_id': 22034, 'category_id': 8, 'bbox': [1384, 10, 18, 51], 'area': 918, 'id': 73025}\n",
      "Invalid category_id 8 in image_id 17017, annotation: {'image_id': 17017, 'category_id': 8, 'bbox': [694, 508, 306, 178], 'area': 54468, 'id': 54455}\n",
      "Invalid category_id 8 in image_id 18661, annotation: {'image_id': 18661, 'category_id': 8, 'bbox': [904, 536, 234, 156], 'area': 36504, 'id': 59607}\n",
      "Invalid category_id 8 in image_id 18661, annotation: {'image_id': 18661, 'category_id': 8, 'bbox': [959, 324, 109, 48], 'area': 5232, 'id': 59608}\n",
      "Invalid category_id 10 in image_id 7020, annotation: {'image_id': 7020, 'category_id': 10, 'bbox': [851, 412, 463, 206], 'area': 95378, 'id': 19535}\n",
      "Invalid category_id 8 in image_id 9225, annotation: {'image_id': 9225, 'category_id': 8, 'bbox': [732, 248, 209, 122], 'area': 25498, 'id': 26324}\n",
      "Invalid category_id 8 in image_id 31553, annotation: {'image_id': 31553, 'category_id': 8, 'bbox': [1144, 317, 119, 82], 'area': 9758, 'id': 125048}\n",
      "Invalid category_id 10 in image_id 31553, annotation: {'image_id': 31553, 'category_id': 10, 'bbox': [1263, 346, 143, 99], 'area': 14157, 'id': 125049}\n",
      "Invalid category_id 8 in image_id 11501, annotation: {'image_id': 11501, 'category_id': 8, 'bbox': [671, 242, 200, 114], 'area': 22800, 'id': 34403}\n",
      "Invalid category_id 8 in image_id 25442, annotation: {'image_id': 25442, 'category_id': 8, 'bbox': [1112, 314, 219, 292], 'area': 63948, 'id': 91494}\n",
      "Invalid category_id 8 in image_id 25442, annotation: {'image_id': 25442, 'category_id': 8, 'bbox': [462, 170, 203, 185], 'area': 37555, 'id': 91495}\n",
      "Invalid category_id 8 in image_id 5153, annotation: {'image_id': 5153, 'category_id': 8, 'bbox': [968, 326, 318, 86], 'area': 27348, 'id': 13936}\n",
      "Invalid category_id 8 in image_id 31385, annotation: {'image_id': 31385, 'category_id': 8, 'bbox': [1288, 101, 585, 344], 'area': 201240, 'id': 124200}\n",
      "Invalid category_id 8 in image_id 5615, annotation: {'image_id': 5615, 'category_id': 8, 'bbox': [1200, 429, 117, 150], 'area': 17550, 'id': 15312}\n",
      "Invalid category_id 8 in image_id 18339, annotation: {'image_id': 18339, 'category_id': 8, 'bbox': [1139, 195, 240, 111], 'area': 26640, 'id': 58583}\n",
      "Invalid category_id 8 in image_id 13163, annotation: {'image_id': 13163, 'category_id': 8, 'bbox': [954, 467, 326, 221], 'area': 72046, 'id': 40467}\n",
      "Invalid category_id 8 in image_id 14409, annotation: {'image_id': 14409, 'category_id': 8, 'bbox': [787, 616, 212, 127], 'area': 26924, 'id': 44896}\n",
      "Invalid category_id 8 in image_id 12827, annotation: {'image_id': 12827, 'category_id': 8, 'bbox': [613, 512, 157, 82], 'area': 12874, 'id': 39063}\n",
      "Invalid category_id 10 in image_id 26684, annotation: {'image_id': 26684, 'category_id': 10, 'bbox': [327, 353, 130, 139], 'area': 18070, 'id': 98778}\n",
      "Invalid category_id 8 in image_id 26684, annotation: {'image_id': 26684, 'category_id': 8, 'bbox': [433, 381, 102, 68], 'area': 6936, 'id': 98779}\n",
      "Invalid category_id 8 in image_id 13248, annotation: {'image_id': 13248, 'category_id': 8, 'bbox': [638, 566, 247, 279], 'area': 68913, 'id': 40765}\n",
      "Invalid category_id 8 in image_id 14708, annotation: {'image_id': 14708, 'category_id': 8, 'bbox': [210, 542, 665, 414], 'area': 275310, 'id': 45996}\n",
      "Invalid category_id 10 in image_id 12451, annotation: {'image_id': 12451, 'category_id': 10, 'bbox': [1541, 3, 379, 162], 'area': 61398, 'id': 37676}\n",
      "Invalid category_id 8 in image_id 28924, annotation: {'image_id': 28924, 'category_id': 8, 'bbox': [0, 315, 118, 119], 'area': 14042, 'id': 110540}\n",
      "Invalid category_id 8 in image_id 28924, annotation: {'image_id': 28924, 'category_id': 8, 'bbox': [1181, 186, 96, 72], 'area': 6912, 'id': 110541}\n",
      "Invalid category_id 10 in image_id 18587, annotation: {'image_id': 18587, 'category_id': 10, 'bbox': [60, 672, 362, 228], 'area': 82536, 'id': 59358}\n",
      "Invalid category_id 10 in image_id 18587, annotation: {'image_id': 18587, 'category_id': 10, 'bbox': [568, 575, 156, 107], 'area': 16692, 'id': 59359}\n",
      "Invalid category_id 8 in image_id 18587, annotation: {'image_id': 18587, 'category_id': 8, 'bbox': [748, 214, 131, 119], 'area': 15589, 'id': 59363}\n",
      "Invalid category_id 8 in image_id 7786, annotation: {'image_id': 7786, 'category_id': 8, 'bbox': [420, 499, 369, 210], 'area': 77490, 'id': 21676}\n",
      "Invalid category_id 8 in image_id 15861, annotation: {'image_id': 15861, 'category_id': 8, 'bbox': [913, 478, 385, 147], 'area': 56595, 'id': 50455}\n",
      "Invalid category_id 8 in image_id 21232, annotation: {'image_id': 21232, 'category_id': 8, 'bbox': [496, 516, 568, 207], 'area': 117576, 'id': 68201}\n",
      "Invalid category_id 8 in image_id 15030, annotation: {'image_id': 15030, 'category_id': 8, 'bbox': [588, 277, 67, 88], 'area': 5896, 'id': 47035}\n",
      "Invalid category_id 10 in image_id 23383, annotation: {'image_id': 23383, 'category_id': 10, 'bbox': [1455, 299, 146, 128], 'area': 18688, 'id': 81202}\n",
      "Invalid category_id 10 in image_id 23383, annotation: {'image_id': 23383, 'category_id': 10, 'bbox': [112, 506, 577, 200], 'area': 115400, 'id': 81203}\n",
      "Invalid category_id 8 in image_id 23269, annotation: {'image_id': 23269, 'category_id': 8, 'bbox': [1063, 506, 175, 113], 'area': 19775, 'id': 80577}\n",
      "Invalid category_id 8 in image_id 8181, annotation: {'image_id': 8181, 'category_id': 8, 'bbox': [1274, 559, 375, 320], 'area': 120000, 'id': 22787}\n",
      "Invalid category_id 8 in image_id 9924, annotation: {'image_id': 9924, 'category_id': 8, 'bbox': [1480, 78, 47, 46], 'area': 2162, 'id': 28800}\n",
      "Invalid category_id 8 in image_id 29268, annotation: {'image_id': 29268, 'category_id': 8, 'bbox': [171, 433, 255, 244], 'area': 62220, 'id': 112336}\n",
      "Invalid category_id 8 in image_id 6087, annotation: {'image_id': 6087, 'category_id': 8, 'bbox': [198, 383, 281, 163], 'area': 45803, 'id': 16772}\n",
      "Invalid category_id 8 in image_id 8564, annotation: {'image_id': 8564, 'category_id': 8, 'bbox': [1295, 269, 183, 294], 'area': 53802, 'id': 23957}\n",
      "Invalid category_id 8 in image_id 2988, annotation: {'image_id': 2988, 'category_id': 8, 'bbox': [1341, 373, 158, 93], 'area': 14694, 'id': 7475}\n",
      "Invalid category_id 8 in image_id 9398, annotation: {'image_id': 9398, 'category_id': 8, 'bbox': [711, 491, 581, 230], 'area': 133630, 'id': 26873}\n",
      "Invalid category_id 8 in image_id 31615, annotation: {'image_id': 31615, 'category_id': 8, 'bbox': [1721, 683, 197, 111], 'area': 21867, 'id': 125427}\n",
      "Invalid category_id 8 in image_id 13902, annotation: {'image_id': 13902, 'category_id': 8, 'bbox': [1102, 236, 161, 86], 'area': 13846, 'id': 43002}\n",
      "Invalid category_id 8 in image_id 13902, annotation: {'image_id': 13902, 'category_id': 8, 'bbox': [1654, 162, 68, 75], 'area': 5100, 'id': 43003}\n",
      "Invalid category_id 8 in image_id 13902, annotation: {'image_id': 13902, 'category_id': 8, 'bbox': [1872, 88, 45, 94], 'area': 4230, 'id': 43004}\n",
      "Invalid category_id 8 in image_id 30458, annotation: {'image_id': 30458, 'category_id': 8, 'bbox': [1100, 422, 45, 34], 'area': 1530, 'id': 119159}\n",
      "Invalid category_id 8 in image_id 10847, annotation: {'image_id': 10847, 'category_id': 8, 'bbox': [266, 544, 337, 165], 'area': 55605, 'id': 32024}\n",
      "Invalid category_id 8 in image_id 10847, annotation: {'image_id': 10847, 'category_id': 8, 'bbox': [0, 440, 204, 118], 'area': 24072, 'id': 32025}\n",
      "Invalid category_id 8 in image_id 31879, annotation: {'image_id': 31879, 'category_id': 8, 'bbox': [1270, 574, 200, 104], 'area': 20800, 'id': 126919}\n",
      "Invalid category_id 8 in image_id 12333, annotation: {'image_id': 12333, 'category_id': 8, 'bbox': [852, 167, 141, 163], 'area': 22983, 'id': 37228}\n",
      "Invalid category_id 8 in image_id 12333, annotation: {'image_id': 12333, 'category_id': 8, 'bbox': [951, 455, 210, 128], 'area': 26880, 'id': 37229}\n",
      "Invalid category_id 8 in image_id 12333, annotation: {'image_id': 12333, 'category_id': 8, 'bbox': [1053, 763, 215, 154], 'area': 33110, 'id': 37230}\n",
      "Invalid category_id 8 in image_id 28976, annotation: {'image_id': 28976, 'category_id': 8, 'bbox': [494, 559, 137, 105], 'area': 14385, 'id': 110824}\n",
      "Invalid category_id 10 in image_id 28976, annotation: {'image_id': 28976, 'category_id': 10, 'bbox': [330, 447, 243, 157], 'area': 38151, 'id': 110825}\n",
      "Invalid category_id 10 in image_id 30541, annotation: {'image_id': 30541, 'category_id': 10, 'bbox': [1388, 269, 161, 111], 'area': 17871, 'id': 119654}\n",
      "Invalid category_id 8 in image_id 21851, annotation: {'image_id': 21851, 'category_id': 8, 'bbox': [0, 281, 176, 143], 'area': 25168, 'id': 71862}\n",
      "Invalid category_id 8 in image_id 29259, annotation: {'image_id': 29259, 'category_id': 8, 'bbox': [788, 613, 400, 198], 'area': 79200, 'id': 112276}\n",
      "Invalid category_id 8 in image_id 14188, annotation: {'image_id': 14188, 'category_id': 8, 'bbox': [286, 674, 403, 235], 'area': 94705, 'id': 44126}\n",
      "Invalid category_id 8 in image_id 14188, annotation: {'image_id': 14188, 'category_id': 8, 'bbox': [906, 209, 346, 157], 'area': 54322, 'id': 44127}\n",
      "Invalid category_id 8 in image_id 21781, annotation: {'image_id': 21781, 'category_id': 8, 'bbox': [13, 542, 499, 324], 'area': 161676, 'id': 71453}\n",
      "Invalid category_id 8 in image_id 19641, annotation: {'image_id': 19641, 'category_id': 8, 'bbox': [1189, 148, 201, 66], 'area': 13266, 'id': 63041}\n",
      "Invalid category_id 10 in image_id 14206, annotation: {'image_id': 14206, 'category_id': 10, 'bbox': [8, 433, 275, 146], 'area': 40150, 'id': 44207}\n",
      "Invalid category_id 8 in image_id 11520, annotation: {'image_id': 11520, 'category_id': 8, 'bbox': [1261, 430, 645, 504], 'area': 325080, 'id': 34453}\n",
      "Invalid category_id 8 in image_id 30580, annotation: {'image_id': 30580, 'category_id': 8, 'bbox': [1653, 219, 267, 241], 'area': 64347, 'id': 119904}\n",
      "Invalid category_id 8 in image_id 32308, annotation: {'image_id': 32308, 'category_id': 8, 'bbox': [788, 10, 351, 163], 'area': 57213, 'id': 129242}\n",
      "Invalid category_id 8 in image_id 11009, annotation: {'image_id': 11009, 'category_id': 8, 'bbox': [68, 589, 465, 182], 'area': 84630, 'id': 32589}\n",
      "Invalid category_id 8 in image_id 11009, annotation: {'image_id': 11009, 'category_id': 8, 'bbox': [283, 387, 203, 124], 'area': 25172, 'id': 32590}\n",
      "Invalid category_id 8 in image_id 15078, annotation: {'image_id': 15078, 'category_id': 8, 'bbox': [1344, 323, 231, 96], 'area': 22176, 'id': 47192}\n",
      "Invalid category_id 8 in image_id 7250, annotation: {'image_id': 7250, 'category_id': 8, 'bbox': [530, 324, 92, 99], 'area': 9108, 'id': 20184}\n",
      "Invalid category_id 8 in image_id 9477, annotation: {'image_id': 9477, 'category_id': 8, 'bbox': [1106, 209, 169, 106], 'area': 17914, 'id': 27091}\n",
      "Invalid category_id 8 in image_id 3026, annotation: {'image_id': 3026, 'category_id': 8, 'bbox': [686, 473, 104, 106], 'area': 11024, 'id': 7583}\n",
      "Invalid category_id 8 in image_id 10407, annotation: {'image_id': 10407, 'category_id': 8, 'bbox': [1633, 132, 136, 101], 'area': 13736, 'id': 30475}\n",
      "Invalid category_id 10 in image_id 27223, annotation: {'image_id': 27223, 'category_id': 10, 'bbox': [1065, 671, 90, 80], 'area': 7200, 'id': 101662}\n",
      "Invalid category_id 8 in image_id 23610, annotation: {'image_id': 23610, 'category_id': 8, 'bbox': [1669, 98, 46, 36], 'area': 1656, 'id': 82496}\n",
      "Invalid category_id 8 in image_id 28394, annotation: {'image_id': 28394, 'category_id': 8, 'bbox': [313, 229, 88, 103], 'area': 9064, 'id': 107724}\n",
      "Invalid category_id 10 in image_id 28394, annotation: {'image_id': 28394, 'category_id': 10, 'bbox': [376, 208, 171, 110], 'area': 18810, 'id': 107725}\n",
      "Invalid category_id 8 in image_id 21115, annotation: {'image_id': 21115, 'category_id': 8, 'bbox': [768, 0, 163, 80], 'area': 13040, 'id': 67782}\n",
      "Invalid category_id 10 in image_id 17205, annotation: {'image_id': 17205, 'category_id': 10, 'bbox': [171, 26, 777, 259], 'area': 201243, 'id': 55016}\n",
      "Invalid category_id 8 in image_id 16977, annotation: {'image_id': 16977, 'category_id': 8, 'bbox': [1376, 156, 225, 86], 'area': 19350, 'id': 54313}\n",
      "Invalid category_id 8 in image_id 23640, annotation: {'image_id': 23640, 'category_id': 8, 'bbox': [1700, 91, 90, 68], 'area': 6120, 'id': 82634}\n",
      "Invalid category_id 8 in image_id 7002, annotation: {'image_id': 7002, 'category_id': 8, 'bbox': [1533, 287, 128, 133], 'area': 17024, 'id': 19475}\n",
      "Invalid category_id 8 in image_id 13992, annotation: {'image_id': 13992, 'category_id': 8, 'bbox': [1387, 286, 130, 183], 'area': 23790, 'id': 43330}\n",
      "Invalid category_id 8 in image_id 2698, annotation: {'image_id': 2698, 'category_id': 8, 'bbox': [1154, 336, 198, 94], 'area': 18612, 'id': 6596}\n",
      "Invalid category_id 10 in image_id 22037, annotation: {'image_id': 22037, 'category_id': 10, 'bbox': [1585, 383, 59, 62], 'area': 3658, 'id': 73044}\n",
      "Invalid category_id 8 in image_id 22037, annotation: {'image_id': 22037, 'category_id': 8, 'bbox': [738, 191, 76, 84], 'area': 6384, 'id': 73049}\n",
      "Invalid category_id 10 in image_id 22037, annotation: {'image_id': 22037, 'category_id': 10, 'bbox': [726, 130, 79, 63], 'area': 4977, 'id': 73051}\n",
      "Invalid category_id 8 in image_id 12135, annotation: {'image_id': 12135, 'category_id': 8, 'bbox': [1218, 215, 69, 109], 'area': 7521, 'id': 36568}\n",
      "Invalid category_id 10 in image_id 12135, annotation: {'image_id': 12135, 'category_id': 10, 'bbox': [1268, 134, 104, 185], 'area': 19240, 'id': 36569}\n",
      "Invalid category_id 8 in image_id 12135, annotation: {'image_id': 12135, 'category_id': 8, 'bbox': [1498, 106, 19, 57], 'area': 1083, 'id': 36572}\n",
      "Invalid category_id 10 in image_id 12135, annotation: {'image_id': 12135, 'category_id': 10, 'bbox': [1496, 78, 42, 94], 'area': 3948, 'id': 36573}\n",
      "Invalid category_id 8 in image_id 23212, annotation: {'image_id': 23212, 'category_id': 8, 'bbox': [1643, 5, 40, 10], 'area': 400, 'id': 80277}\n",
      "Invalid category_id 8 in image_id 12479, annotation: {'image_id': 12479, 'category_id': 8, 'bbox': [69, 70, 265, 258], 'area': 68370, 'id': 37770}\n",
      "Invalid category_id 8 in image_id 13832, annotation: {'image_id': 13832, 'category_id': 8, 'bbox': [1496, 84, 45, 40], 'area': 1800, 'id': 42773}\n",
      "Invalid category_id 10 in image_id 25926, annotation: {'image_id': 25926, 'category_id': 10, 'bbox': [135, 327, 67, 33], 'area': 2211, 'id': 94641}\n",
      "Invalid category_id 10 in image_id 25926, annotation: {'image_id': 25926, 'category_id': 10, 'bbox': [87, 330, 61, 37], 'area': 2257, 'id': 94642}\n",
      "Invalid category_id 10 in image_id 25926, annotation: {'image_id': 25926, 'category_id': 10, 'bbox': [274, 312, 56, 35], 'area': 1960, 'id': 94643}\n",
      "Invalid category_id 10 in image_id 25926, annotation: {'image_id': 25926, 'category_id': 10, 'bbox': [310, 310, 45, 30], 'area': 1350, 'id': 94644}\n",
      "Invalid category_id 8 in image_id 19705, annotation: {'image_id': 19705, 'category_id': 8, 'bbox': [1263, 252, 187, 96], 'area': 17952, 'id': 63239}\n",
      "Invalid category_id 8 in image_id 19548, annotation: {'image_id': 19548, 'category_id': 8, 'bbox': [951, 359, 250, 243], 'area': 60750, 'id': 62768}\n",
      "Invalid category_id 10 in image_id 24471, annotation: {'image_id': 24471, 'category_id': 10, 'bbox': [1400, 334, 503, 383], 'area': 192649, 'id': 86982}\n",
      "Invalid category_id 8 in image_id 24471, annotation: {'image_id': 24471, 'category_id': 8, 'bbox': [1025, 259, 411, 280], 'area': 115080, 'id': 86983}\n",
      "Invalid category_id 10 in image_id 24471, annotation: {'image_id': 24471, 'category_id': 10, 'bbox': [264, 101, 56, 39], 'area': 2184, 'id': 86990}\n",
      "Invalid category_id 8 in image_id 12718, annotation: {'image_id': 12718, 'category_id': 8, 'bbox': [1121, 349, 389, 285], 'area': 110865, 'id': 38682}\n",
      "Invalid category_id 8 in image_id 19456, annotation: {'image_id': 19456, 'category_id': 8, 'bbox': [741, 203, 44, 44], 'area': 1936, 'id': 62438}\n",
      "Invalid category_id 8 in image_id 2825, annotation: {'image_id': 2825, 'category_id': 8, 'bbox': [420, 366, 252, 128], 'area': 32256, 'id': 7025}\n",
      "Invalid category_id 8 in image_id 24104, annotation: {'image_id': 24104, 'category_id': 8, 'bbox': [1403, 141, 136, 91], 'area': 12376, 'id': 85152}\n",
      "Invalid category_id 8 in image_id 12539, annotation: {'image_id': 12539, 'category_id': 8, 'bbox': [1512, 159, 107, 164], 'area': 17548, 'id': 38015}\n",
      "Invalid category_id 8 in image_id 12539, annotation: {'image_id': 12539, 'category_id': 8, 'bbox': [1163, 0, 69, 79], 'area': 5451, 'id': 38016}\n",
      "Invalid category_id 8 in image_id 12539, annotation: {'image_id': 12539, 'category_id': 8, 'bbox': [0, 249, 114, 274], 'area': 31236, 'id': 38017}\n",
      "Invalid category_id 8 in image_id 9051, annotation: {'image_id': 9051, 'category_id': 8, 'bbox': [0, 706, 754, 349], 'area': 263146, 'id': 25689}\n",
      "Invalid category_id 8 in image_id 30990, annotation: {'image_id': 30990, 'category_id': 8, 'bbox': [164, 661, 569, 391], 'area': 222479, 'id': 122040}\n",
      "Invalid category_id 8 in image_id 14523, annotation: {'image_id': 14523, 'category_id': 8, 'bbox': [1523, 160, 185, 106], 'area': 19610, 'id': 45326}\n",
      "Invalid category_id 8 in image_id 4230, annotation: {'image_id': 4230, 'category_id': 8, 'bbox': [372, 365, 239, 99], 'area': 23661, 'id': 11077}\n",
      "Invalid category_id 8 in image_id 6757, annotation: {'image_id': 6757, 'category_id': 8, 'bbox': [316, 378, 314, 94], 'area': 29516, 'id': 18761}\n",
      "Invalid category_id 8 in image_id 6757, annotation: {'image_id': 6757, 'category_id': 8, 'bbox': [1565, 307, 136, 121], 'area': 16456, 'id': 18764}\n",
      "Invalid category_id 8 in image_id 6948, annotation: {'image_id': 6948, 'category_id': 8, 'bbox': [1375, 307, 175, 94], 'area': 16450, 'id': 19314}\n",
      "Invalid category_id 8 in image_id 6948, annotation: {'image_id': 6948, 'category_id': 8, 'bbox': [1550, 333, 106, 83], 'area': 8798, 'id': 19315}\n",
      "Invalid category_id 10 in image_id 10671, annotation: {'image_id': 10671, 'category_id': 10, 'bbox': [837, 244, 162, 254], 'area': 41148, 'id': 31412}\n",
      "Invalid category_id 8 in image_id 10671, annotation: {'image_id': 10671, 'category_id': 8, 'bbox': [1178, 561, 321, 242], 'area': 77682, 'id': 31413}\n",
      "Invalid category_id 8 in image_id 10671, annotation: {'image_id': 10671, 'category_id': 8, 'bbox': [529, 670, 344, 182], 'area': 62608, 'id': 31414}\n",
      "Invalid category_id 8 in image_id 3410, annotation: {'image_id': 3410, 'category_id': 8, 'bbox': [568, 537, 154, 75], 'area': 11550, 'id': 8694}\n",
      "Invalid category_id 8 in image_id 11482, annotation: {'image_id': 11482, 'category_id': 8, 'bbox': [2, 345, 478, 231], 'area': 110418, 'id': 34328}\n",
      "Invalid category_id 10 in image_id 5230, annotation: {'image_id': 5230, 'category_id': 10, 'bbox': [555, 340, 135, 96], 'area': 12960, 'id': 14152}\n",
      "Invalid category_id 8 in image_id 5290, annotation: {'image_id': 5290, 'category_id': 8, 'bbox': [1488, 485, 105, 96], 'area': 10080, 'id': 14310}\n",
      "Invalid category_id 8 in image_id 3045, annotation: {'image_id': 3045, 'category_id': 8, 'bbox': [1145, 375, 150, 145], 'area': 21750, 'id': 7644}\n",
      "Invalid category_id 10 in image_id 12662, annotation: {'image_id': 12662, 'category_id': 10, 'bbox': [2, 319, 148, 265], 'area': 39220, 'id': 38399}\n",
      "Invalid category_id 8 in image_id 8873, annotation: {'image_id': 8873, 'category_id': 8, 'bbox': [437, 53, 491, 144], 'area': 70704, 'id': 25087}\n",
      "Invalid category_id 8 in image_id 30654, annotation: {'image_id': 30654, 'category_id': 8, 'bbox': [588, 490, 176, 182], 'area': 32032, 'id': 120275}\n",
      "Invalid category_id 8 in image_id 18809, annotation: {'image_id': 18809, 'category_id': 8, 'bbox': [1848, 150, 55, 31], 'area': 1705, 'id': 60120}\n",
      "Invalid category_id 10 in image_id 6142, annotation: {'image_id': 6142, 'category_id': 10, 'bbox': [1207, 512, 140, 188], 'area': 26320, 'id': 16923}\n",
      "Invalid category_id 8 in image_id 18311, annotation: {'image_id': 18311, 'category_id': 8, 'bbox': [1729, 707, 187, 373], 'area': 69751, 'id': 58493}\n",
      "Invalid category_id 8 in image_id 3078, annotation: {'image_id': 3078, 'category_id': 8, 'bbox': [1603, 336, 79, 54], 'area': 4266, 'id': 7739}\n",
      "Invalid category_id 10 in image_id 3078, annotation: {'image_id': 3078, 'category_id': 10, 'bbox': [1646, 322, 76, 43], 'area': 3268, 'id': 7740}\n",
      "Invalid category_id 8 in image_id 6029, annotation: {'image_id': 6029, 'category_id': 8, 'bbox': [1340, 304, 270, 105], 'area': 28350, 'id': 16607}\n",
      "Invalid category_id 8 in image_id 13202, annotation: {'image_id': 13202, 'category_id': 8, 'bbox': [1755, 224, 56, 50], 'area': 2800, 'id': 40636}\n",
      "Invalid category_id 8 in image_id 13323, annotation: {'image_id': 13323, 'category_id': 8, 'bbox': [911, 783, 304, 263], 'area': 79952, 'id': 41034}\n",
      "Invalid category_id 8 in image_id 17202, annotation: {'image_id': 17202, 'category_id': 8, 'bbox': [781, 262, 226, 109], 'area': 24634, 'id': 55006}\n",
      "Invalid category_id 8 in image_id 8654, annotation: {'image_id': 8654, 'category_id': 8, 'bbox': [1096, 467, 56, 117], 'area': 6552, 'id': 24299}\n",
      "Invalid category_id 10 in image_id 8654, annotation: {'image_id': 8654, 'category_id': 10, 'bbox': [912, 485, 196, 126], 'area': 24696, 'id': 24300}\n",
      "Invalid category_id 8 in image_id 27635, annotation: {'image_id': 27635, 'category_id': 8, 'bbox': [1364, 440, 233, 141], 'area': 32853, 'id': 103876}\n",
      "Invalid category_id 8 in image_id 21534, annotation: {'image_id': 21534, 'category_id': 8, 'bbox': [460, 393, 270, 180], 'area': 48600, 'id': 69967}\n",
      "Invalid category_id 10 in image_id 26859, annotation: {'image_id': 26859, 'category_id': 10, 'bbox': [6, 845, 420, 228], 'area': 95760, 'id': 99658}\n",
      "Invalid category_id 8 in image_id 26859, annotation: {'image_id': 26859, 'category_id': 8, 'bbox': [429, 821, 154, 160], 'area': 24640, 'id': 99659}\n",
      "Invalid category_id 8 in image_id 6137, annotation: {'image_id': 6137, 'category_id': 8, 'bbox': [463, 484, 103, 101], 'area': 10403, 'id': 16912}\n",
      "Invalid category_id 8 in image_id 20123, annotation: {'image_id': 20123, 'category_id': 8, 'bbox': [705, 300, 195, 107], 'area': 20865, 'id': 64736}\n",
      "Invalid category_id 8 in image_id 9431, annotation: {'image_id': 9431, 'category_id': 8, 'bbox': [1507, 163, 103, 163], 'area': 16789, 'id': 26975}\n",
      "Invalid category_id 8 in image_id 9431, annotation: {'image_id': 9431, 'category_id': 8, 'bbox': [0, 250, 87, 129], 'area': 11223, 'id': 26981}\n",
      "Invalid category_id 8 in image_id 26676, annotation: {'image_id': 26676, 'category_id': 8, 'bbox': [1019, 486, 462, 149], 'area': 68838, 'id': 98707}\n",
      "Invalid category_id 8 in image_id 8539, annotation: {'image_id': 8539, 'category_id': 8, 'bbox': [0, 611, 561, 444], 'area': 249084, 'id': 23907}\n",
      "Invalid category_id 8 in image_id 9085, annotation: {'image_id': 9085, 'category_id': 8, 'bbox': [599, 151, 117, 182], 'area': 21294, 'id': 25776}\n",
      "Invalid category_id 8 in image_id 11880, annotation: {'image_id': 11880, 'category_id': 8, 'bbox': [551, 406, 486, 253], 'area': 122958, 'id': 35743}\n",
      "Invalid category_id 8 in image_id 6364, annotation: {'image_id': 6364, 'category_id': 8, 'bbox': [1095, 321, 249, 191], 'area': 47559, 'id': 17613}\n",
      "Invalid category_id 8 in image_id 17053, annotation: {'image_id': 17053, 'category_id': 8, 'bbox': [690, 687, 455, 310], 'area': 141050, 'id': 54582}\n",
      "Invalid category_id 8 in image_id 18482, annotation: {'image_id': 18482, 'category_id': 8, 'bbox': [1317, 540, 354, 340], 'area': 120360, 'id': 59003}\n",
      "Invalid category_id 8 in image_id 8486, annotation: {'image_id': 8486, 'category_id': 8, 'bbox': [1007, 249, 240, 198], 'area': 47520, 'id': 23665}\n",
      "Invalid category_id 8 in image_id 19000, annotation: {'image_id': 19000, 'category_id': 8, 'bbox': [1340, 644, 124, 162], 'area': 20088, 'id': 60857}\n",
      "Invalid category_id 8 in image_id 27349, annotation: {'image_id': 27349, 'category_id': 8, 'bbox': [957, 719, 218, 129], 'area': 28122, 'id': 102369}\n",
      "Invalid category_id 8 in image_id 5162, annotation: {'image_id': 5162, 'category_id': 8, 'bbox': [932, 319, 185, 96], 'area': 17760, 'id': 13956}\n",
      "Invalid category_id 8 in image_id 6745, annotation: {'image_id': 6745, 'category_id': 8, 'bbox': [483, 437, 89, 92], 'area': 8188, 'id': 18737}\n",
      "Invalid category_id 10 in image_id 10581, annotation: {'image_id': 10581, 'category_id': 10, 'bbox': [581, 151, 173, 206], 'area': 35638, 'id': 31048}\n",
      "Invalid category_id 10 in image_id 32442, annotation: {'image_id': 32442, 'category_id': 10, 'bbox': [105, 80, 176, 94], 'area': 16544, 'id': 129925}\n",
      "Invalid category_id 8 in image_id 17873, annotation: {'image_id': 17873, 'category_id': 8, 'bbox': [974, 175, 123, 46], 'area': 5658, 'id': 57122}\n",
      "Invalid category_id 8 in image_id 8625, annotation: {'image_id': 8625, 'category_id': 8, 'bbox': [585, 301, 71, 121], 'area': 8591, 'id': 24169}\n",
      "Invalid category_id 8 in image_id 21249, annotation: {'image_id': 21249, 'category_id': 8, 'bbox': [1271, 188, 189, 84], 'area': 15876, 'id': 68309}\n",
      "Invalid category_id 8 in image_id 21249, annotation: {'image_id': 21249, 'category_id': 8, 'bbox': [666, 116, 68, 94], 'area': 6392, 'id': 68311}\n",
      "Invalid category_id 10 in image_id 23846, annotation: {'image_id': 23846, 'category_id': 10, 'bbox': [1148, 279, 130, 66], 'area': 8580, 'id': 83872}\n",
      "Invalid category_id 8 in image_id 30935, annotation: {'image_id': 30935, 'category_id': 8, 'bbox': [1604, 587, 290, 215], 'area': 62350, 'id': 121801}\n",
      "Invalid category_id 8 in image_id 20293, annotation: {'image_id': 20293, 'category_id': 8, 'bbox': [1770, 81, 150, 112], 'area': 16800, 'id': 65310}\n",
      "Invalid category_id 10 in image_id 10154, annotation: {'image_id': 10154, 'category_id': 10, 'bbox': [937, 382, 225, 70], 'area': 15750, 'id': 29558}\n",
      "Invalid category_id 8 in image_id 28087, annotation: {'image_id': 28087, 'category_id': 8, 'bbox': [980, 613, 519, 179], 'area': 92901, 'id': 106239}\n",
      "Invalid category_id 10 in image_id 15568, annotation: {'image_id': 15568, 'category_id': 10, 'bbox': [593, 176, 204, 106], 'area': 21624, 'id': 49222}\n",
      "Invalid category_id 8 in image_id 3539, annotation: {'image_id': 3539, 'category_id': 8, 'bbox': [1436, 356, 138, 89], 'area': 12282, 'id': 9061}\n",
      "Invalid category_id 8 in image_id 13155, annotation: {'image_id': 13155, 'category_id': 8, 'bbox': [139, 287, 193, 94], 'area': 18142, 'id': 40427}\n",
      "Invalid category_id 8 in image_id 13155, annotation: {'image_id': 13155, 'category_id': 8, 'bbox': [0, 228, 79, 124], 'area': 9796, 'id': 40428}\n",
      "Invalid category_id 8 in image_id 13131, annotation: {'image_id': 13131, 'category_id': 8, 'bbox': [0, 606, 362, 185], 'area': 66970, 'id': 40333}\n",
      "Invalid category_id 8 in image_id 13131, annotation: {'image_id': 13131, 'category_id': 8, 'bbox': [565, 320, 83, 93], 'area': 7719, 'id': 40334}\n",
      "Invalid category_id 10 in image_id 28906, annotation: {'image_id': 28906, 'category_id': 10, 'bbox': [448, 310, 110, 41], 'area': 4510, 'id': 110434}\n",
      "Invalid category_id 10 in image_id 7801, annotation: {'image_id': 7801, 'category_id': 10, 'bbox': [1461, 318, 228, 136], 'area': 31008, 'id': 21723}\n",
      "Invalid category_id 8 in image_id 23419, annotation: {'image_id': 23419, 'category_id': 8, 'bbox': [0, 240, 927, 347], 'area': 321669, 'id': 81446}\n",
      "Invalid category_id 8 in image_id 5096, annotation: {'image_id': 5096, 'category_id': 8, 'bbox': [1221, 439, 129, 131], 'area': 16899, 'id': 13779}\n",
      "Invalid category_id 8 in image_id 22050, annotation: {'image_id': 22050, 'category_id': 8, 'bbox': [1779, 408, 141, 124], 'area': 17484, 'id': 73176}\n",
      "Invalid category_id 8 in image_id 12064, annotation: {'image_id': 12064, 'category_id': 8, 'bbox': [471, 588, 207, 105], 'area': 21735, 'id': 36330}\n",
      "Invalid category_id 8 in image_id 22704, annotation: {'image_id': 22704, 'category_id': 8, 'bbox': [475, 728, 282, 185], 'area': 52170, 'id': 77246}\n",
      "Invalid category_id 8 in image_id 9231, annotation: {'image_id': 9231, 'category_id': 8, 'bbox': [1130, 312, 180, 193], 'area': 34740, 'id': 26336}\n",
      "Invalid category_id 10 in image_id 9231, annotation: {'image_id': 9231, 'category_id': 10, 'bbox': [1251, 463, 255, 226], 'area': 57630, 'id': 26337}\n",
      "Invalid category_id 10 in image_id 8823, annotation: {'image_id': 8823, 'category_id': 10, 'bbox': [1195, 445, 655, 503], 'area': 329465, 'id': 24884}\n",
      "Invalid category_id 8 in image_id 5959, annotation: {'image_id': 5959, 'category_id': 8, 'bbox': [1578, 774, 342, 306], 'area': 104652, 'id': 16375}\n",
      "Invalid category_id 8 in image_id 29308, annotation: {'image_id': 29308, 'category_id': 8, 'bbox': [1456, 373, 434, 165], 'area': 71610, 'id': 112570}\n",
      "Invalid category_id 8 in image_id 24177, annotation: {'image_id': 24177, 'category_id': 8, 'bbox': [355, 15, 70, 53], 'area': 3710, 'id': 85527}\n",
      "Invalid category_id 8 in image_id 30239, annotation: {'image_id': 30239, 'category_id': 8, 'bbox': [13, 257, 225, 116], 'area': 26100, 'id': 117841}\n",
      "Invalid category_id 8 in image_id 23682, annotation: {'image_id': 23682, 'category_id': 8, 'bbox': [485, 343, 505, 233], 'area': 117665, 'id': 82849}\n",
      "Invalid category_id 8 in image_id 23840, annotation: {'image_id': 23840, 'category_id': 8, 'bbox': [307, 208, 82, 78], 'area': 6396, 'id': 83832}\n",
      "Invalid category_id 8 in image_id 23840, annotation: {'image_id': 23840, 'category_id': 8, 'bbox': [501, 46, 33, 37], 'area': 1221, 'id': 83838}\n",
      "Invalid category_id 8 in image_id 19067, annotation: {'image_id': 19067, 'category_id': 8, 'bbox': [975, 406, 214, 191], 'area': 40874, 'id': 61092}\n",
      "Invalid category_id 8 in image_id 5254, annotation: {'image_id': 5254, 'category_id': 8, 'bbox': [599, 567, 468, 398], 'area': 186264, 'id': 14228}\n",
      "Invalid category_id 8 in image_id 11051, annotation: {'image_id': 11051, 'category_id': 8, 'bbox': [1534, 178, 96, 72], 'area': 6912, 'id': 32758}\n",
      "Invalid category_id 10 in image_id 6327, annotation: {'image_id': 6327, 'category_id': 10, 'bbox': [1684, 272, 140, 101], 'area': 14140, 'id': 17491}\n",
      "Invalid category_id 8 in image_id 13437, annotation: {'image_id': 13437, 'category_id': 8, 'bbox': [206, 58, 499, 278], 'area': 138722, 'id': 41415}\n",
      "Invalid category_id 8 in image_id 12428, annotation: {'image_id': 12428, 'category_id': 8, 'bbox': [1315, 192, 200, 111], 'area': 22200, 'id': 37592}\n",
      "Invalid category_id 8 in image_id 12428, annotation: {'image_id': 12428, 'category_id': 8, 'bbox': [1076, 231, 126, 102], 'area': 12852, 'id': 37596}\n",
      "Invalid category_id 8 in image_id 11081, annotation: {'image_id': 11081, 'category_id': 8, 'bbox': [1352, 117, 383, 285], 'area': 109155, 'id': 32870}\n",
      "Invalid category_id 8 in image_id 29187, annotation: {'image_id': 29187, 'category_id': 8, 'bbox': [1417, 401, 203, 126], 'area': 25578, 'id': 111928}\n",
      "Invalid category_id 8 in image_id 29187, annotation: {'image_id': 29187, 'category_id': 8, 'bbox': [654, 338, 366, 98], 'area': 35868, 'id': 111933}\n",
      "Invalid category_id 8 in image_id 18289, annotation: {'image_id': 18289, 'category_id': 8, 'bbox': [1710, 90, 157, 102], 'area': 16014, 'id': 58399}\n",
      "Invalid category_id 8 in image_id 5534, annotation: {'image_id': 5534, 'category_id': 8, 'bbox': [6, 462, 549, 262], 'area': 143838, 'id': 15046}\n",
      "Invalid category_id 8 in image_id 8080, annotation: {'image_id': 8080, 'category_id': 8, 'bbox': [1055, 393, 199, 250], 'area': 49750, 'id': 22499}\n",
      "Invalid category_id 8 in image_id 6584, annotation: {'image_id': 6584, 'category_id': 8, 'bbox': [328, 420, 190, 186], 'area': 35340, 'id': 18275}\n",
      "Invalid category_id 8 in image_id 15204, annotation: {'image_id': 15204, 'category_id': 8, 'bbox': [1514, 127, 161, 127], 'area': 20447, 'id': 47584}\n",
      "Invalid category_id 10 in image_id 14260, annotation: {'image_id': 14260, 'category_id': 10, 'bbox': [928, 481, 325, 170], 'area': 55250, 'id': 44386}\n",
      "Invalid category_id 8 in image_id 29710, annotation: {'image_id': 29710, 'category_id': 8, 'bbox': [1582, 355, 338, 170], 'area': 57460, 'id': 114889}\n",
      "Invalid category_id 8 in image_id 4315, annotation: {'image_id': 4315, 'category_id': 8, 'bbox': [1798, 291, 50, 40], 'area': 2000, 'id': 11322}\n",
      "Invalid category_id 8 in image_id 5527, annotation: {'image_id': 5527, 'category_id': 8, 'bbox': [1305, 360, 208, 170], 'area': 35360, 'id': 15008}\n",
      "Invalid category_id 8 in image_id 10828, annotation: {'image_id': 10828, 'category_id': 8, 'bbox': [1127, 389, 364, 249], 'area': 90636, 'id': 31964}\n",
      "Invalid category_id 8 in image_id 27556, annotation: {'image_id': 27556, 'category_id': 8, 'bbox': [30, 665, 693, 415], 'area': 287595, 'id': 103485}\n",
      "Invalid category_id 10 in image_id 9355, annotation: {'image_id': 9355, 'category_id': 10, 'bbox': [561, 635, 445, 135], 'area': 60075, 'id': 26740}\n",
      "Invalid category_id 8 in image_id 9355, annotation: {'image_id': 9355, 'category_id': 8, 'bbox': [1368, 193, 332, 128], 'area': 42496, 'id': 26741}\n",
      "Invalid category_id 8 in image_id 13271, annotation: {'image_id': 13271, 'category_id': 8, 'bbox': [1272, 322, 208, 247], 'area': 51376, 'id': 40859}\n",
      "Invalid category_id 8 in image_id 25120, annotation: {'image_id': 25120, 'category_id': 8, 'bbox': [961, 260, 645, 344], 'area': 221880, 'id': 89669}\n",
      "Invalid category_id 10 in image_id 19210, annotation: {'image_id': 19210, 'category_id': 10, 'bbox': [1162, 288, 159, 72], 'area': 11448, 'id': 61620}\n",
      "Invalid category_id 8 in image_id 11683, annotation: {'image_id': 11683, 'category_id': 8, 'bbox': [520, 220, 377, 167], 'area': 62959, 'id': 35009}\n",
      "Invalid category_id 8 in image_id 11683, annotation: {'image_id': 11683, 'category_id': 8, 'bbox': [1828, 66, 76, 91], 'area': 6916, 'id': 35012}\n",
      "Invalid category_id 8 in image_id 20071, annotation: {'image_id': 20071, 'category_id': 8, 'bbox': [1428, 41, 77, 39], 'area': 3003, 'id': 64548}\n",
      "Invalid category_id 8 in image_id 21900, annotation: {'image_id': 21900, 'category_id': 8, 'bbox': [270, 207, 112, 83], 'area': 9296, 'id': 72156}\n",
      "Invalid category_id 8 in image_id 21900, annotation: {'image_id': 21900, 'category_id': 8, 'bbox': [504, 41, 22, 38], 'area': 836, 'id': 72166}\n",
      "Invalid category_id 8 in image_id 11634, annotation: {'image_id': 11634, 'category_id': 8, 'bbox': [1573, 685, 314, 296], 'area': 92944, 'id': 34841}\n",
      "Invalid category_id 8 in image_id 14065, annotation: {'image_id': 14065, 'category_id': 8, 'bbox': [393, 378, 107, 95], 'area': 10165, 'id': 43655}\n",
      "Invalid category_id 8 in image_id 30680, annotation: {'image_id': 30680, 'category_id': 8, 'bbox': [1642, 389, 212, 98], 'area': 20776, 'id': 120419}\n",
      "Invalid category_id 8 in image_id 30680, annotation: {'image_id': 30680, 'category_id': 8, 'bbox': [34, 298, 271, 161], 'area': 43631, 'id': 120421}\n",
      "Invalid category_id 8 in image_id 15581, annotation: {'image_id': 15581, 'category_id': 8, 'bbox': [1128, 374, 147, 176], 'area': 25872, 'id': 49269}\n",
      "Invalid category_id 10 in image_id 28670, annotation: {'image_id': 28670, 'category_id': 10, 'bbox': [1396, 35, 33, 33], 'area': 1089, 'id': 109207}\n",
      "Invalid category_id 10 in image_id 28670, annotation: {'image_id': 28670, 'category_id': 10, 'bbox': [1446, 31, 42, 39], 'area': 1638, 'id': 109209}\n",
      "Invalid category_id 10 in image_id 28670, annotation: {'image_id': 28670, 'category_id': 10, 'bbox': [1589, 26, 44, 27], 'area': 1188, 'id': 109210}\n",
      "Invalid category_id 8 in image_id 5442, annotation: {'image_id': 5442, 'category_id': 8, 'bbox': [1249, 544, 34, 112], 'area': 3808, 'id': 14758}\n",
      "Invalid category_id 10 in image_id 5442, annotation: {'image_id': 5442, 'category_id': 10, 'bbox': [1287, 530, 593, 435], 'area': 257955, 'id': 14759}\n",
      "Invalid category_id 8 in image_id 20368, annotation: {'image_id': 20368, 'category_id': 8, 'bbox': [118, 464, 88, 216], 'area': 19008, 'id': 65570}\n",
      "Invalid category_id 8 in image_id 15296, annotation: {'image_id': 15296, 'category_id': 8, 'bbox': [1131, 312, 380, 218], 'area': 82840, 'id': 47924}\n",
      "Invalid category_id 10 in image_id 29643, annotation: {'image_id': 29643, 'category_id': 10, 'bbox': [49, 670, 593, 410], 'area': 243130, 'id': 114498}\n",
      "Invalid category_id 8 in image_id 29643, annotation: {'image_id': 29643, 'category_id': 8, 'bbox': [293, 353, 101, 98], 'area': 9898, 'id': 114504}\n",
      "Invalid category_id 8 in image_id 27382, annotation: {'image_id': 27382, 'category_id': 8, 'bbox': [1289, 108, 198, 184], 'area': 36432, 'id': 102541}\n",
      "Invalid category_id 8 in image_id 6544, annotation: {'image_id': 6544, 'category_id': 8, 'bbox': [556, 274, 56, 83], 'area': 4648, 'id': 18130}\n",
      "Invalid category_id 8 in image_id 7899, annotation: {'image_id': 7899, 'category_id': 8, 'bbox': [508, 629, 296, 254], 'area': 75184, 'id': 22007}\n",
      "Invalid category_id 8 in image_id 30318, annotation: {'image_id': 30318, 'category_id': 8, 'bbox': [524, 495, 119, 91], 'area': 10829, 'id': 118276}\n",
      "Invalid category_id 8 in image_id 8968, annotation: {'image_id': 8968, 'category_id': 8, 'bbox': [1155, 131, 83, 176], 'area': 14608, 'id': 25380}\n",
      "Invalid category_id 8 in image_id 8968, annotation: {'image_id': 8968, 'category_id': 8, 'bbox': [585, 174, 103, 142], 'area': 14626, 'id': 25381}\n",
      "Invalid category_id 8 in image_id 32758, annotation: {'image_id': 32758, 'category_id': 8, 'bbox': [713, 167, 109, 38], 'area': 4142, 'id': 131697}\n",
      "Invalid category_id 8 in image_id 24109, annotation: {'image_id': 24109, 'category_id': 8, 'bbox': [405, 8, 148, 96], 'area': 14208, 'id': 85167}\n",
      "Invalid category_id 8 in image_id 31729, annotation: {'image_id': 31729, 'category_id': 8, 'bbox': [1471, 449, 60, 124], 'area': 7440, 'id': 126121}\n",
      "Invalid category_id 10 in image_id 31729, annotation: {'image_id': 31729, 'category_id': 10, 'bbox': [108, 462, 197, 124], 'area': 24428, 'id': 126122}\n",
      "Invalid category_id 8 in image_id 12824, annotation: {'image_id': 12824, 'category_id': 8, 'bbox': [190, 154, 334, 134], 'area': 44756, 'id': 39058}\n",
      "Invalid category_id 8 in image_id 17560, annotation: {'image_id': 17560, 'category_id': 8, 'bbox': [0, 119, 490, 297], 'area': 145530, 'id': 56108}\n",
      "Invalid category_id 10 in image_id 24248, annotation: {'image_id': 24248, 'category_id': 10, 'bbox': [1583, 302, 59, 49], 'area': 2891, 'id': 85919}\n",
      "Invalid category_id 10 in image_id 24248, annotation: {'image_id': 24248, 'category_id': 10, 'bbox': [1684, 301, 48, 58], 'area': 2784, 'id': 85920}\n",
      "Invalid category_id 8 in image_id 14896, annotation: {'image_id': 14896, 'category_id': 8, 'bbox': [1713, 234, 64, 81], 'area': 5184, 'id': 46585}\n",
      "Invalid category_id 8 in image_id 2593, annotation: {'image_id': 2593, 'category_id': 8, 'bbox': [739, 486, 279, 136], 'area': 37944, 'id': 6301}\n",
      "Invalid category_id 8 in image_id 2593, annotation: {'image_id': 2593, 'category_id': 8, 'bbox': [1627, 309, 87, 82], 'area': 7134, 'id': 6302}\n",
      "Invalid category_id 8 in image_id 2593, annotation: {'image_id': 2593, 'category_id': 8, 'bbox': [1774, 276, 83, 71], 'area': 5893, 'id': 6303}\n",
      "Invalid category_id 8 in image_id 17446, annotation: {'image_id': 17446, 'category_id': 8, 'bbox': [1067, 229, 311, 161], 'area': 50071, 'id': 55756}\n",
      "Invalid category_id 10 in image_id 26811, annotation: {'image_id': 26811, 'category_id': 10, 'bbox': [626, 83, 38, 28], 'area': 1064, 'id': 99386}\n",
      "Invalid category_id 10 in image_id 26811, annotation: {'image_id': 26811, 'category_id': 10, 'bbox': [670, 80, 37, 27], 'area': 999, 'id': 99387}\n",
      "Invalid category_id 10 in image_id 26811, annotation: {'image_id': 26811, 'category_id': 10, 'bbox': [703, 57, 30, 22], 'area': 660, 'id': 99388}\n",
      "Invalid category_id 8 in image_id 29167, annotation: {'image_id': 29167, 'category_id': 8, 'bbox': [1324, 310, 517, 155], 'area': 80135, 'id': 111797}\n",
      "Invalid category_id 8 in image_id 30910, annotation: {'image_id': 30910, 'category_id': 8, 'bbox': [859, 22, 316, 133], 'area': 42028, 'id': 121673}\n",
      "Invalid category_id 8 in image_id 13209, annotation: {'image_id': 13209, 'category_id': 8, 'bbox': [1046, 209, 351, 109], 'area': 38259, 'id': 40649}\n",
      "Invalid category_id 10 in image_id 13209, annotation: {'image_id': 13209, 'category_id': 10, 'bbox': [882, 674, 160, 124], 'area': 19840, 'id': 40650}\n",
      "Invalid category_id 8 in image_id 28939, annotation: {'image_id': 28939, 'category_id': 8, 'bbox': [958, 460, 574, 187], 'area': 107338, 'id': 110617}\n",
      "Invalid category_id 10 in image_id 24621, annotation: {'image_id': 24621, 'category_id': 10, 'bbox': [871, 195, 286, 152], 'area': 43472, 'id': 87622}\n",
      "Invalid category_id 10 in image_id 18262, annotation: {'image_id': 18262, 'category_id': 10, 'bbox': [1258, 414, 61, 51], 'area': 3111, 'id': 58293}\n",
      "Invalid category_id 8 in image_id 22534, annotation: {'image_id': 22534, 'category_id': 8, 'bbox': [545, 286, 592, 223], 'area': 132016, 'id': 76231}\n",
      "Invalid category_id 10 in image_id 22534, annotation: {'image_id': 22534, 'category_id': 10, 'bbox': [1278, 237, 104, 149], 'area': 15496, 'id': 76232}\n",
      "Invalid category_id 8 in image_id 32169, annotation: {'image_id': 32169, 'category_id': 8, 'bbox': [1417, 577, 503, 275], 'area': 138325, 'id': 128533}\n",
      "Invalid category_id 8 in image_id 21914, annotation: {'image_id': 21914, 'category_id': 8, 'bbox': [1047, 97, 185, 189], 'area': 34965, 'id': 72249}\n",
      "Invalid category_id 8 in image_id 10307, annotation: {'image_id': 10307, 'category_id': 8, 'bbox': [766, 257, 68, 90], 'area': 6120, 'id': 30094}\n",
      "Invalid category_id 10 in image_id 10307, annotation: {'image_id': 10307, 'category_id': 10, 'bbox': [824, 257, 169, 98], 'area': 16562, 'id': 30095}\n",
      "Invalid category_id 10 in image_id 32464, annotation: {'image_id': 32464, 'category_id': 10, 'bbox': [10, 275, 104, 117], 'area': 12168, 'id': 130026}\n",
      "Invalid category_id 8 in image_id 3530, annotation: {'image_id': 3530, 'category_id': 8, 'bbox': [5, 517, 297, 236], 'area': 70092, 'id': 9044}\n",
      "Invalid category_id 8 in image_id 19271, annotation: {'image_id': 19271, 'category_id': 8, 'bbox': [509, 508, 131, 109], 'area': 14279, 'id': 61818}\n",
      "Invalid category_id 8 in image_id 19271, annotation: {'image_id': 19271, 'category_id': 8, 'bbox': [0, 696, 313, 252], 'area': 78876, 'id': 61819}\n",
      "Invalid category_id 8 in image_id 19271, annotation: {'image_id': 19271, 'category_id': 8, 'bbox': [761, 225, 155, 96], 'area': 14880, 'id': 61820}\n",
      "Invalid category_id 8 in image_id 23569, annotation: {'image_id': 23569, 'category_id': 8, 'bbox': [562, 356, 390, 203], 'area': 79170, 'id': 82282}\n",
      "Invalid category_id 10 in image_id 14202, annotation: {'image_id': 14202, 'category_id': 10, 'bbox': [899, 247, 228, 79], 'area': 18012, 'id': 44195}\n",
      "Invalid category_id 8 in image_id 23800, annotation: {'image_id': 23800, 'category_id': 8, 'bbox': [1462, 292, 129, 135], 'area': 17415, 'id': 83609}\n",
      "Invalid category_id 10 in image_id 23800, annotation: {'image_id': 23800, 'category_id': 10, 'bbox': [104, 497, 492, 235], 'area': 115620, 'id': 83610}\n",
      "Invalid category_id 8 in image_id 26188, annotation: {'image_id': 26188, 'category_id': 8, 'bbox': [117, 382, 305, 175], 'area': 53375, 'id': 96026}\n",
      "Invalid category_id 8 in image_id 9465, annotation: {'image_id': 9465, 'category_id': 8, 'bbox': [1121, 298, 162, 148], 'area': 23976, 'id': 27064}\n",
      "Invalid category_id 10 in image_id 9465, annotation: {'image_id': 9465, 'category_id': 10, 'bbox': [1198, 419, 225, 178], 'area': 40050, 'id': 27065}\n",
      "Invalid category_id 8 in image_id 20665, annotation: {'image_id': 20665, 'category_id': 8, 'bbox': [1661, 627, 259, 451], 'area': 116809, 'id': 66442}\n",
      "Invalid category_id 8 in image_id 15349, annotation: {'image_id': 15349, 'category_id': 8, 'bbox': [1362, 135, 545, 301], 'area': 164045, 'id': 48177}\n",
      "Invalid category_id 8 in image_id 11127, annotation: {'image_id': 11127, 'category_id': 8, 'bbox': [1106, 395, 118, 147], 'area': 17346, 'id': 33062}\n",
      "Invalid category_id 10 in image_id 17990, annotation: {'image_id': 17990, 'category_id': 10, 'bbox': [1028, 607, 179, 177], 'area': 31683, 'id': 57471}\n",
      "Invalid category_id 8 in image_id 29370, annotation: {'image_id': 29370, 'category_id': 8, 'bbox': [576, 425, 404, 184], 'area': 74336, 'id': 112936}\n",
      "Invalid category_id 8 in image_id 17089, annotation: {'image_id': 17089, 'category_id': 8, 'bbox': [908, 0, 137, 66], 'area': 9042, 'id': 54670}\n",
      "Invalid category_id 8 in image_id 7365, annotation: {'image_id': 7365, 'category_id': 8, 'bbox': [285, 382, 202, 140], 'area': 28280, 'id': 20499}\n",
      "Invalid category_id 8 in image_id 7729, annotation: {'image_id': 7729, 'category_id': 8, 'bbox': [1008, 453, 218, 123], 'area': 26814, 'id': 21516}\n",
      "Invalid category_id 8 in image_id 7729, annotation: {'image_id': 7729, 'category_id': 8, 'bbox': [1763, 285, 67, 60], 'area': 4020, 'id': 21519}\n",
      "Invalid category_id 8 in image_id 28551, annotation: {'image_id': 28551, 'category_id': 8, 'bbox': [248, 377, 461, 234], 'area': 107874, 'id': 108543}\n",
      "Invalid category_id 8 in image_id 9648, annotation: {'image_id': 9648, 'category_id': 8, 'bbox': [1220, 169, 35, 96], 'area': 3360, 'id': 27777}\n",
      "Invalid category_id 8 in image_id 4398, annotation: {'image_id': 4398, 'category_id': 8, 'bbox': [515, 501, 201, 142], 'area': 28542, 'id': 11590}\n",
      "Invalid category_id 8 in image_id 15034, annotation: {'image_id': 15034, 'category_id': 8, 'bbox': [940, 226, 183, 131], 'area': 23973, 'id': 47054}\n",
      "Invalid category_id 8 in image_id 11080, annotation: {'image_id': 11080, 'category_id': 8, 'bbox': [1520, 183, 190, 198], 'area': 37620, 'id': 32866}\n",
      "Invalid category_id 8 in image_id 10743, annotation: {'image_id': 10743, 'category_id': 8, 'bbox': [1645, 189, 70, 134], 'area': 9380, 'id': 31677}\n",
      "Invalid category_id 8 in image_id 10743, annotation: {'image_id': 10743, 'category_id': 8, 'bbox': [1266, 8, 28, 33], 'area': 924, 'id': 31678}\n",
      "Invalid category_id 8 in image_id 27321, annotation: {'image_id': 27321, 'category_id': 8, 'bbox': [1120, 345, 206, 210], 'area': 43260, 'id': 102189}\n",
      "Invalid category_id 10 in image_id 25725, annotation: {'image_id': 25725, 'category_id': 10, 'bbox': [107, 98, 129, 192], 'area': 24768, 'id': 93381}\n",
      "Invalid category_id 8 in image_id 15828, annotation: {'image_id': 15828, 'category_id': 8, 'bbox': [562, 512, 297, 173], 'area': 51381, 'id': 50279}\n",
      "Invalid category_id 8 in image_id 15828, annotation: {'image_id': 15828, 'category_id': 8, 'bbox': [614, 109, 146, 115], 'area': 16790, 'id': 50280}\n",
      "Invalid category_id 8 in image_id 15715, annotation: {'image_id': 15715, 'category_id': 8, 'bbox': [1369, 38, 148, 154], 'area': 22792, 'id': 49860}\n",
      "Invalid category_id 8 in image_id 6473, annotation: {'image_id': 6473, 'category_id': 8, 'bbox': [1661, 723, 259, 355], 'area': 91945, 'id': 17909}\n",
      "Invalid category_id 8 in image_id 21743, annotation: {'image_id': 21743, 'category_id': 8, 'bbox': [1231, 50, 93, 106], 'area': 9858, 'id': 71222}\n",
      "Invalid category_id 8 in image_id 31645, annotation: {'image_id': 31645, 'category_id': 8, 'bbox': [910, 441, 96, 65], 'area': 6240, 'id': 125632}\n",
      "Invalid category_id 10 in image_id 22149, annotation: {'image_id': 22149, 'category_id': 10, 'bbox': [1829, 416, 91, 88], 'area': 8008, 'id': 73747}\n",
      "Invalid category_id 8 in image_id 22309, annotation: {'image_id': 22309, 'category_id': 8, 'bbox': [1389, 34, 24, 33], 'area': 792, 'id': 74758}\n",
      "Invalid category_id 10 in image_id 3459, annotation: {'image_id': 3459, 'category_id': 10, 'bbox': [1576, 349, 140, 57], 'area': 7980, 'id': 8825}\n",
      "Invalid category_id 8 in image_id 3459, annotation: {'image_id': 3459, 'category_id': 8, 'bbox': [882, 258, 116, 31], 'area': 3596, 'id': 8827}\n",
      "Invalid category_id 8 in image_id 19630, annotation: {'image_id': 19630, 'category_id': 8, 'bbox': [910, 26, 642, 231], 'area': 148302, 'id': 62996}\n",
      "Invalid category_id 8 in image_id 15673, annotation: {'image_id': 15673, 'category_id': 8, 'bbox': [672, 524, 313, 145], 'area': 45385, 'id': 49649}\n",
      "Invalid category_id 8 in image_id 5606, annotation: {'image_id': 5606, 'category_id': 8, 'bbox': [1415, 349, 106, 50], 'area': 5300, 'id': 15287}\n",
      "Invalid category_id 8 in image_id 5643, annotation: {'image_id': 5643, 'category_id': 8, 'bbox': [1833, 316, 36, 29], 'area': 1044, 'id': 15409}\n",
      "Invalid category_id 8 in image_id 4733, annotation: {'image_id': 4733, 'category_id': 8, 'bbox': [1287, 403, 150, 70], 'area': 10500, 'id': 12659}\n",
      "Invalid category_id 8 in image_id 4733, annotation: {'image_id': 4733, 'category_id': 8, 'bbox': [1635, 305, 108, 63], 'area': 6804, 'id': 12660}\n",
      "Invalid category_id 8 in image_id 4733, annotation: {'image_id': 4733, 'category_id': 8, 'bbox': [1806, 290, 90, 68], 'area': 6120, 'id': 12661}\n",
      "Invalid category_id 8 in image_id 14046, annotation: {'image_id': 14046, 'category_id': 8, 'bbox': [846, 304, 260, 266], 'area': 69160, 'id': 43579}\n",
      "Invalid category_id 10 in image_id 14046, annotation: {'image_id': 14046, 'category_id': 10, 'bbox': [278, 700, 287, 189], 'area': 54243, 'id': 43581}\n",
      "Invalid category_id 8 in image_id 3340, annotation: {'image_id': 3340, 'category_id': 8, 'bbox': [0, 403, 320, 177], 'area': 56640, 'id': 8486}\n",
      "Invalid category_id 8 in image_id 27322, annotation: {'image_id': 27322, 'category_id': 8, 'bbox': [1400, 445, 222, 136], 'area': 30192, 'id': 102195}\n",
      "Invalid category_id 8 in image_id 27322, annotation: {'image_id': 27322, 'category_id': 8, 'bbox': [1447, 224, 47, 32], 'area': 1504, 'id': 102208}\n",
      "Invalid category_id 8 in image_id 23078, annotation: {'image_id': 23078, 'category_id': 8, 'bbox': [328, 195, 108, 89], 'area': 9612, 'id': 79496}\n",
      "Invalid category_id 8 in image_id 23078, annotation: {'image_id': 23078, 'category_id': 8, 'bbox': [502, 50, 32, 52], 'area': 1664, 'id': 79502}\n",
      "Invalid category_id 10 in image_id 23078, annotation: {'image_id': 23078, 'category_id': 10, 'bbox': [1247, 116, 69, 65], 'area': 4485, 'id': 79503}\n",
      "Invalid category_id 8 in image_id 23078, annotation: {'image_id': 23078, 'category_id': 8, 'bbox': [1310, 134, 27, 44], 'area': 1188, 'id': 79504}\n",
      "Invalid category_id 8 in image_id 26209, annotation: {'image_id': 26209, 'category_id': 8, 'bbox': [0, 531, 657, 450], 'area': 295650, 'id': 96133}\n",
      "Invalid category_id 8 in image_id 29514, annotation: {'image_id': 29514, 'category_id': 8, 'bbox': [860, 666, 153, 184], 'area': 28152, 'id': 113692}\n",
      "Invalid category_id 8 in image_id 2998, annotation: {'image_id': 2998, 'category_id': 8, 'bbox': [0, 555, 118, 127], 'area': 14986, 'id': 7509}\n",
      "Invalid category_id 10 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 10, 'bbox': [329, 407, 238, 293], 'area': 69734, 'id': 114633}\n",
      "Invalid category_id 10 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 10, 'bbox': [433, 95, 46, 38], 'area': 1748, 'id': 114641}\n",
      "Invalid category_id 10 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 10, 'bbox': [486, 88, 39, 34], 'area': 1326, 'id': 114642}\n",
      "Invalid category_id 10 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 10, 'bbox': [575, 75, 42, 27], 'area': 1134, 'id': 114643}\n",
      "Invalid category_id 10 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 10, 'bbox': [553, 87, 26, 25], 'area': 650, 'id': 114644}\n",
      "Invalid category_id 10 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 10, 'bbox': [155, 88, 120, 61], 'area': 7320, 'id': 114646}\n",
      "Invalid category_id 8 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 8, 'bbox': [537, 57, 21, 20], 'area': 420, 'id': 114647}\n",
      "Invalid category_id 8 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 8, 'bbox': [466, 64, 73, 19], 'area': 1387, 'id': 114648}\n",
      "Invalid category_id 8 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 8, 'bbox': [813, 28, 24, 30], 'area': 720, 'id': 114649}\n",
      "Invalid category_id 8 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 8, 'bbox': [780, 20, 13, 21], 'area': 273, 'id': 114650}\n",
      "Invalid category_id 8 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 8, 'bbox': [756, 17, 14, 17], 'area': 238, 'id': 114651}\n",
      "Invalid category_id 8 in image_id 29669, annotation: {'image_id': 29669, 'category_id': 8, 'bbox': [728, 21, 20, 13], 'area': 260, 'id': 114652}\n",
      "Invalid category_id 8 in image_id 19715, annotation: {'image_id': 19715, 'category_id': 8, 'bbox': [1262, 306, 108, 47], 'area': 5076, 'id': 63260}\n",
      "Invalid category_id 8 in image_id 24452, annotation: {'image_id': 24452, 'category_id': 8, 'bbox': [1077, 305, 126, 97], 'area': 12222, 'id': 86899}\n",
      "Invalid category_id 8 in image_id 24452, annotation: {'image_id': 24452, 'category_id': 8, 'bbox': [490, 193, 100, 97], 'area': 9700, 'id': 86900}\n",
      "Invalid category_id 8 in image_id 27914, annotation: {'image_id': 27914, 'category_id': 8, 'bbox': [0, 595, 339, 444], 'area': 150516, 'id': 105302}\n",
      "Invalid category_id 8 in image_id 27914, annotation: {'image_id': 27914, 'category_id': 8, 'bbox': [207, 311, 236, 108], 'area': 25488, 'id': 105303}\n",
      "Invalid category_id 8 in image_id 25868, annotation: {'image_id': 25868, 'category_id': 8, 'bbox': [787, 264, 149, 95], 'area': 14155, 'id': 94334}\n",
      "Invalid category_id 8 in image_id 25868, annotation: {'image_id': 25868, 'category_id': 8, 'bbox': [1330, 55, 60, 79], 'area': 4740, 'id': 94335}\n",
      "Invalid category_id 8 in image_id 25868, annotation: {'image_id': 25868, 'category_id': 8, 'bbox': [662, 48, 159, 57], 'area': 9063, 'id': 94340}\n",
      "Invalid category_id 8 in image_id 15372, annotation: {'image_id': 15372, 'category_id': 8, 'bbox': [1684, 162, 232, 245], 'area': 56840, 'id': 48305}\n",
      "Invalid category_id 8 in image_id 30469, annotation: {'image_id': 30469, 'category_id': 8, 'bbox': [1506, 382, 365, 131], 'area': 47815, 'id': 119220}\n",
      "Invalid category_id 8 in image_id 9508, annotation: {'image_id': 9508, 'category_id': 8, 'bbox': [1046, 483, 416, 246], 'area': 102336, 'id': 27188}\n",
      "Invalid category_id 8 in image_id 9508, annotation: {'image_id': 9508, 'category_id': 8, 'bbox': [0, 316, 110, 163], 'area': 17930, 'id': 27189}\n",
      "Invalid category_id 8 in image_id 9508, annotation: {'image_id': 9508, 'category_id': 8, 'bbox': [1514, 173, 96, 152], 'area': 14592, 'id': 27191}\n",
      "Invalid category_id 8 in image_id 21480, annotation: {'image_id': 21480, 'category_id': 8, 'bbox': [1195, 311, 83, 60], 'area': 4980, 'id': 69619}\n",
      "Invalid category_id 8 in image_id 7105, annotation: {'image_id': 7105, 'category_id': 8, 'bbox': [953, 337, 98, 71], 'area': 6958, 'id': 19784}\n",
      "Invalid category_id 8 in image_id 31694, annotation: {'image_id': 31694, 'category_id': 8, 'bbox': [0, 334, 99, 109], 'area': 10791, 'id': 125912}\n",
      "Invalid category_id 8 in image_id 22922, annotation: {'image_id': 22922, 'category_id': 8, 'bbox': [784, 137, 31, 42], 'area': 1302, 'id': 78580}\n",
      "Invalid category_id 8 in image_id 22922, annotation: {'image_id': 22922, 'category_id': 8, 'bbox': [765, 135, 28, 26], 'area': 728, 'id': 78581}\n",
      "Invalid category_id 10 in image_id 31103, annotation: {'image_id': 31103, 'category_id': 10, 'bbox': [564, 615, 90, 46], 'area': 4140, 'id': 122647}\n",
      "Invalid category_id 8 in image_id 4065, annotation: {'image_id': 4065, 'category_id': 8, 'bbox': [548, 516, 180, 106], 'area': 19080, 'id': 10643}\n",
      "Invalid category_id 8 in image_id 30757, annotation: {'image_id': 30757, 'category_id': 8, 'bbox': [356, 351, 225, 157], 'area': 35325, 'id': 120867}\n",
      "Invalid category_id 8 in image_id 14156, annotation: {'image_id': 14156, 'category_id': 8, 'bbox': [218, 407, 193, 112], 'area': 21616, 'id': 44041}\n",
      "Invalid category_id 8 in image_id 9711, annotation: {'image_id': 9711, 'category_id': 8, 'bbox': [398, 593, 222, 95], 'area': 21090, 'id': 27985}\n",
      "Invalid category_id 8 in image_id 3529, annotation: {'image_id': 3529, 'category_id': 8, 'bbox': [1608, 329, 202, 110], 'area': 22220, 'id': 9043}\n",
      "Invalid category_id 8 in image_id 4615, annotation: {'image_id': 4615, 'category_id': 8, 'bbox': [379, 376, 192, 85], 'area': 16320, 'id': 12286}\n",
      "Invalid category_id 8 in image_id 19764, annotation: {'image_id': 19764, 'category_id': 8, 'bbox': [1468, 183, 193, 136], 'area': 26248, 'id': 63450}\n",
      "Invalid category_id 8 in image_id 19764, annotation: {'image_id': 19764, 'category_id': 8, 'bbox': [1680, 86, 124, 69], 'area': 8556, 'id': 63451}\n",
      "Invalid category_id 8 in image_id 6182, annotation: {'image_id': 6182, 'category_id': 8, 'bbox': [1215, 396, 147, 143], 'area': 21021, 'id': 17040}\n",
      "Invalid category_id 8 in image_id 3619, annotation: {'image_id': 3619, 'category_id': 8, 'bbox': [254, 501, 544, 202], 'area': 109888, 'id': 9307}\n",
      "Invalid category_id 8 in image_id 3619, annotation: {'image_id': 3619, 'category_id': 8, 'bbox': [31, 550, 232, 192], 'area': 44544, 'id': 9308}\n",
      "Invalid category_id 8 in image_id 14669, annotation: {'image_id': 14669, 'category_id': 8, 'bbox': [584, 58, 169, 135], 'area': 22815, 'id': 45858}\n",
      "Invalid category_id 10 in image_id 23220, annotation: {'image_id': 23220, 'category_id': 10, 'bbox': [1534, 651, 386, 429], 'area': 165594, 'id': 80344}\n",
      "Invalid category_id 8 in image_id 18780, annotation: {'image_id': 18780, 'category_id': 8, 'bbox': [547, 508, 406, 313], 'area': 127078, 'id': 60047}\n",
      "Invalid category_id 8 in image_id 15850, annotation: {'image_id': 15850, 'category_id': 8, 'bbox': [1585, 71, 90, 81], 'area': 7290, 'id': 50411}\n",
      "Invalid category_id 10 in image_id 26521, annotation: {'image_id': 26521, 'category_id': 10, 'bbox': [452, 66, 43, 22], 'area': 946, 'id': 97869}\n",
      "Invalid category_id 8 in image_id 26521, annotation: {'image_id': 26521, 'category_id': 8, 'bbox': [517, 43, 20, 22], 'area': 440, 'id': 97871}\n",
      "Invalid category_id 8 in image_id 6374, annotation: {'image_id': 6374, 'category_id': 8, 'bbox': [1347, 327, 183, 67], 'area': 12261, 'id': 17643}\n",
      "Invalid category_id 8 in image_id 13941, annotation: {'image_id': 13941, 'category_id': 8, 'bbox': [1028, 218, 155, 101], 'area': 15655, 'id': 43173}\n",
      "Invalid category_id 8 in image_id 26760, annotation: {'image_id': 26760, 'category_id': 8, 'bbox': [1413, 423, 208, 153], 'area': 31824, 'id': 99127}\n",
      "Invalid category_id 10 in image_id 26760, annotation: {'image_id': 26760, 'category_id': 10, 'bbox': [1160, 464, 261, 105], 'area': 27405, 'id': 99128}\n",
      "Invalid category_id 10 in image_id 30926, annotation: {'image_id': 30926, 'category_id': 10, 'bbox': [1101, 78, 440, 76], 'area': 33440, 'id': 121755}\n",
      "Invalid category_id 8 in image_id 25621, annotation: {'image_id': 25621, 'category_id': 8, 'bbox': [428, 79, 96, 87], 'area': 8352, 'id': 92488}\n",
      "Invalid category_id 8 in image_id 10754, annotation: {'image_id': 10754, 'category_id': 8, 'bbox': [1500, 65, 51, 56], 'area': 2856, 'id': 31708}\n",
      "Invalid category_id 8 in image_id 5272, annotation: {'image_id': 5272, 'category_id': 8, 'bbox': [1230, 414, 118, 68], 'area': 8024, 'id': 14265}\n",
      "Invalid category_id 8 in image_id 24274, annotation: {'image_id': 24274, 'category_id': 8, 'bbox': [635, 333, 133, 109], 'area': 14497, 'id': 86098}\n",
      "Invalid category_id 8 in image_id 19815, annotation: {'image_id': 19815, 'category_id': 8, 'bbox': [1485, 67, 76, 109], 'area': 8284, 'id': 63675}\n",
      "Invalid category_id 8 in image_id 4197, annotation: {'image_id': 4197, 'category_id': 8, 'bbox': [1605, 510, 142, 91], 'area': 12922, 'id': 10988}\n",
      "Invalid category_id 8 in image_id 21885, annotation: {'image_id': 21885, 'category_id': 8, 'bbox': [1345, 45, 169, 149], 'area': 25181, 'id': 72061}\n",
      "Invalid category_id 8 in image_id 30300, annotation: {'image_id': 30300, 'category_id': 8, 'bbox': [216, 328, 121, 92], 'area': 11132, 'id': 118165}\n",
      "Invalid category_id 8 in image_id 30300, annotation: {'image_id': 30300, 'category_id': 8, 'bbox': [1867, 609, 53, 175], 'area': 9275, 'id': 118173}\n",
      "Invalid category_id 8 in image_id 8006, annotation: {'image_id': 8006, 'category_id': 8, 'bbox': [257, 503, 354, 178], 'area': 63012, 'id': 22335}\n",
      "Invalid category_id 8 in image_id 14592, annotation: {'image_id': 14592, 'category_id': 8, 'bbox': [437, 488, 67, 88], 'area': 5896, 'id': 45572}\n",
      "Invalid category_id 8 in image_id 14199, annotation: {'image_id': 14199, 'category_id': 8, 'bbox': [885, 668, 582, 401], 'area': 233382, 'id': 44186}\n",
      "Invalid category_id 8 in image_id 13627, annotation: {'image_id': 13627, 'category_id': 8, 'bbox': [0, 380, 345, 195], 'area': 67275, 'id': 42065}\n",
      "Invalid category_id 8 in image_id 13627, annotation: {'image_id': 13627, 'category_id': 8, 'bbox': [648, 334, 67, 44], 'area': 2948, 'id': 42067}\n",
      "Invalid category_id 8 in image_id 13627, annotation: {'image_id': 13627, 'category_id': 8, 'bbox': [1570, 239, 90, 74], 'area': 6660, 'id': 42070}\n",
      "Invalid category_id 8 in image_id 14124, annotation: {'image_id': 14124, 'category_id': 8, 'bbox': [0, 333, 458, 284], 'area': 130072, 'id': 43885}\n",
      "Invalid category_id 8 in image_id 30861, annotation: {'image_id': 30861, 'category_id': 8, 'bbox': [1223, 406, 152, 175], 'area': 26600, 'id': 121401}\n",
      "Invalid category_id 8 in image_id 22990, annotation: {'image_id': 22990, 'category_id': 8, 'bbox': [577, 259, 230, 103], 'area': 23690, 'id': 78971}\n",
      "Invalid category_id 8 in image_id 15009, annotation: {'image_id': 15009, 'category_id': 8, 'bbox': [1331, 428, 328, 343], 'area': 112504, 'id': 46972}\n",
      "Invalid category_id 8 in image_id 8397, annotation: {'image_id': 8397, 'category_id': 8, 'bbox': [1419, 159, 426, 369], 'area': 157194, 'id': 23443}\n",
      "Invalid category_id 8 in image_id 6601, annotation: {'image_id': 6601, 'category_id': 8, 'bbox': [269, 375, 157, 171], 'area': 26847, 'id': 18330}\n",
      "Invalid category_id 8 in image_id 21125, annotation: {'image_id': 21125, 'category_id': 8, 'bbox': [597, 194, 172, 59], 'area': 10148, 'id': 67815}\n",
      "Invalid category_id 8 in image_id 30009, annotation: {'image_id': 30009, 'category_id': 8, 'bbox': [1588, 238, 329, 185], 'area': 60865, 'id': 116455}\n",
      "Invalid category_id 8 in image_id 8566, annotation: {'image_id': 8566, 'category_id': 8, 'bbox': [104, 108, 292, 187], 'area': 54604, 'id': 23961}\n",
      "Invalid category_id 8 in image_id 10477, annotation: {'image_id': 10477, 'category_id': 8, 'bbox': [441, 307, 194, 89], 'area': 17266, 'id': 30746}\n",
      "Invalid category_id 8 in image_id 8109, annotation: {'image_id': 8109, 'category_id': 8, 'bbox': [1124, 493, 154, 139], 'area': 21406, 'id': 22568}\n",
      "Invalid category_id 8 in image_id 14991, annotation: {'image_id': 14991, 'category_id': 8, 'bbox': [1121, 126, 98, 138], 'area': 13524, 'id': 46938}\n",
      "Invalid category_id 8 in image_id 23629, annotation: {'image_id': 23629, 'category_id': 8, 'bbox': [304, 27, 405, 321], 'area': 130005, 'id': 82595}\n",
      "Invalid category_id 10 in image_id 11218, annotation: {'image_id': 11218, 'category_id': 10, 'bbox': [1003, 228, 187, 112], 'area': 20944, 'id': 33388}\n",
      "Invalid category_id 8 in image_id 11218, annotation: {'image_id': 11218, 'category_id': 8, 'bbox': [1390, 159, 55, 51], 'area': 2805, 'id': 33390}\n",
      "Invalid category_id 8 in image_id 9070, annotation: {'image_id': 9070, 'category_id': 8, 'bbox': [0, 559, 609, 242], 'area': 147378, 'id': 25748}\n",
      "Invalid category_id 8 in image_id 7556, annotation: {'image_id': 7556, 'category_id': 8, 'bbox': [1303, 377, 211, 161], 'area': 33971, 'id': 21038}\n",
      "Invalid category_id 8 in image_id 21107, annotation: {'image_id': 21107, 'category_id': 8, 'bbox': [1758, 26, 98, 101], 'area': 9898, 'id': 67740}\n",
      "Invalid category_id 8 in image_id 21107, annotation: {'image_id': 21107, 'category_id': 8, 'bbox': [594, 144, 74, 72], 'area': 5328, 'id': 67742}\n",
      "Invalid category_id 8 in image_id 25896, annotation: {'image_id': 25896, 'category_id': 8, 'bbox': [1110, 348, 496, 175], 'area': 86800, 'id': 94472}\n",
      "Invalid category_id 8 in image_id 14300, annotation: {'image_id': 14300, 'category_id': 8, 'bbox': [956, 229, 267, 187], 'area': 49929, 'id': 44502}\n",
      "Invalid category_id 8 in image_id 5533, annotation: {'image_id': 5533, 'category_id': 8, 'bbox': [1015, 369, 95, 44], 'area': 4180, 'id': 15041}\n",
      "Invalid category_id 8 in image_id 27280, annotation: {'image_id': 27280, 'category_id': 8, 'bbox': [625, 483, 345, 202], 'area': 69690, 'id': 101981}\n",
      "Invalid category_id 10 in image_id 27280, annotation: {'image_id': 27280, 'category_id': 10, 'bbox': [225, 578, 382, 187], 'area': 71434, 'id': 101982}\n",
      "Invalid category_id 8 in image_id 29748, annotation: {'image_id': 29748, 'category_id': 8, 'bbox': [1391, 392, 258, 182], 'area': 46956, 'id': 115106}\n",
      "Invalid category_id 8 in image_id 9126, annotation: {'image_id': 9126, 'category_id': 8, 'bbox': [807, 232, 56, 119], 'area': 6664, 'id': 25908}\n",
      "Invalid category_id 10 in image_id 12818, annotation: {'image_id': 12818, 'category_id': 10, 'bbox': [1170, 185, 80, 76], 'area': 6080, 'id': 39043}\n",
      "Invalid category_id 8 in image_id 25022, annotation: {'image_id': 25022, 'category_id': 8, 'bbox': [887, 317, 543, 383], 'area': 207969, 'id': 89280}\n",
      "Invalid category_id 8 in image_id 7183, annotation: {'image_id': 7183, 'category_id': 8, 'bbox': [553, 432, 595, 197], 'area': 117215, 'id': 20023}\n",
      "Invalid category_id 8 in image_id 4450, annotation: {'image_id': 4450, 'category_id': 8, 'bbox': [1200, 513, 525, 439], 'area': 230475, 'id': 11728}\n",
      "Invalid category_id 8 in image_id 31110, annotation: {'image_id': 31110, 'category_id': 8, 'bbox': [380, 379, 167, 121], 'area': 20207, 'id': 122698}\n",
      "Invalid category_id 10 in image_id 24442, annotation: {'image_id': 24442, 'category_id': 10, 'bbox': [791, 175, 441, 156], 'area': 68796, 'id': 86860}\n",
      "Invalid category_id 8 in image_id 14394, annotation: {'image_id': 14394, 'category_id': 8, 'bbox': [1707, 157, 54, 48], 'area': 2592, 'id': 44860}\n",
      "Invalid category_id 10 in image_id 14394, annotation: {'image_id': 14394, 'category_id': 10, 'bbox': [1489, 239, 37, 22], 'area': 814, 'id': 44863}\n",
      "Invalid category_id 10 in image_id 9565, annotation: {'image_id': 9565, 'category_id': 10, 'bbox': [0, 260, 89, 178], 'area': 15842, 'id': 27476}\n",
      "Invalid category_id 10 in image_id 9565, annotation: {'image_id': 9565, 'category_id': 10, 'bbox': [1512, 182, 88, 132], 'area': 11616, 'id': 27500}\n",
      "Invalid category_id 8 in image_id 28382, annotation: {'image_id': 28382, 'category_id': 8, 'bbox': [163, 35, 337, 391], 'area': 131767, 'id': 107685}\n",
      "Invalid category_id 10 in image_id 19991, annotation: {'image_id': 19991, 'category_id': 10, 'bbox': [511, 339, 246, 133], 'area': 32718, 'id': 64216}\n",
      "Invalid category_id 8 in image_id 19991, annotation: {'image_id': 19991, 'category_id': 8, 'bbox': [997, 268, 180, 195], 'area': 35100, 'id': 64217}\n",
      "Invalid category_id 8 in image_id 25469, annotation: {'image_id': 25469, 'category_id': 8, 'bbox': [752, 209, 35, 46], 'area': 1610, 'id': 91658}\n",
      "Invalid category_id 10 in image_id 25469, annotation: {'image_id': 25469, 'category_id': 10, 'bbox': [706, 219, 48, 43], 'area': 2064, 'id': 91659}\n",
      "Invalid category_id 8 in image_id 28839, annotation: {'image_id': 28839, 'category_id': 8, 'bbox': [918, 0, 313, 103], 'area': 32239, 'id': 110100}\n",
      "Invalid category_id 8 in image_id 7381, annotation: {'image_id': 7381, 'category_id': 8, 'bbox': [1391, 509, 91, 66], 'area': 6006, 'id': 20527}\n",
      "Invalid category_id 8 in image_id 15301, annotation: {'image_id': 15301, 'category_id': 8, 'bbox': [225, 230, 483, 248], 'area': 119784, 'id': 47962}\n",
      "Invalid category_id 8 in image_id 25752, annotation: {'image_id': 25752, 'category_id': 8, 'bbox': [282, 339, 110, 97], 'area': 10670, 'id': 93583}\n",
      "Invalid category_id 8 in image_id 5033, annotation: {'image_id': 5033, 'category_id': 8, 'bbox': [329, 425, 149, 163], 'area': 24287, 'id': 13601}\n",
      "Invalid category_id 8 in image_id 5033, annotation: {'image_id': 5033, 'category_id': 8, 'bbox': [1363, 345, 132, 61], 'area': 8052, 'id': 13606}\n",
      "Invalid category_id 8 in image_id 5033, annotation: {'image_id': 5033, 'category_id': 8, 'bbox': [1146, 345, 189, 80], 'area': 15120, 'id': 13607}\n",
      "Invalid category_id 8 in image_id 29826, annotation: {'image_id': 29826, 'category_id': 8, 'bbox': [1128, 400, 125, 146], 'area': 18250, 'id': 115515}\n",
      "Invalid category_id 8 in image_id 26174, annotation: {'image_id': 26174, 'category_id': 8, 'bbox': [586, 637, 119, 143], 'area': 17017, 'id': 95952}\n",
      "Invalid category_id 10 in image_id 26174, annotation: {'image_id': 26174, 'category_id': 10, 'bbox': [140, 650, 430, 399], 'area': 171570, 'id': 95953}\n",
      "Invalid category_id 8 in image_id 29423, annotation: {'image_id': 29423, 'category_id': 8, 'bbox': [9, 211, 277, 164], 'area': 45428, 'id': 113183}\n",
      "Invalid category_id 8 in image_id 21465, annotation: {'image_id': 21465, 'category_id': 8, 'bbox': [1197, 62, 104, 105], 'area': 10920, 'id': 69489}\n",
      "Invalid category_id 8 in image_id 27412, annotation: {'image_id': 27412, 'category_id': 8, 'bbox': [1472, 429, 168, 60], 'area': 10080, 'id': 102693}\n",
      "Invalid category_id 10 in image_id 10134, annotation: {'image_id': 10134, 'category_id': 10, 'bbox': [1191, 482, 606, 370], 'area': 224220, 'id': 29492}\n",
      "Invalid category_id 8 in image_id 4221, annotation: {'image_id': 4221, 'category_id': 8, 'bbox': [937, 449, 478, 225], 'area': 107550, 'id': 11050}\n",
      "Invalid category_id 8 in image_id 5685, annotation: {'image_id': 5685, 'category_id': 8, 'bbox': [1243, 499, 677, 509], 'area': 344593, 'id': 15534}\n",
      "Invalid category_id 10 in image_id 20112, annotation: {'image_id': 20112, 'category_id': 10, 'bbox': [64, 446, 106, 49], 'area': 5194, 'id': 64714}\n",
      "Invalid category_id 8 in image_id 7927, annotation: {'image_id': 7927, 'category_id': 8, 'bbox': [1131, 507, 226, 107], 'area': 24182, 'id': 22088}\n",
      "Invalid category_id 8 in image_id 28267, annotation: {'image_id': 28267, 'category_id': 8, 'bbox': [1652, 263, 268, 144], 'area': 38592, 'id': 107161}\n",
      "Invalid category_id 8 in image_id 15427, annotation: {'image_id': 15427, 'category_id': 8, 'bbox': [1015, 431, 229, 190], 'area': 43510, 'id': 48538}\n",
      "Invalid category_id 8 in image_id 17609, annotation: {'image_id': 17609, 'category_id': 8, 'bbox': [481, 301, 122, 263], 'area': 32086, 'id': 56276}\n",
      "Invalid category_id 8 in image_id 17609, annotation: {'image_id': 17609, 'category_id': 8, 'bbox': [1158, 192, 188, 90], 'area': 16920, 'id': 56278}\n",
      "Invalid category_id 8 in image_id 24044, annotation: {'image_id': 24044, 'category_id': 8, 'bbox': [340, 105, 106, 143], 'area': 15158, 'id': 84843}\n",
      "Invalid category_id 8 in image_id 2797, annotation: {'image_id': 2797, 'category_id': 8, 'bbox': [1816, 286, 98, 53], 'area': 5194, 'id': 6936}\n",
      "Invalid category_id 8 in image_id 31605, annotation: {'image_id': 31605, 'category_id': 8, 'bbox': [868, 360, 528, 154], 'area': 81312, 'id': 125319}\n",
      "Invalid category_id 10 in image_id 30432, annotation: {'image_id': 30432, 'category_id': 10, 'bbox': [702, 61, 47, 22], 'area': 1034, 'id': 118986}\n",
      "Invalid category_id 10 in image_id 30432, annotation: {'image_id': 30432, 'category_id': 10, 'bbox': [756, 72, 30, 24], 'area': 720, 'id': 118987}\n",
      "Invalid category_id 10 in image_id 30432, annotation: {'image_id': 30432, 'category_id': 10, 'bbox': [657, 86, 41, 23], 'area': 943, 'id': 118988}\n",
      "Invalid category_id 10 in image_id 30432, annotation: {'image_id': 30432, 'category_id': 10, 'bbox': [619, 86, 33, 27], 'area': 891, 'id': 118989}\n",
      "Invalid category_id 10 in image_id 22872, annotation: {'image_id': 22872, 'category_id': 10, 'bbox': [1076, 746, 90, 64], 'area': 5760, 'id': 78312}\n",
      "Invalid category_id 8 in image_id 22872, annotation: {'image_id': 22872, 'category_id': 8, 'bbox': [723, 137, 83, 132], 'area': 10956, 'id': 78313}\n",
      "Invalid category_id 8 in image_id 12246, annotation: {'image_id': 12246, 'category_id': 8, 'bbox': [212, 815, 301, 155], 'area': 46655, 'id': 36945}\n",
      "Invalid category_id 8 in image_id 18783, annotation: {'image_id': 18783, 'category_id': 8, 'bbox': [1243, 500, 341, 299], 'area': 101959, 'id': 60050}\n",
      "Invalid category_id 8 in image_id 31118, annotation: {'image_id': 31118, 'category_id': 8, 'bbox': [1294, 456, 211, 105], 'area': 22155, 'id': 122748}\n",
      "Invalid category_id 8 in image_id 14303, annotation: {'image_id': 14303, 'category_id': 8, 'bbox': [177, 561, 376, 163], 'area': 61288, 'id': 44509}\n",
      "Invalid category_id 8 in image_id 14303, annotation: {'image_id': 14303, 'category_id': 8, 'bbox': [165, 428, 171, 118], 'area': 20178, 'id': 44510}\n",
      "Invalid category_id 10 in image_id 26035, annotation: {'image_id': 26035, 'category_id': 10, 'bbox': [286, 212, 60, 31], 'area': 1860, 'id': 95176}\n",
      "Invalid category_id 8 in image_id 29936, annotation: {'image_id': 29936, 'category_id': 8, 'bbox': [456, 460, 422, 242], 'area': 102124, 'id': 116058}\n",
      "Invalid category_id 8 in image_id 24786, annotation: {'image_id': 24786, 'category_id': 8, 'bbox': [1207, 292, 53, 83], 'area': 4399, 'id': 88304}\n",
      "Invalid category_id 10 in image_id 24786, annotation: {'image_id': 24786, 'category_id': 10, 'bbox': [1250, 263, 139, 110], 'area': 15290, 'id': 88305}\n",
      "Invalid category_id 10 in image_id 24786, annotation: {'image_id': 24786, 'category_id': 10, 'bbox': [1386, 261, 120, 98], 'area': 11760, 'id': 88306}\n",
      "Invalid category_id 8 in image_id 18277, annotation: {'image_id': 18277, 'category_id': 8, 'bbox': [965, 233, 185, 98], 'area': 18130, 'id': 58366}\n",
      "Invalid category_id 8 in image_id 30754, annotation: {'image_id': 30754, 'category_id': 8, 'bbox': [1335, 481, 300, 261], 'area': 78300, 'id': 120843}\n",
      "Invalid category_id 8 in image_id 8201, annotation: {'image_id': 8201, 'category_id': 8, 'bbox': [1755, 286, 93, 82], 'area': 7626, 'id': 22837}\n",
      "Invalid category_id 8 in image_id 8201, annotation: {'image_id': 8201, 'category_id': 8, 'bbox': [1852, 275, 68, 65], 'area': 4420, 'id': 22838}\n",
      "Invalid category_id 8 in image_id 8201, annotation: {'image_id': 8201, 'category_id': 8, 'bbox': [963, 340, 55, 67], 'area': 3685, 'id': 22839}\n",
      "Invalid category_id 10 in image_id 23527, annotation: {'image_id': 23527, 'category_id': 10, 'bbox': [857, 306, 235, 149], 'area': 35015, 'id': 82061}\n",
      "Invalid category_id 8 in image_id 19572, annotation: {'image_id': 19572, 'category_id': 8, 'bbox': [804, 73, 558, 173], 'area': 96534, 'id': 62824}\n",
      "Invalid category_id 8 in image_id 17561, annotation: {'image_id': 17561, 'category_id': 8, 'bbox': [46, 75, 659, 268], 'area': 176612, 'id': 56112}\n",
      "Invalid category_id 10 in image_id 31269, annotation: {'image_id': 31269, 'category_id': 10, 'bbox': [306, 485, 202, 70], 'area': 14140, 'id': 123555}\n",
      "Invalid category_id 8 in image_id 4222, annotation: {'image_id': 4222, 'category_id': 8, 'bbox': [170, 349, 658, 242], 'area': 159236, 'id': 11053}\n",
      "Invalid category_id 8 in image_id 5357, annotation: {'image_id': 5357, 'category_id': 8, 'bbox': [476, 525, 452, 137], 'area': 61924, 'id': 14495}\n",
      "Invalid category_id 8 in image_id 19516, annotation: {'image_id': 19516, 'category_id': 8, 'bbox': [1269, 292, 127, 65], 'area': 8255, 'id': 62642}\n",
      "Invalid category_id 8 in image_id 21809, annotation: {'image_id': 21809, 'category_id': 8, 'bbox': [962, 805, 399, 249], 'area': 99351, 'id': 71602}\n",
      "Invalid category_id 10 in image_id 13309, annotation: {'image_id': 13309, 'category_id': 10, 'bbox': [532, 406, 56, 26], 'area': 1456, 'id': 40956}\n",
      "Invalid category_id 8 in image_id 13309, annotation: {'image_id': 13309, 'category_id': 8, 'bbox': [0, 597, 230, 167], 'area': 38410, 'id': 40961}\n",
      "Invalid category_id 8 in image_id 17938, annotation: {'image_id': 17938, 'category_id': 8, 'bbox': [270, 142, 364, 938], 'area': 341432, 'id': 57303}\n",
      "Invalid category_id 10 in image_id 21542, annotation: {'image_id': 21542, 'category_id': 10, 'bbox': [215, 614, 240, 118], 'area': 28320, 'id': 70023}\n",
      "Invalid category_id 8 in image_id 7771, annotation: {'image_id': 7771, 'category_id': 8, 'bbox': [707, 590, 790, 490], 'area': 387100, 'id': 21636}\n",
      "Invalid category_id 8 in image_id 4084, annotation: {'image_id': 4084, 'category_id': 8, 'bbox': [664, 498, 114, 96], 'area': 10944, 'id': 10686}\n",
      "Invalid category_id 8 in image_id 19993, annotation: {'image_id': 19993, 'category_id': 8, 'bbox': [818, 145, 201, 121], 'area': 24321, 'id': 64224}\n",
      "Invalid category_id 8 in image_id 27037, annotation: {'image_id': 27037, 'category_id': 8, 'bbox': [10, 333, 49, 76], 'area': 3724, 'id': 100666}\n",
      "Invalid category_id 8 in image_id 11889, annotation: {'image_id': 11889, 'category_id': 8, 'bbox': [1645, 772, 275, 284], 'area': 78100, 'id': 35767}\n",
      "Invalid category_id 8 in image_id 13076, annotation: {'image_id': 13076, 'category_id': 8, 'bbox': [1729, 122, 165, 112], 'area': 18480, 'id': 40059}\n",
      "Invalid category_id 8 in image_id 3738, annotation: {'image_id': 3738, 'category_id': 8, 'bbox': [1317, 366, 103, 89], 'area': 9167, 'id': 9675}\n",
      "Invalid category_id 10 in image_id 10659, annotation: {'image_id': 10659, 'category_id': 10, 'bbox': [1039, 748, 756, 332], 'area': 250992, 'id': 31325}\n",
      "Invalid category_id 8 in image_id 15106, annotation: {'image_id': 15106, 'category_id': 8, 'bbox': [798, 513, 272, 152], 'area': 41344, 'id': 47255}\n",
      "Invalid category_id 10 in image_id 5665, annotation: {'image_id': 5665, 'category_id': 10, 'bbox': [1426, 339, 71, 77], 'area': 5467, 'id': 15468}\n",
      "Invalid category_id 10 in image_id 31040, annotation: {'image_id': 31040, 'category_id': 10, 'bbox': [978, 320, 111, 151], 'area': 16761, 'id': 122272}\n",
      "Invalid category_id 8 in image_id 15684, annotation: {'image_id': 15684, 'category_id': 8, 'bbox': [78, 654, 837, 426], 'area': 356562, 'id': 49715}\n",
      "Invalid category_id 8 in image_id 15684, annotation: {'image_id': 15684, 'category_id': 8, 'bbox': [480, 242, 192, 194], 'area': 37248, 'id': 49716}\n",
      "Invalid category_id 8 in image_id 24746, annotation: {'image_id': 24746, 'category_id': 8, 'bbox': [499, 206, 62, 67], 'area': 4154, 'id': 88169}\n",
      "Invalid category_id 10 in image_id 24746, annotation: {'image_id': 24746, 'category_id': 10, 'bbox': [470, 150, 68, 91], 'area': 6188, 'id': 88170}\n",
      "Invalid category_id 8 in image_id 12530, annotation: {'image_id': 12530, 'category_id': 8, 'bbox': [419, 627, 302, 206], 'area': 62212, 'id': 37981}\n",
      "Invalid category_id 8 in image_id 12530, annotation: {'image_id': 12530, 'category_id': 8, 'bbox': [997, 263, 116, 82], 'area': 9512, 'id': 37982}\n",
      "Invalid category_id 8 in image_id 29637, annotation: {'image_id': 29637, 'category_id': 8, 'bbox': [1489, 528, 51, 31], 'area': 1581, 'id': 114423}\n",
      "Invalid category_id 8 in image_id 29637, annotation: {'image_id': 29637, 'category_id': 8, 'bbox': [437, 188, 63, 27], 'area': 1701, 'id': 114431}\n",
      "Invalid category_id 8 in image_id 29637, annotation: {'image_id': 29637, 'category_id': 8, 'bbox': [492, 186, 27, 23], 'area': 621, 'id': 114432}\n",
      "Invalid category_id 8 in image_id 13424, annotation: {'image_id': 13424, 'category_id': 8, 'bbox': [1390, 353, 103, 103], 'area': 10609, 'id': 41379}\n",
      "Invalid category_id 8 in image_id 3774, annotation: {'image_id': 3774, 'category_id': 8, 'bbox': [1261, 422, 119, 112], 'area': 13328, 'id': 9787}\n",
      "Invalid category_id 8 in image_id 13008, annotation: {'image_id': 13008, 'category_id': 8, 'bbox': [995, 419, 623, 426], 'area': 265398, 'id': 39783}\n",
      "Invalid category_id 8 in image_id 13008, annotation: {'image_id': 13008, 'category_id': 8, 'bbox': [1688, 723, 232, 346], 'area': 80272, 'id': 39784}\n",
      "Invalid category_id 8 in image_id 13008, annotation: {'image_id': 13008, 'category_id': 8, 'bbox': [1501, 125, 145, 160], 'area': 23200, 'id': 39785}\n",
      "Invalid category_id 8 in image_id 16945, annotation: {'image_id': 16945, 'category_id': 8, 'bbox': [750, 308, 228, 109], 'area': 24852, 'id': 54224}\n",
      "Invalid category_id 10 in image_id 4820, annotation: {'image_id': 4820, 'category_id': 10, 'bbox': [1198, 415, 179, 203], 'area': 36337, 'id': 12964}\n",
      "Invalid category_id 8 in image_id 4820, annotation: {'image_id': 4820, 'category_id': 8, 'bbox': [1170, 448, 261, 145], 'area': 37845, 'id': 12965}\n",
      "Invalid category_id 8 in image_id 9825, annotation: {'image_id': 9825, 'category_id': 8, 'bbox': [1031, 545, 756, 416], 'area': 314496, 'id': 28346}\n",
      "Invalid category_id 8 in image_id 5934, annotation: {'image_id': 5934, 'category_id': 8, 'bbox': [1367, 515, 88, 30], 'area': 2640, 'id': 16298}\n",
      "Invalid category_id 8 in image_id 22750, annotation: {'image_id': 22750, 'category_id': 8, 'bbox': [590, 351, 327, 137], 'area': 44799, 'id': 77543}\n",
      "Invalid category_id 8 in image_id 12435, annotation: {'image_id': 12435, 'category_id': 8, 'bbox': [891, 553, 158, 150], 'area': 23700, 'id': 37620}\n",
      "Invalid category_id 8 in image_id 12435, annotation: {'image_id': 12435, 'category_id': 8, 'bbox': [1443, 579, 477, 403], 'area': 192231, 'id': 37622}\n",
      "Invalid category_id 8 in image_id 22830, annotation: {'image_id': 22830, 'category_id': 8, 'bbox': [1416, 312, 120, 107], 'area': 12840, 'id': 77990}\n",
      "Invalid category_id 8 in image_id 22830, annotation: {'image_id': 22830, 'category_id': 8, 'bbox': [332, 436, 418, 163], 'area': 68134, 'id': 77996}\n",
      "Invalid category_id 10 in image_id 22649, annotation: {'image_id': 22649, 'category_id': 10, 'bbox': [494, 516, 43, 99], 'area': 4257, 'id': 76898}\n",
      "Invalid category_id 8 in image_id 32039, annotation: {'image_id': 32039, 'category_id': 8, 'bbox': [0, 257, 115, 152], 'area': 17480, 'id': 127840}\n",
      "Invalid category_id 10 in image_id 22899, annotation: {'image_id': 22899, 'category_id': 10, 'bbox': [445, 349, 100, 107], 'area': 10700, 'id': 78442}\n",
      "Invalid category_id 8 in image_id 12793, annotation: {'image_id': 12793, 'category_id': 8, 'bbox': [24, 749, 3, 41], 'area': 123, 'id': 38929}\n",
      "Invalid category_id 8 in image_id 12793, annotation: {'image_id': 12793, 'category_id': 8, 'bbox': [10, 760, 433, 226], 'area': 97858, 'id': 38930}\n",
      "Invalid category_id 8 in image_id 12793, annotation: {'image_id': 12793, 'category_id': 8, 'bbox': [1449, 252, 172, 189], 'area': 32508, 'id': 38931}\n",
      "Invalid category_id 8 in image_id 23009, annotation: {'image_id': 23009, 'category_id': 8, 'bbox': [1296, 112, 103, 158], 'area': 16274, 'id': 79093}\n",
      "Invalid category_id 8 in image_id 9140, annotation: {'image_id': 9140, 'category_id': 8, 'bbox': [1208, 299, 356, 290], 'area': 103240, 'id': 25985}\n",
      "Invalid category_id 8 in image_id 15214, annotation: {'image_id': 15214, 'category_id': 8, 'bbox': [285, 733, 559, 270], 'area': 150930, 'id': 47622}\n",
      "Invalid category_id 8 in image_id 8587, annotation: {'image_id': 8587, 'category_id': 8, 'bbox': [879, 486, 263, 139], 'area': 36557, 'id': 24027}\n",
      "Invalid category_id 8 in image_id 13863, annotation: {'image_id': 13863, 'category_id': 8, 'bbox': [1359, 339, 201, 318], 'area': 63918, 'id': 42864}\n",
      "Invalid category_id 8 in image_id 23177, annotation: {'image_id': 23177, 'category_id': 8, 'bbox': [851, 307, 432, 184], 'area': 79488, 'id': 80030}\n",
      "Invalid category_id 8 in image_id 22205, annotation: {'image_id': 22205, 'category_id': 8, 'bbox': [319, 510, 571, 258], 'area': 147318, 'id': 74058}\n",
      "Invalid category_id 8 in image_id 6898, annotation: {'image_id': 6898, 'category_id': 8, 'bbox': [15, 593, 471, 260], 'area': 122460, 'id': 19181}\n",
      "Invalid category_id 8 in image_id 30175, annotation: {'image_id': 30175, 'category_id': 8, 'bbox': [0, 274, 147, 98], 'area': 14406, 'id': 117443}\n",
      "Invalid category_id 8 in image_id 8978, annotation: {'image_id': 8978, 'category_id': 8, 'bbox': [600, 196, 94, 158], 'area': 14852, 'id': 25408}\n",
      "Invalid category_id 8 in image_id 31538, annotation: {'image_id': 31538, 'category_id': 8, 'bbox': [1176, 385, 59, 105], 'area': 6195, 'id': 124968}\n",
      "Invalid category_id 10 in image_id 31538, annotation: {'image_id': 31538, 'category_id': 10, 'bbox': [1243, 398, 262, 118], 'area': 30916, 'id': 124969}\n",
      "Invalid category_id 10 in image_id 17671, annotation: {'image_id': 17671, 'category_id': 10, 'bbox': [1662, 123, 153, 103], 'area': 15759, 'id': 56502}\n",
      "Invalid category_id 8 in image_id 21083, annotation: {'image_id': 21083, 'category_id': 8, 'bbox': [1657, 50, 147, 99], 'area': 14553, 'id': 67681}\n",
      "Invalid category_id 8 in image_id 16448, annotation: {'image_id': 16448, 'category_id': 8, 'bbox': [1845, 40, 69, 68], 'area': 4692, 'id': 52629}\n",
      "Invalid category_id 8 in image_id 5042, annotation: {'image_id': 5042, 'category_id': 8, 'bbox': [1405, 691, 470, 352], 'area': 165440, 'id': 13639}\n",
      "Invalid category_id 8 in image_id 8197, annotation: {'image_id': 8197, 'category_id': 8, 'bbox': [1310, 342, 172, 136], 'area': 23392, 'id': 22824}\n",
      "Invalid category_id 8 in image_id 3232, annotation: {'image_id': 3232, 'category_id': 8, 'bbox': [1429, 518, 182, 76], 'area': 13832, 'id': 8190}\n",
      "Invalid category_id 8 in image_id 7493, annotation: {'image_id': 7493, 'category_id': 8, 'bbox': [6, 448, 506, 345], 'area': 174570, 'id': 20856}\n",
      "Invalid category_id 8 in image_id 2999, annotation: {'image_id': 2999, 'category_id': 8, 'bbox': [746, 449, 289, 164], 'area': 47396, 'id': 7510}\n",
      "Invalid category_id 10 in image_id 4037, annotation: {'image_id': 4037, 'category_id': 10, 'bbox': [1329, 350, 90, 109], 'area': 9810, 'id': 10567}\n",
      "Invalid category_id 10 in image_id 4037, annotation: {'image_id': 4037, 'category_id': 10, 'bbox': [1291, 330, 81, 117], 'area': 9477, 'id': 10568}\n",
      "Invalid category_id 8 in image_id 27371, annotation: {'image_id': 27371, 'category_id': 8, 'bbox': [925, 483, 144, 160], 'area': 23040, 'id': 102499}\n",
      "Invalid category_id 10 in image_id 10783, annotation: {'image_id': 10783, 'category_id': 10, 'bbox': [552, 533, 637, 264], 'area': 168168, 'id': 31826}\n",
      "Invalid category_id 8 in image_id 10783, annotation: {'image_id': 10783, 'category_id': 8, 'bbox': [1519, 686, 260, 258], 'area': 67080, 'id': 31827}\n",
      "Invalid category_id 8 in image_id 10783, annotation: {'image_id': 10783, 'category_id': 8, 'bbox': [1816, 128, 54, 53], 'area': 2862, 'id': 31828}\n",
      "Invalid category_id 8 in image_id 18256, annotation: {'image_id': 18256, 'category_id': 8, 'bbox': [1419, 648, 446, 361], 'area': 161006, 'id': 58280}\n",
      "Invalid category_id 8 in image_id 16810, annotation: {'image_id': 16810, 'category_id': 8, 'bbox': [1523, 113, 187, 117], 'area': 21879, 'id': 53725}\n",
      "Invalid category_id 8 in image_id 16810, annotation: {'image_id': 16810, 'category_id': 8, 'bbox': [1671, 102, 191, 136], 'area': 25976, 'id': 53726}\n",
      "Invalid category_id 8 in image_id 13685, annotation: {'image_id': 13685, 'category_id': 8, 'bbox': [23, 561, 441, 222], 'area': 97902, 'id': 42282}\n",
      "Invalid category_id 8 in image_id 13685, annotation: {'image_id': 13685, 'category_id': 8, 'bbox': [248, 561, 3, 21], 'area': 63, 'id': 42283}\n",
      "Invalid category_id 10 in image_id 13685, annotation: {'image_id': 13685, 'category_id': 10, 'bbox': [0, 638, 41, 171], 'area': 7011, 'id': 42284}\n",
      "Invalid category_id 8 in image_id 26246, annotation: {'image_id': 26246, 'category_id': 8, 'bbox': [1540, 620, 380, 149], 'area': 56620, 'id': 96358}\n",
      "Invalid category_id 8 in image_id 26246, annotation: {'image_id': 26246, 'category_id': 8, 'bbox': [119, 306, 128, 92], 'area': 11776, 'id': 96365}\n",
      "Invalid category_id 8 in image_id 24665, annotation: {'image_id': 24665, 'category_id': 8, 'bbox': [771, 186, 100, 79], 'area': 7900, 'id': 87803}\n",
      "Invalid category_id 8 in image_id 24665, annotation: {'image_id': 24665, 'category_id': 8, 'bbox': [520, 216, 68, 134], 'area': 9112, 'id': 87804}\n",
      "Invalid category_id 8 in image_id 28108, annotation: {'image_id': 28108, 'category_id': 8, 'bbox': [751, 477, 23, 169], 'area': 3887, 'id': 106352}\n",
      "Invalid category_id 8 in image_id 28108, annotation: {'image_id': 28108, 'category_id': 8, 'bbox': [0, 494, 695, 390], 'area': 271050, 'id': 106353}\n",
      "Invalid category_id 8 in image_id 28468, annotation: {'image_id': 28468, 'category_id': 8, 'bbox': [464, 320, 100, 56], 'area': 5600, 'id': 108093}\n",
      "Invalid category_id 8 in image_id 25441, annotation: {'image_id': 25441, 'category_id': 8, 'bbox': [493, 197, 113, 123], 'area': 13899, 'id': 91485}\n",
      "Invalid category_id 8 in image_id 25441, annotation: {'image_id': 25441, 'category_id': 8, 'bbox': [972, 229, 144, 130], 'area': 18720, 'id': 91488}\n",
      "Invalid category_id 10 in image_id 17783, annotation: {'image_id': 17783, 'category_id': 10, 'bbox': [0, 257, 578, 212], 'area': 122536, 'id': 56813}\n",
      "Invalid category_id 8 in image_id 12834, annotation: {'image_id': 12834, 'category_id': 8, 'bbox': [188, 604, 262, 126], 'area': 33012, 'id': 39078}\n",
      "Invalid category_id 8 in image_id 9564, annotation: {'image_id': 9564, 'category_id': 8, 'bbox': [954, 343, 128, 83], 'area': 10624, 'id': 27471}\n",
      "Invalid category_id 8 in image_id 21176, annotation: {'image_id': 21176, 'category_id': 8, 'bbox': [1200, 463, 76, 92], 'area': 6992, 'id': 67986}\n",
      "Invalid category_id 10 in image_id 30676, annotation: {'image_id': 30676, 'category_id': 10, 'bbox': [463, 376, 95, 56], 'area': 5320, 'id': 120399}\n",
      "Invalid category_id 8 in image_id 5324, annotation: {'image_id': 5324, 'category_id': 8, 'bbox': [645, 502, 404, 154], 'area': 62216, 'id': 14394}\n",
      "Invalid category_id 8 in image_id 21958, annotation: {'image_id': 21958, 'category_id': 8, 'bbox': [1606, 193, 32, 43], 'area': 1376, 'id': 72557}\n",
      "Invalid category_id 8 in image_id 21958, annotation: {'image_id': 21958, 'category_id': 8, 'bbox': [1690, 203, 42, 38], 'area': 1596, 'id': 72558}\n",
      "Invalid category_id 8 in image_id 23481, annotation: {'image_id': 23481, 'category_id': 8, 'bbox': [762, 903, 449, 127], 'area': 57023, 'id': 81830}\n",
      "Invalid category_id 8 in image_id 9072, annotation: {'image_id': 9072, 'category_id': 8, 'bbox': [1707, 132, 85, 71], 'area': 6035, 'id': 25752}\n",
      "Invalid category_id 10 in image_id 17484, annotation: {'image_id': 17484, 'category_id': 10, 'bbox': [157, 41, 636, 265], 'area': 168540, 'id': 55888}\n",
      "Invalid category_id 8 in image_id 14507, annotation: {'image_id': 14507, 'category_id': 8, 'bbox': [1069, 470, 134, 84], 'area': 11256, 'id': 45212}\n",
      "Invalid category_id 8 in image_id 11774, annotation: {'image_id': 11774, 'category_id': 8, 'bbox': [413, 372, 180, 291], 'area': 52380, 'id': 35355}\n",
      "Invalid category_id 10 in image_id 12887, annotation: {'image_id': 12887, 'category_id': 10, 'bbox': [996, 218, 100, 50], 'area': 5000, 'id': 39296}\n",
      "Invalid category_id 8 in image_id 9723, annotation: {'image_id': 9723, 'category_id': 8, 'bbox': [994, 412, 654, 430], 'area': 281220, 'id': 28018}\n",
      "Invalid category_id 8 in image_id 30014, annotation: {'image_id': 30014, 'category_id': 8, 'bbox': [971, 198, 95, 149], 'area': 14155, 'id': 116488}\n",
      "Invalid category_id 8 in image_id 15502, annotation: {'image_id': 15502, 'category_id': 8, 'bbox': [1056, 863, 201, 92], 'area': 18492, 'id': 48901}\n",
      "Invalid category_id 8 in image_id 7161, annotation: {'image_id': 7161, 'category_id': 8, 'bbox': [1608, 353, 82, 63], 'area': 5166, 'id': 19967}\n",
      "Invalid category_id 8 in image_id 7161, annotation: {'image_id': 7161, 'category_id': 8, 'bbox': [1273, 467, 95, 135], 'area': 12825, 'id': 19968}\n",
      "Invalid category_id 8 in image_id 15517, annotation: {'image_id': 15517, 'category_id': 8, 'bbox': [1467, 35, 113, 89], 'area': 10057, 'id': 48991}\n",
      "Invalid category_id 8 in image_id 23914, annotation: {'image_id': 23914, 'category_id': 8, 'bbox': [1735, 8, 75, 43], 'area': 3225, 'id': 84190}\n",
      "Invalid category_id 10 in image_id 22325, annotation: {'image_id': 22325, 'category_id': 10, 'bbox': [1460, 107, 53, 44], 'area': 2332, 'id': 74850}\n",
      "Invalid category_id 8 in image_id 30451, annotation: {'image_id': 30451, 'category_id': 8, 'bbox': [0, 616, 449, 281], 'area': 126169, 'id': 119108}\n",
      "Invalid category_id 8 in image_id 7752, annotation: {'image_id': 7752, 'category_id': 8, 'bbox': [1329, 374, 202, 114], 'area': 23028, 'id': 21579}\n",
      "Invalid category_id 8 in image_id 22625, annotation: {'image_id': 22625, 'category_id': 8, 'bbox': [1327, 55, 110, 204], 'area': 22440, 'id': 76804}\n",
      "Invalid category_id 8 in image_id 28625, annotation: {'image_id': 28625, 'category_id': 8, 'bbox': [82, 331, 236, 130], 'area': 30680, 'id': 108971}\n",
      "Invalid category_id 10 in image_id 13143, annotation: {'image_id': 13143, 'category_id': 10, 'bbox': [857, 214, 139, 278], 'area': 38642, 'id': 40384}\n",
      "Invalid category_id 8 in image_id 13143, annotation: {'image_id': 13143, 'category_id': 8, 'bbox': [632, 697, 258, 132], 'area': 34056, 'id': 40385}\n",
      "Invalid category_id 8 in image_id 13143, annotation: {'image_id': 13143, 'category_id': 8, 'bbox': [1118, 558, 279, 208], 'area': 58032, 'id': 40386}\n",
      "Invalid category_id 8 in image_id 13143, annotation: {'image_id': 13143, 'category_id': 8, 'bbox': [1536, 168, 109, 158], 'area': 17222, 'id': 40387}\n",
      "Invalid category_id 10 in image_id 16786, annotation: {'image_id': 16786, 'category_id': 10, 'bbox': [1566, 81, 161, 124], 'area': 19964, 'id': 53646}\n",
      "Invalid category_id 8 in image_id 19506, annotation: {'image_id': 19506, 'category_id': 8, 'bbox': [507, 166, 209, 83], 'area': 17347, 'id': 62612}\n",
      "Invalid category_id 8 in image_id 31659, annotation: {'image_id': 31659, 'category_id': 8, 'bbox': [295, 344, 202, 110], 'area': 22220, 'id': 125701}\n",
      "Invalid category_id 8 in image_id 30975, annotation: {'image_id': 30975, 'category_id': 8, 'bbox': [1169, 319, 60, 82], 'area': 4920, 'id': 121987}\n",
      "Invalid category_id 10 in image_id 25363, annotation: {'image_id': 25363, 'category_id': 10, 'bbox': [723, 57, 46, 50], 'area': 2300, 'id': 90986}\n",
      "Invalid category_id 8 in image_id 25493, annotation: {'image_id': 25493, 'category_id': 8, 'bbox': [458, 158, 91, 102], 'area': 9282, 'id': 91782}\n",
      "Invalid category_id 8 in image_id 5150, annotation: {'image_id': 5150, 'category_id': 8, 'bbox': [358, 478, 112, 104], 'area': 11648, 'id': 13927}\n",
      "Invalid category_id 8 in image_id 5150, annotation: {'image_id': 5150, 'category_id': 8, 'bbox': [1758, 275, 107, 77], 'area': 8239, 'id': 13928}\n",
      "Invalid category_id 8 in image_id 26566, annotation: {'image_id': 26566, 'category_id': 8, 'bbox': [772, 157, 179, 107], 'area': 19153, 'id': 98155}\n",
      "Invalid category_id 8 in image_id 9264, annotation: {'image_id': 9264, 'category_id': 8, 'bbox': [804, 247, 211, 102], 'area': 21522, 'id': 26427}\n",
      "Invalid category_id 10 in image_id 11400, annotation: {'image_id': 11400, 'category_id': 10, 'bbox': [952, 372, 69, 26], 'area': 1794, 'id': 34039}\n",
      "Invalid category_id 8 in image_id 6714, annotation: {'image_id': 6714, 'category_id': 8, 'bbox': [1801, 288, 55, 40], 'area': 2200, 'id': 18645}\n",
      "Invalid category_id 8 in image_id 6714, annotation: {'image_id': 6714, 'category_id': 8, 'bbox': [1712, 299, 60, 66], 'area': 3960, 'id': 18646}\n",
      "Invalid category_id 8 in image_id 20113, annotation: {'image_id': 20113, 'category_id': 8, 'bbox': [0, 409, 281, 212], 'area': 59572, 'id': 64715}\n",
      "Invalid category_id 8 in image_id 20049, annotation: {'image_id': 20049, 'category_id': 8, 'bbox': [677, 299, 59, 72], 'area': 4248, 'id': 64468}\n",
      "Invalid category_id 8 in image_id 7761, annotation: {'image_id': 7761, 'category_id': 8, 'bbox': [1, 454, 213, 285], 'area': 60705, 'id': 21611}\n",
      "Invalid category_id 10 in image_id 3746, annotation: {'image_id': 3746, 'category_id': 10, 'bbox': [0, 529, 288, 148], 'area': 42624, 'id': 9694}\n",
      "Invalid category_id 10 in image_id 23491, annotation: {'image_id': 23491, 'category_id': 10, 'bbox': [1124, 436, 215, 142], 'area': 30530, 'id': 81852}\n",
      "Invalid category_id 8 in image_id 2597, annotation: {'image_id': 2597, 'category_id': 8, 'bbox': [1174, 532, 332, 251], 'area': 83332, 'id': 6317}\n",
      "Invalid category_id 8 in image_id 12749, annotation: {'image_id': 12749, 'category_id': 8, 'bbox': [578, 472, 340, 294], 'area': 99960, 'id': 38779}\n",
      "Invalid category_id 8 in image_id 9625, annotation: {'image_id': 9625, 'category_id': 8, 'bbox': [419, 249, 397, 201], 'area': 79797, 'id': 27696}\n",
      "Invalid category_id 8 in image_id 27840, annotation: {'image_id': 27840, 'category_id': 8, 'bbox': [1173, 130, 11, 4], 'area': 44, 'id': 104895}\n",
      "Invalid category_id 8 in image_id 27840, annotation: {'image_id': 27840, 'category_id': 8, 'bbox': [1165, 134, 47, 8], 'area': 376, 'id': 104896}\n",
      "Invalid category_id 8 in image_id 27840, annotation: {'image_id': 27840, 'category_id': 8, 'bbox': [1216, 126, 32, 16], 'area': 512, 'id': 104897}\n",
      "Invalid category_id 8 in image_id 27840, annotation: {'image_id': 27840, 'category_id': 8, 'bbox': [1272, 118, 35, 16], 'area': 560, 'id': 104898}\n",
      "Invalid category_id 8 in image_id 27840, annotation: {'image_id': 27840, 'category_id': 8, 'bbox': [1335, 114, 44, 28], 'area': 1232, 'id': 104899}\n",
      "Invalid category_id 8 in image_id 27840, annotation: {'image_id': 27840, 'category_id': 8, 'bbox': [1379, 106, 24, 16], 'area': 384, 'id': 104900}\n",
      "Invalid category_id 8 in image_id 2938, annotation: {'image_id': 2938, 'category_id': 8, 'bbox': [603, 536, 413, 218], 'area': 90034, 'id': 7357}\n",
      "Invalid category_id 10 in image_id 2938, annotation: {'image_id': 2938, 'category_id': 10, 'bbox': [206, 551, 422, 127], 'area': 53594, 'id': 7358}\n",
      "Invalid category_id 10 in image_id 2938, annotation: {'image_id': 2938, 'category_id': 10, 'bbox': [211, 523, 383, 162], 'area': 62046, 'id': 7360}\n",
      "Invalid category_id 10 in image_id 2938, annotation: {'image_id': 2938, 'category_id': 10, 'bbox': [395, 515, 3, 5], 'area': 15, 'id': 7361}\n",
      "Invalid category_id 8 in image_id 31201, annotation: {'image_id': 31201, 'category_id': 8, 'bbox': [0, 234, 269, 160], 'area': 43040, 'id': 123149}\n",
      "Invalid category_id 10 in image_id 9175, annotation: {'image_id': 9175, 'category_id': 10, 'bbox': [1122, 341, 519, 307], 'area': 159333, 'id': 26104}\n",
      "Invalid category_id 10 in image_id 9175, annotation: {'image_id': 9175, 'category_id': 10, 'bbox': [1412, 557, 23, 303], 'area': 6969, 'id': 26105}\n",
      "Invalid category_id 10 in image_id 9175, annotation: {'image_id': 9175, 'category_id': 10, 'bbox': [1421, 301, 29, 184], 'area': 5336, 'id': 26106}\n",
      "Invalid category_id 8 in image_id 22357, annotation: {'image_id': 22357, 'category_id': 8, 'bbox': [578, 370, 221, 129], 'area': 28509, 'id': 75038}\n",
      "Invalid category_id 10 in image_id 17849, annotation: {'image_id': 17849, 'category_id': 10, 'bbox': [1487, 227, 200, 258], 'area': 51600, 'id': 57052}\n",
      "Invalid category_id 8 in image_id 7917, annotation: {'image_id': 7917, 'category_id': 8, 'bbox': [1749, 284, 134, 90], 'area': 12060, 'id': 22053}\n",
      "Invalid category_id 10 in image_id 3598, annotation: {'image_id': 3598, 'category_id': 10, 'bbox': [5, 470, 380, 281], 'area': 106780, 'id': 9237}\n",
      "Invalid category_id 10 in image_id 6325, annotation: {'image_id': 6325, 'category_id': 10, 'bbox': [1858, 310, 31, 27], 'area': 837, 'id': 17486}\n",
      "Invalid category_id 10 in image_id 22386, annotation: {'image_id': 22386, 'category_id': 10, 'bbox': [1318, 203, 84, 49], 'area': 4116, 'id': 75235}\n",
      "Invalid category_id 8 in image_id 3616, annotation: {'image_id': 3616, 'category_id': 8, 'bbox': [460, 533, 651, 300], 'area': 195300, 'id': 9301}\n",
      "Invalid category_id 8 in image_id 32195, annotation: {'image_id': 32195, 'category_id': 8, 'bbox': [682, 434, 143, 80], 'area': 11440, 'id': 128656}\n",
      "Invalid category_id 8 in image_id 32195, annotation: {'image_id': 32195, 'category_id': 8, 'bbox': [636, 185, 46, 23], 'area': 1058, 'id': 128660}\n",
      "Invalid category_id 8 in image_id 8959, annotation: {'image_id': 8959, 'category_id': 8, 'bbox': [59, 409, 359, 133], 'area': 47747, 'id': 25357}\n",
      "Invalid category_id 8 in image_id 8959, annotation: {'image_id': 8959, 'category_id': 8, 'bbox': [1201, 916, 316, 161], 'area': 50876, 'id': 25362}\n",
      "Invalid category_id 8 in image_id 8748, annotation: {'image_id': 8748, 'category_id': 8, 'bbox': [564, 584, 543, 235], 'area': 127605, 'id': 24618}\n",
      "Invalid category_id 10 in image_id 14360, annotation: {'image_id': 14360, 'category_id': 10, 'bbox': [523, 290, 82, 61], 'area': 5002, 'id': 44738}\n",
      "Invalid category_id 8 in image_id 23739, annotation: {'image_id': 23739, 'category_id': 8, 'bbox': [1155, 228, 252, 177], 'area': 44604, 'id': 83230}\n",
      "Invalid category_id 8 in image_id 26923, annotation: {'image_id': 26923, 'category_id': 8, 'bbox': [0, 257, 245, 179], 'area': 43855, 'id': 99991}\n",
      "Invalid category_id 8 in image_id 18290, annotation: {'image_id': 18290, 'category_id': 8, 'bbox': [1668, 313, 183, 540], 'area': 98820, 'id': 58407}\n",
      "Invalid category_id 10 in image_id 8584, annotation: {'image_id': 8584, 'category_id': 10, 'bbox': [991, 26, 6, 3], 'area': 18, 'id': 24002}\n",
      "Invalid category_id 10 in image_id 8584, annotation: {'image_id': 8584, 'category_id': 10, 'bbox': [997, 34, 426, 125], 'area': 53250, 'id': 24003}\n",
      "Invalid category_id 8 in image_id 19035, annotation: {'image_id': 19035, 'category_id': 8, 'bbox': [1656, 86, 107, 69], 'area': 7383, 'id': 60980}\n",
      "Invalid category_id 8 in image_id 32587, annotation: {'image_id': 32587, 'category_id': 8, 'bbox': [625, 420, 443, 217], 'area': 96131, 'id': 130695}\n",
      "Invalid category_id 8 in image_id 27979, annotation: {'image_id': 27979, 'category_id': 8, 'bbox': [142, 356, 153, 177], 'area': 27081, 'id': 105703}\n",
      "Invalid category_id 10 in image_id 27979, annotation: {'image_id': 27979, 'category_id': 10, 'bbox': [140, 113, 305, 279], 'area': 85095, 'id': 105704}\n",
      "Invalid category_id 8 in image_id 22880, annotation: {'image_id': 22880, 'category_id': 8, 'bbox': [1233, 446, 120, 162], 'area': 19440, 'id': 78360}\n",
      "Invalid category_id 8 in image_id 11881, annotation: {'image_id': 11881, 'category_id': 8, 'bbox': [1393, 457, 172, 160], 'area': 27520, 'id': 35745}\n",
      "Invalid category_id 8 in image_id 15366, annotation: {'image_id': 15366, 'category_id': 8, 'bbox': [715, 491, 447, 186], 'area': 83142, 'id': 48287}\n",
      "Invalid category_id 8 in image_id 7739, annotation: {'image_id': 7739, 'category_id': 8, 'bbox': [275, 330, 253, 202], 'area': 51106, 'id': 21536}\n",
      "Invalid category_id 8 in image_id 19734, annotation: {'image_id': 19734, 'category_id': 8, 'bbox': [675, 120, 198, 158], 'area': 31284, 'id': 63343}\n",
      "Invalid category_id 8 in image_id 14235, annotation: {'image_id': 14235, 'category_id': 8, 'bbox': [1240, 177, 58, 75], 'area': 4350, 'id': 44308}\n",
      "Invalid category_id 8 in image_id 8904, annotation: {'image_id': 8904, 'category_id': 8, 'bbox': [1354, 302, 108, 85], 'area': 9180, 'id': 25174}\n",
      "Invalid category_id 8 in image_id 18257, annotation: {'image_id': 18257, 'category_id': 8, 'bbox': [948, 192, 224, 97], 'area': 21728, 'id': 58281}\n",
      "Invalid category_id 8 in image_id 18257, annotation: {'image_id': 18257, 'category_id': 8, 'bbox': [441, 390, 356, 333], 'area': 118548, 'id': 58282}\n",
      "Invalid category_id 8 in image_id 9394, annotation: {'image_id': 9394, 'category_id': 8, 'bbox': [562, 102, 106, 119], 'area': 12614, 'id': 26861}\n",
      "Invalid category_id 8 in image_id 11534, annotation: {'image_id': 11534, 'category_id': 8, 'bbox': [600, 295, 53, 91], 'area': 4823, 'id': 34511}\n",
      "Invalid category_id 8 in image_id 11534, annotation: {'image_id': 11534, 'category_id': 8, 'bbox': [5, 614, 323, 189], 'area': 61047, 'id': 34512}\n",
      "Invalid category_id 8 in image_id 12332, annotation: {'image_id': 12332, 'category_id': 8, 'bbox': [0, 354, 190, 113], 'area': 21470, 'id': 37227}\n",
      "Invalid category_id 8 in image_id 20642, annotation: {'image_id': 20642, 'category_id': 8, 'bbox': [387, 334, 209, 87], 'area': 18183, 'id': 66344}\n",
      "Invalid category_id 8 in image_id 6992, annotation: {'image_id': 6992, 'category_id': 8, 'bbox': [954, 268, 522, 231], 'area': 120582, 'id': 19442}\n",
      "Invalid category_id 8 in image_id 18703, annotation: {'image_id': 18703, 'category_id': 8, 'bbox': [1431, 831, 251, 249], 'area': 62499, 'id': 59744}\n",
      "Invalid category_id 8 in image_id 21823, annotation: {'image_id': 21823, 'category_id': 8, 'bbox': [1231, 323, 136, 181], 'area': 24616, 'id': 71689}\n",
      "Invalid category_id 10 in image_id 21823, annotation: {'image_id': 21823, 'category_id': 10, 'bbox': [1082, 348, 86, 27], 'area': 2322, 'id': 71692}\n",
      "Invalid category_id 8 in image_id 26442, annotation: {'image_id': 26442, 'category_id': 8, 'bbox': [1215, 426, 194, 236], 'area': 45784, 'id': 97460}\n",
      "Invalid category_id 8 in image_id 16080, annotation: {'image_id': 16080, 'category_id': 8, 'bbox': [454, 389, 134, 78], 'area': 10452, 'id': 51194}\n",
      "Invalid category_id 8 in image_id 15055, annotation: {'image_id': 15055, 'category_id': 8, 'bbox': [1148, 187, 151, 75], 'area': 11325, 'id': 47121}\n",
      "Invalid category_id 10 in image_id 31862, annotation: {'image_id': 31862, 'category_id': 10, 'bbox': [415, 15, 46, 30], 'area': 1380, 'id': 126812}\n",
      "Invalid category_id 10 in image_id 31862, annotation: {'image_id': 31862, 'category_id': 10, 'bbox': [458, 8, 44, 30], 'area': 1320, 'id': 126813}\n",
      "Invalid category_id 8 in image_id 6147, annotation: {'image_id': 6147, 'category_id': 8, 'bbox': [690, 597, 1177, 473], 'area': 556721, 'id': 16935}\n",
      "Invalid category_id 10 in image_id 29155, annotation: {'image_id': 29155, 'category_id': 10, 'bbox': [178, 121, 39, 34], 'area': 1326, 'id': 111737}\n",
      "Invalid category_id 10 in image_id 29155, annotation: {'image_id': 29155, 'category_id': 10, 'bbox': [82, 113, 58, 40], 'area': 2320, 'id': 111738}\n",
      "Invalid category_id 10 in image_id 29155, annotation: {'image_id': 29155, 'category_id': 10, 'bbox': [32, 139, 74, 48], 'area': 3552, 'id': 111739}\n",
      "Invalid category_id 8 in image_id 6075, annotation: {'image_id': 6075, 'category_id': 8, 'bbox': [1061, 321, 231, 206], 'area': 47586, 'id': 16742}\n",
      "Invalid category_id 8 in image_id 5345, annotation: {'image_id': 5345, 'category_id': 8, 'bbox': [287, 347, 168, 130], 'area': 21840, 'id': 14467}\n",
      "Invalid category_id 8 in image_id 10511, annotation: {'image_id': 10511, 'category_id': 8, 'bbox': [1372, 269, 136, 155], 'area': 21080, 'id': 30840}\n",
      "Invalid category_id 8 in image_id 26992, annotation: {'image_id': 26992, 'category_id': 8, 'bbox': [1212, 507, 96, 50], 'area': 4800, 'id': 100415}\n",
      "Invalid category_id 8 in image_id 26992, annotation: {'image_id': 26992, 'category_id': 8, 'bbox': [1018, 485, 80, 37], 'area': 2960, 'id': 100416}\n",
      "Invalid category_id 10 in image_id 19959, annotation: {'image_id': 19959, 'category_id': 10, 'bbox': [981, 78, 29, 57], 'area': 1653, 'id': 64097}\n",
      "Invalid category_id 8 in image_id 15396, annotation: {'image_id': 15396, 'category_id': 8, 'bbox': [502, 483, 259, 195], 'area': 50505, 'id': 48393}\n",
      "Invalid category_id 8 in image_id 15396, annotation: {'image_id': 15396, 'category_id': 8, 'bbox': [630, 124, 180, 122], 'area': 21960, 'id': 48395}\n",
      "Invalid category_id 8 in image_id 11145, annotation: {'image_id': 11145, 'category_id': 8, 'bbox': [1023, 463, 604, 376], 'area': 227104, 'id': 33153}\n",
      "Invalid category_id 8 in image_id 11145, annotation: {'image_id': 11145, 'category_id': 8, 'bbox': [1668, 747, 252, 231], 'area': 58212, 'id': 33154}\n",
      "Invalid category_id 8 in image_id 12117, annotation: {'image_id': 12117, 'category_id': 8, 'bbox': [705, 477, 309, 198], 'area': 61182, 'id': 36479}\n",
      "Invalid category_id 10 in image_id 29477, annotation: {'image_id': 29477, 'category_id': 10, 'bbox': [94, 667, 663, 364], 'area': 241332, 'id': 113485}\n",
      "Invalid category_id 8 in image_id 31556, annotation: {'image_id': 31556, 'category_id': 8, 'bbox': [541, 494, 136, 127], 'area': 17272, 'id': 125067}\n",
      "Invalid category_id 10 in image_id 25223, annotation: {'image_id': 25223, 'category_id': 10, 'bbox': [776, 200, 37, 38], 'area': 1406, 'id': 90211}\n",
      "Invalid category_id 8 in image_id 25223, annotation: {'image_id': 25223, 'category_id': 8, 'bbox': [799, 200, 32, 32], 'area': 1024, 'id': 90212}\n",
      "Invalid category_id 10 in image_id 13820, annotation: {'image_id': 13820, 'category_id': 10, 'bbox': [56, 545, 507, 188], 'area': 95316, 'id': 42723}\n",
      "Invalid category_id 8 in image_id 19774, annotation: {'image_id': 19774, 'category_id': 8, 'bbox': [656, 207, 70, 78], 'area': 5460, 'id': 63495}\n",
      "Invalid category_id 8 in image_id 28663, annotation: {'image_id': 28663, 'category_id': 8, 'bbox': [1517, 474, 249, 92], 'area': 22908, 'id': 109161}\n",
      "Invalid category_id 8 in image_id 3111, annotation: {'image_id': 3111, 'category_id': 8, 'bbox': [1511, 286, 65, 60], 'area': 3900, 'id': 7841}\n",
      "Invalid category_id 8 in image_id 20300, annotation: {'image_id': 20300, 'category_id': 8, 'bbox': [1175, 267, 150, 84], 'area': 12600, 'id': 65334}\n",
      "Invalid category_id 8 in image_id 20300, annotation: {'image_id': 20300, 'category_id': 8, 'bbox': [663, 205, 69, 95], 'area': 6555, 'id': 65338}\n",
      "Invalid category_id 8 in image_id 15003, annotation: {'image_id': 15003, 'category_id': 8, 'bbox': [510, 284, 238, 95], 'area': 22610, 'id': 46957}\n",
      "Invalid category_id 10 in image_id 14911, annotation: {'image_id': 14911, 'category_id': 10, 'bbox': [1633, 100, 168, 124], 'area': 20832, 'id': 46641}\n",
      "Invalid category_id 10 in image_id 12885, annotation: {'image_id': 12885, 'category_id': 10, 'bbox': [552, 266, 266, 66], 'area': 17556, 'id': 39291}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 295\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    293\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(val_annotations, f)\n\u001b[1;32m--> 295\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 196\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Count valid samples\u001b[39;00m\n\u001b[0;32m    195\u001b[0m valid_train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 196\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_train_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m valid_val_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 101\u001b[0m, in \u001b[0;36mAUAIRDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Process image and annotations\u001b[39;00m\n\u001b[0;32m     91\u001b[0m processor_annotations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     92\u001b[0m     {\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x, y, w, h],  \u001b[38;5;66;03m# COCO format [x, y, width, height]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m                              [ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m anns \u001b[38;5;28;01mif\u001b[39;00m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m label_map])\n\u001b[0;32m    100\u001b[0m ]\n\u001b[1;32m--> 101\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_annotations\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    107\u001b[0m encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\image_processing_utils.py:42\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\yolos\\image_processing_yolos.py:1380\u001b[0m, in \u001b[0;36mYolosImageProcessor.preprocess\u001b[1;34m(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_annotations, do_pad, format, return_tensors, data_format, input_data_format, pad_size, **kwargs)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, annotations):\n\u001b[0;32m   1379\u001b[0m     orig_size \u001b[38;5;241m=\u001b[39m get_image_size(image, input_data_format)\n\u001b[1;32m-> 1380\u001b[0m     resized_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1383\u001b[0m     resized_annotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize_annotation(\n\u001b[0;32m   1384\u001b[0m         target, orig_size, get_image_size(resized_image, input_data_format)\n\u001b[0;32m   1385\u001b[0m     )\n\u001b[0;32m   1386\u001b[0m     resized_images\u001b[38;5;241m.\u001b[39mappend(resized_image)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\yolos\\image_processing_yolos.py:951\u001b[0m, in \u001b[0;36mYolosImageProcessor.resize\u001b[1;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_edge\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest_edge\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys. Got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    950\u001b[0m     )\n\u001b[1;32m--> 951\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\image_transforms.py:370\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    369\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[1;32m--> 370\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\image_transforms.py:209\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(image, do_rescale, image_mode, input_data_format)\u001b[0m\n\u001b[0;32m    206\u001b[0m     image \u001b[38;5;241m=\u001b[39m rescale(image, \u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    208\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\PIL\\Image.py:3304\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3301\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires either tobytes() or tostring()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 3304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\PIL\\Image.py:3206\u001b[0m, in \u001b[0;36mfrombuffer\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   3203\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[1;32m-> 3206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\PIL\\Image.py:3137\u001b[0m, in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   3112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3113\u001b[0m \u001b[38;5;124;03mCreates a copy of an image memory from pixel data in a buffer.\u001b[39;00m\n\u001b[0;32m   3114\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3132\u001b[0m \u001b[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m   3133\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3135\u001b[0m _check_size(size)\n\u001b[1;32m-> 3137\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m im\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   3139\u001b[0m     decoder_args: Any \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\PIL\\Image.py:3102\u001b[0m, in \u001b[0;36mnew\u001b[1;34m(mode, size, color)\u001b[0m\n\u001b[0;32m   3100\u001b[0m         im\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;241m=\u001b[39m ImagePalette\u001b[38;5;241m.\u001b[39mImagePalette()\n\u001b[0;32m   3101\u001b[0m         color \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mgetcolor(color_ints)\n\u001b[1;32m-> 3102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39m_new(\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 🔧 Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 🟣 Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-train\")\n",
    "\n",
    "# ⚙️ Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model.to(device)\n",
    "\n",
    "# ⚙️ Verify category mapping\n",
    "print(\"Model id2label:\", model.config.id2label)\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "    print(\"AUAIR categories:\", raw_annotations[\"categories\"])\n",
    "\n",
    "# ⚙️ Category ID mapping (AUAIR to COCO)\n",
    "label_map = {0: 1, 1: 3, 2: 8, 3: 7, 4: 4, 5: 2, 6: 6, 7: 10}  # AUAIR to COCO\n",
    "\n",
    "# ⚙️ Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"train\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.split = split\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        self.invalid_category_count = 0  # Track invalid category IDs\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        img_width, img_height = img_info[\"width\"], img_info[\"height\"]\n",
    "        for ann in anns:\n",
    "            # COCO-style bbox: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to [x_min, y_min, x_max, y_max] and normalize\n",
    "            x_min, y_min = x / img_width, y / img_height\n",
    "            x_max, y_max = (x + w) / img_width, (y + h) / img_height\n",
    "            if ann[\"category_id\"] in label_map:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(label_map[ann[\"category_id\"]])  # Map AUAIR IDs to COCO IDs\n",
    "            else:\n",
    "                self.invalid_category_count += 1\n",
    "                #print(f\"Invalid category_id {ann['category_id']} in image_id {img_id}, annotation: {ann}\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image and annotations\n",
    "        processor_annotations = [\n",
    "            {\n",
    "                \"bbox\": [x, y, w, h],  # COCO format [x, y, width, height]\n",
    "                \"category_id\": label_map[l],  # Map AUAIR IDs to COCO IDs\n",
    "                \"area\": float(w * h),\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            for (x, y, w, h), l in zip((ann[\"bbox\"] for ann in anns if ann[\"category_id\"] in label_map), \n",
    "                                     [ann[\"category_id\"] for ann in anns if ann[\"category_id\"] in label_map])\n",
    "        ]\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": processor_annotations},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        # Debug: Log category IDs for first few samples\n",
    "        if idx < 3 and self.split == \"train\":\n",
    "            print(f\"Sample {idx}, Image ID: {img_id}, Labels: {labels}\")\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "# 📂 Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Debug: Check unique category IDs in raw annotations\n",
    "unique_category_ids = set()\n",
    "for ann in raw_annotations[\"annotations\"]:\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        unique_category_ids.add(bbox[\"class\"])\n",
    "print(f\"Unique category IDs in raw annotations: {unique_category_ids}\")\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}\n",
    "coco_annotations = []\n",
    "invalid_annotation_count = 0  # Track invalid annotations during processing\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        if bbox[\"class\"] in label_map:  # Only include valid category IDs\n",
    "            coco_annotations.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": label_map[bbox[\"class\"]],  # Map AUAIR IDs to COCO IDs\n",
    "                \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "                \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "                \"id\": len(coco_annotations) + 1,\n",
    "            })\n",
    "        else:\n",
    "            #print(f\"Skipping annotation with invalid category_id {bbox['class']} in image {img_name}\")\n",
    "            invalid_annotation_count += 1\n",
    "\n",
    "\n",
    "# Print invalid annotation count from annotation processing\n",
    "print(f\"invalid_category_count: {invalid_annotation_count}\")\n",
    "\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": model.config.id2label[i]} for i in label_map.values()],\n",
    "}\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:train_size]\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "train_images = [img for img in annotations[\"images\"] if img[\"id\"] in train_ids]\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "train_annotations = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "train_dataset = AUAIRDataset(train_annotations, img_dir, processor, split=\"train\")\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "# 🧠 Training Loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Count valid samples\n",
    "    valid_train_samples = 0\n",
    "    for batch in train_loader:\n",
    "        valid_train_samples += len(batch)\n",
    "    valid_val_samples = 0\n",
    "    for batch in val_loader:\n",
    "        valid_val_samples += len(batch)\n",
    "    print(f\"Valid training samples: {valid_train_samples}, Valid validation samples: {valid_val_samples}\")\n",
    "    #print(f\"Invalid category IDs encountered (train): {train_dataset.invalid_category_count}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss_ce = 0.0\n",
    "        train_loss_bbox = 0.0\n",
    "        train_loss_giou = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "            labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "            # Move all tensors in labels to the correct device\n",
    "            labels = [\n",
    "                {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                for label in labels\n",
    "            ]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss_dict = outputs.loss_dict  # Contains loss_ce, loss_bbox, loss_giou\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_loss_ce += loss_dict.get(\"loss_ce\", 0.0).item()\n",
    "            train_loss_bbox += loss_dict.get(\"loss_bbox\", 0.0).item()\n",
    "            train_loss_giou += loss_dict.get(\"loss_giou\", 0.0).item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_loss_ce = train_loss_ce / len(train_loader)\n",
    "        avg_train_loss_bbox = train_loss_bbox / len(train_loader)\n",
    "        avg_train_loss_giou = train_loss_giou / len(train_loader)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_loss_ce\": avg_train_loss_ce,\n",
    "            \"train_loss_bbox\": avg_train_loss_bbox,\n",
    "            \"train_loss_giou\": avg_train_loss_giou\n",
    "        })\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loss_ce = 0.0\n",
    "        val_loss_bbox = 0.0\n",
    "        val_loss_giou = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "                labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "                labels = [\n",
    "                    {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                    for label in labels\n",
    "                ]\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                val_loss_ce += outputs.loss_dict.get(\"loss_ce\", 0.0).item()\n",
    "                val_loss_bbox += outputs.loss_dict.get(\"loss_bbox\", 0.0).item()\n",
    "                val_loss_giou += outputs.loss_dict.get(\"loss_giou\", 0.0).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_loss_ce = val_loss_ce / len(val_loader)\n",
    "        avg_val_loss_bbox = val_loss_bbox / len(val_loader)\n",
    "        avg_val_loss_giou = val_loss_giou / len(val_loader)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_loss_ce\": avg_val_loss_ce,\n",
    "            \"val_loss_bbox\": avg_val_loss_bbox,\n",
    "            \"val_loss_giou\": avg_val_loss_giou,\n",
    "            \"invalid_category_count_train\": train_dataset.invalid_category_count,\n",
    "            \"invalid_category_count_val\": val_dataset.invalid_category_count\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f} (CE: {avg_train_loss_ce:.4f}, BBox: {avg_train_loss_bbox:.4f}, GIoU: {avg_train_loss_giou:.4f}), Val Loss: {avg_val_loss:.4f} (CE: {avg_val_loss_ce:.4f}, BBox: {avg_val_loss_bbox:.4f}, GIoU: {avg_val_loss_giou:.4f})\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "    processor.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "\n",
    "# 🧪 Run Training\n",
    "# Save ground truth annotations for evaluation\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562293c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d68dc7",
   "metadata": {},
   "source": [
    "INFERENCE PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 🔧 Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 🟣 Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-inference\")\n",
    "\n",
    "# ⚙️ Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"yolos-tiny-finetuned\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"yolos-tiny-finetuned\")\n",
    "model.to(device)\n",
    "\n",
    "# ⚙️ Category ID mapping (COCO to AUAIR for evaluation)\n",
    "reverse_map = {1: 0, 3: 1, 8: 2, 7: 3, 4: 4, 2: 5, 6: 6, 10: 7}  # COCO to AUAIR\n",
    "\n",
    "# ⚙️ Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"val\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image (not used for inference, but kept for compatibility)\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Convert to tensors (not used for inference, but kept for compatibility)\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image\n",
    "        encoding = self.processor(images=image, return_tensors=\"pt\")\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)  # Remove batch dimension\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "# 📂 Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}  # Map image_id to image metadata\n",
    "coco_annotations = []  # COCO-style annotations\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1  # Assign unique image_id (1-based indexing)\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        coco_annotations.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": bbox[\"class\"],\n",
    "            \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "            \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "            \"id\": len(coco_annotations) + 1,\n",
    "        })\n",
    "\n",
    "# Create pseudo-COCO structure\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(raw_annotations[\"categories\"])],\n",
    "}\n",
    "\n",
    "# Split dataset (use validation set for inference)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# 🧠 Inference\n",
    "def run_yolos_inference(model, dataset, output_path=\"yolos_pred.json\", log_images=False):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        inputs, image, img_id, image_name = dataset[idx]\n",
    "        if inputs is None:\n",
    "            continue\n",
    "        inputs = {k: v.unsqueeze(0).to(device) for k, v in inputs.items() if k == \"pixel_values\"}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        width, height = image.size\n",
    "        target_sizes = torch.tensor([[height, width]]).to(device)\n",
    "        \n",
    "        result = processor.post_process_object_detection(\n",
    "            outputs,\n",
    "            target_sizes=target_sizes,\n",
    "            threshold=0.5\n",
    "        )[0]\n",
    "\n",
    "        if log_images and idx % 50 == 0 and len(result[\"boxes\"]) > 0:\n",
    "            boxes = result[\"boxes\"].cpu().tolist()\n",
    "            scores = result[\"scores\"].cpu().tolist()\n",
    "            labels = result[\"labels\"].cpu().tolist()\n",
    "\n",
    "            wandb.log({\n",
    "                \"prediction\": wandb.Image(image, boxes={\n",
    "                    \"predictions\": {\n",
    "                        \"box_data\": [\n",
    "                            {\n",
    "                                \"position\": {\n",
    "                                    \"minX\": b[0] / width,\n",
    "                                    \"minY\": b[1] / height,\n",
    "                                    \"maxX\": b[2] / width,\n",
    "                                    \"maxY\": b[3] / height,\n",
    "                                },\n",
    "                                \"score\": s,\n",
    "                                \"class_id\": reverse_map.get(l, l)  # Map back to AUAIR IDs for logging\n",
    "                            }\n",
    "                            for b, s, l in zip(boxes, scores, labels)\n",
    "                        ],\n",
    "                        \"class_labels\": {i: name for i, name in enumerate(raw_annotations[\"categories\"])}\n",
    "                    }\n",
    "                }),\n",
    "                \"step\": idx\n",
    "            })\n",
    "\n",
    "        for box, label, score in zip(result[\"boxes\"], result[\"labels\"], result[\"scores\"]):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": reverse_map.get(int(label), int(label)),  # Map COCO to AUAIR IDs\n",
    "                \"bbox\": [float(xmin), float(ymin), float(xmax - xmin), float(ymax - ymin)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    return output_path\n",
    "\n",
    "# 🧪 Run Inference\n",
    "pred_json = run_yolos_inference(model, val_dataset, output_path=\"yolos_pred.json\", log_images=True)\n",
    "print(f\"Predictions saved to {pred_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91085e2",
   "metadata": {},
   "source": [
    "EVALUATION PART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 🔧 Paths\n",
    "gt_path = \"gt.json\"\n",
    "pred_path = \"yolos_pred.json\"\n",
    "\n",
    "# 🟣 Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-evaluate\")\n",
    "\n",
    "# 📊 mAP Evaluation\n",
    "def evaluate_map(gt_path, pred_path):\n",
    "    coco_gt = COCO(gt_path)\n",
    "    coco_dt = coco_gt.loadRes(pred_path)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"mAP@[0.5:0.95]\": coco_eval.stats[0],\n",
    "        \"AP50\": coco_eval.stats[1],\n",
    "        \"AP75\": coco_eval.stats[2],\n",
    "        \"AP_small\": coco_eval.stats[3],\n",
    "        \"AP_medium\": coco_eval.stats[4],\n",
    "        \"AP_large\": coco_eval.stats[5]\n",
    "    }\n",
    "\n",
    "    precisions = coco_eval.eval[\"precision\"]\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    categories = coco_gt.loadCats(cat_ids)\n",
    "\n",
    "    print(\"\\n📊 Per-category AP (IoU=0.50:0.95):\")\n",
    "    for idx, cat in enumerate(categories):\n",
    "        precision = precisions[:, :, idx, 0, 0]\n",
    "        precision = precision[precision > -1]\n",
    "        ap = precision.mean() if precision.size > 0 else float(\"nan\")\n",
    "        metrics[f\"AP_{cat['name']}\"] = ap\n",
    "        print(f\"  {cat['name']:20s}: {ap:.4f}\")\n",
    "\n",
    "    wandb.log(metrics)\n",
    "    print(\"✅ mAP + per-class AP metrics logged to W&B.\")\n",
    "    return metrics\n",
    "\n",
    "# 🧪 Run Evaluation\n",
    "metrics = evaluate_map(gt_path, pred_path)\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d30290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
