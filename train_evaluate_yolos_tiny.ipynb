{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d7be84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'annotations'])\n",
      "{'image_name': 'frame_20190829091111_x_0001973.jpg', 'image_width:': 1920.0, 'image_height': 1080.0, 'platform': 'Parrot Bebop 2', 'time': {'year': 2019, 'month': 8, 'day': 29, 'hour': 9, 'min': 11, 'sec': 11, 'ms': 394400.0}, 'longtitude': 10.18798203255313, 'latitude': 56.20630134795274, 'altitude': 19921.6, 'linear_x': 0.03130074199289083, 'linear_y': 0.028357808757573367, 'linear_z': 0.0744575835764408, 'angle_phi': -0.06713105738162994, 'angle_theta': 0.06894744634628296, 'angle_psi': 1.1161083340644837, 'bbox': [{'top': 163, 'left': 1098, 'height': 185, 'width': 420, 'class': 1}, {'top': 421, 'left': 1128, 'height': 176, 'width': 393, 'class': 1}, {'top': 927, 'left': 1703, 'height': 153, 'width': 183, 'class': 0}]}\n"
     ]
    }
   ],
   "source": [
    "print(annotations.keys())  # Check top-level keys\n",
    "print(annotations['annotations'][0])  # Check the first annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba437b7",
   "metadata": {},
   "source": [
    " YOLOS-Tiny Object Detection Script (Train + Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c3c219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:f6akk5st) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-train</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/f6akk5st' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/f6akk5st</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250417_163349-f6akk5st\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:f6akk5st). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250417_164215-labp4rbw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/labp4rbw' target=\"_blank\">yolos-tiny-train</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/labp4rbw' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/labp4rbw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6565 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 301\u001b[0m\n\u001b[0;32m    298\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(val_annotations, f)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Run inference on validation set\u001b[39;00m\n\u001b[0;32m    304\u001b[0m pred_json \u001b[38;5;241m=\u001b[39m run_yolos_inference(model, val_dataset, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolos_pred.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[18], line 168\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    165\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    166\u001b[0m labels \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    170\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\yolos\\modeling_yolos.py:869\u001b[0m, in \u001b[0;36mYolosForObjectDetection.forward\u001b[1;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    867\u001b[0m         outputs_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_labels_classifier(intermediate)\n\u001b[0;32m    868\u001b[0m         outputs_coord \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_predictor(intermediate)\u001b[38;5;241m.\u001b[39msigmoid()\n\u001b[1;32m--> 869\u001b[0m     loss, loss_dict, auxiliary_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_coord\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m auxiliary_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:552\u001b[0m, in \u001b[0;36mForObjectDetectionLoss\u001b[1;34m(logits, labels, device, pred_boxes, config, outputs_class, outputs_coord, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m     auxiliary_outputs \u001b[38;5;241m=\u001b[39m _set_aux_loss(outputs_class, outputs_coord)\n\u001b[0;32m    550\u001b[0m     outputs_loss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauxiliary_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auxiliary_outputs\n\u001b[1;32m--> 552\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# Fourth: compute total loss, as a weighted sum of the various losses\u001b[39;00m\n\u001b[0;32m    554\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_ce\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_bbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mbbox_loss_coefficient}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:253\u001b[0m, in \u001b[0;36mImageLoss.forward\u001b[1;34m(self, outputs, targets)\u001b[0m\n\u001b[0;32m    250\u001b[0m outputs_without_aux \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauxiliary_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Retrieve the matching between the outputs of the last layer and the targets\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_without_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Compute the average number of target boxes across all nodes, for normalization purposes\u001b[39;00m\n\u001b[0;32m    256\u001b[0m num_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:341\u001b[0m, in \u001b[0;36mHungarianMatcher.forward\u001b[1;34m(self, outputs, targets)\u001b[0m\n\u001b[0;32m    338\u001b[0m out_bbox \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size * num_queries, 4]\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# Also concat the target labels and boxes\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m target_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[0;32m    342\u001b[0m target_bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Compute the classification cost. Contrary to the loss, we don't use the NLL,\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# but approximate it in 1 - proba[target class].\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# The 1 is a constant that doesn't change the matching, it can be ommitted.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class_labels'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ”§ Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ðŸŸ£ Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-train\")\n",
    "\n",
    "# âš™ï¸ Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model.to(device)\n",
    "\n",
    "# âš™ï¸ Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"train\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            # COCO-style bbox: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to [x_min, y_min, x_max, y_max] for YOLOS\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Convert to tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image and annotations\n",
    "        # Include \"area\" and \"iscrowd\" for COCO compatibility\n",
    "        processor_annotations = [\n",
    "            {\n",
    "                \"bbox\": [x, y, w, h],  # COCO format [x, y, width, height]\n",
    "                \"category_id\": l,\n",
    "                \"area\": float(w * h),  # Compute area\n",
    "                \"iscrowd\": 0  # Default to 0 (no crowd)\n",
    "            }\n",
    "            for (x, y, w, h), l in zip((ann[\"bbox\"] for ann in anns), labels)\n",
    "        ]\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": processor_annotations},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)  # Remove batch dimension\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "# ðŸ“‚ Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}  # Map image_id to image metadata\n",
    "coco_annotations = []  # COCO-style annotations\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1  # Assign unique image_id (1-based indexing)\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    # Convert bbox list to COCO-style annotations\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        coco_annotations.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": bbox[\"class\"],\n",
    "            \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "            \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "            \"id\": len(coco_annotations) + 1,  # Unique annotation ID\n",
    "        })\n",
    "\n",
    "# Create pseudo-COCO structure\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(raw_annotations[\"categories\"])],\n",
    "}\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:train_size]\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "train_images = [img for img in annotations[\"images\"] if img[\"id\"] in train_ids]\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "train_annotations = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "train_dataset = AUAIRDataset(train_annotations, img_dir, processor, split=\"train\")\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "# ðŸ§  Training Loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "            labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": avg_train_loss})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "                labels = [item[0][\"labels\"] for item in batch]\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"val_loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "    processor.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "\n",
    "# ðŸ§  Inference\n",
    "def run_yolos_inference(model, dataset, output_path=\"yolos_pred.json\", log_images=False):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        inputs, image, img_id, image_name = dataset[idx]\n",
    "        if inputs is None:\n",
    "            continue\n",
    "        inputs = {k: v.unsqueeze(0).to(device) for k, v in inputs.items() if k == \"pixel_values\"}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        width, height = image.size\n",
    "        target_sizes = torch.tensor([[height, width]]).to(device)\n",
    "        \n",
    "        result = processor.post_process_object_detection(\n",
    "            outputs,\n",
    "            target_sizes=target_sizes,\n",
    "            threshold=0.5\n",
    "        )[0]\n",
    "\n",
    "        if log_images and idx % 50 == 0 and len(result[\"boxes\"]) > 0:\n",
    "            boxes = result[\"boxes\"].cpu().tolist()\n",
    "            scores = result[\"scores\"].cpu().tolist()\n",
    "            labels = result[\"labels\"].cpu().tolist()\n",
    "\n",
    "            wandb.log({\n",
    "                \"prediction\": wandb.Image(image, boxes={\n",
    "                    \"predictions\": {\n",
    "                        \"box_data\": [\n",
    "                            {\n",
    "                                \"position\": {\n",
    "                                    \"minX\": b[0] / width,\n",
    "                                    \"minY\": b[1] / height,\n",
    "                                    \"maxX\": b[2] / width,\n",
    "                                    \"maxY\": b[3] / height,\n",
    "                                },\n",
    "                                \"score\": s,\n",
    "                                \"class_id\": l\n",
    "                            }\n",
    "                            for b, s, l in zip(boxes, scores, labels)\n",
    "                        ],\n",
    "                        \"class_labels\": {i: name for i, name in enumerate(raw_annotations[\"categories\"])}\n",
    "                    }\n",
    "                }),\n",
    "                \"step\": idx\n",
    "            })\n",
    "\n",
    "        for box, label, score in zip(result[\"boxes\"], result[\"labels\"], result[\"scores\"]):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": int(label),\n",
    "                \"bbox\": [float(xmin), float(ymin), float(xmax - xmin), float(ymax - ymin)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    return output_path\n",
    "\n",
    "# ðŸ“Š mAP Evaluation\n",
    "def evaluate_map(gt_path, pred_path):\n",
    "    coco_gt = COCO(gt_path)\n",
    "    coco_dt = coco_gt.loadRes(pred_path)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"mAP@[0.5:0.95]\": coco_eval.stats[0],\n",
    "        \"AP50\": coco_eval.stats[1],\n",
    "        \"AP75\": coco_eval.stats[2],\n",
    "        \"AP_small\": coco_eval.stats[3],\n",
    "        \"AP_medium\": coco_eval.stats[4],\n",
    "        \"AP_large\": coco_eval.stats[5]\n",
    "    }\n",
    "\n",
    "    precisions = coco_eval.eval[\"precision\"]\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    categories = coco_gt.loadCats(cat_ids)\n",
    "\n",
    "    print(\"\\nðŸ“Š Per-category AP (IoU=0.50:0.95):\")\n",
    "    for idx, cat in enumerate(categories):\n",
    "        precision = precisions[:, :, idx, 0, 0]\n",
    "        precision = precision[precision > -1]\n",
    "        ap = precision.mean() if precision.size > 0 else float(\"nan\")\n",
    "        metrics[f\"AP_{cat['name']}\"] = ap\n",
    "        print(f\"  {cat['name']:20s}: {ap:.4f}\")\n",
    "\n",
    "    wandb.log(metrics)\n",
    "    print(\"âœ… mAP + per-class AP metrics logged to W&B.\")\n",
    "    return metrics\n",
    "\n",
    "# ðŸ§ª Run Training and Evaluation\n",
    "# Save ground truth annotations to gt.json\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5)\n",
    "\n",
    "# Run inference on validation set\n",
    "pred_json = run_yolos_inference(model, val_dataset, output_path=\"yolos_pred.json\", log_images=True)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_map(\"gt.json\", pred_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb73cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8104f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:labp4rbw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-train</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/labp4rbw' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/labp4rbw</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250417_164215-labp4rbw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:labp4rbw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250417_164957-fw8xmppq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fw8xmppq' target=\"_blank\">yolos-tiny-train</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fw8xmppq' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fw8xmppq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6565 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument x2 in method wrapper_CUDA___cdist_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 204\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    202\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(val_annotations, f)\n\u001b[1;32m--> 204\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 169\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    166\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    167\u001b[0m labels \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m--> 169\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    171\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\yolos\\modeling_yolos.py:869\u001b[0m, in \u001b[0;36mYolosForObjectDetection.forward\u001b[1;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    867\u001b[0m         outputs_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_labels_classifier(intermediate)\n\u001b[0;32m    868\u001b[0m         outputs_coord \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_predictor(intermediate)\u001b[38;5;241m.\u001b[39msigmoid()\n\u001b[1;32m--> 869\u001b[0m     loss, loss_dict, auxiliary_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_coord\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m auxiliary_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:552\u001b[0m, in \u001b[0;36mForObjectDetectionLoss\u001b[1;34m(logits, labels, device, pred_boxes, config, outputs_class, outputs_coord, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m     auxiliary_outputs \u001b[38;5;241m=\u001b[39m _set_aux_loss(outputs_class, outputs_coord)\n\u001b[0;32m    550\u001b[0m     outputs_loss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauxiliary_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auxiliary_outputs\n\u001b[1;32m--> 552\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# Fourth: compute total loss, as a weighted sum of the various losses\u001b[39;00m\n\u001b[0;32m    554\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_ce\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_bbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mbbox_loss_coefficient}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:253\u001b[0m, in \u001b[0;36mImageLoss.forward\u001b[1;34m(self, outputs, targets)\u001b[0m\n\u001b[0;32m    250\u001b[0m outputs_without_aux \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauxiliary_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Retrieve the matching between the outputs of the last layer and the targets\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_without_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Compute the average number of target boxes across all nodes, for normalization purposes\u001b[39;00m\n\u001b[0;32m    256\u001b[0m num_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\loss\\loss_for_object_detection.py:350\u001b[0m, in \u001b[0;36mHungarianMatcher.forward\u001b[1;34m(self, outputs, targets)\u001b[0m\n\u001b[0;32m    347\u001b[0m class_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mout_prob[:, target_ids]\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# Compute the L1 cost between boxes\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m bbox_cost \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# Compute the giou cost between boxes\u001b[39;00m\n\u001b[0;32m    353\u001b[0m giou_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mgeneralized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\functional.py:1478\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1475\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode\n\u001b[0;32m   1476\u001b[0m     )\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument x2 in method wrapper_CUDA___cdist_forward)"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ”§ Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ðŸŸ£ Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-train\")\n",
    "\n",
    "# âš™ï¸ Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model.to(device)\n",
    "\n",
    "# âš™ï¸ Category ID mapping (AUAIR to COCO)\n",
    "label_map = {0: 1, 1: 3, 2: 8, 3: 7, 4: 4, 5: 2, 6: 6, 7: 10}  # AUAIR to COCO\n",
    "\n",
    "# âš™ï¸ Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"train\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            # COCO-style bbox: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to [x_min, y_min, x_max, y_max] for YOLOS\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(label_map[ann[\"category_id\"]])  # Map AUAIR to COCO IDs\n",
    "\n",
    "        # Convert to tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image and annotations\n",
    "        # Include \"area\" and \"iscrowd\" for COCO compatibility\n",
    "        processor_annotations = [\n",
    "            {\n",
    "                \"bbox\": [x, y, w, h],  # COCO format [x, y, width, height]\n",
    "                \"category_id\": label_map[l],  # Map AUAIR to COCO IDs\n",
    "                \"area\": float(w * h),  # Compute area\n",
    "                \"iscrowd\": 0  # Default to 0 (no crowd)\n",
    "            }\n",
    "            for (x, y, w, h), l in zip((ann[\"bbox\"] for ann in anns), [ann[\"category_id\"] for ann in anns])\n",
    "        ]\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": processor_annotations},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)  # Remove batch dimension\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "# ðŸ“‚ Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}  # Map image_id to image metadata\n",
    "coco_annotations = []  # COCO-style annotations\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1  # Assign unique image_id (1-based indexing)\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    # Convert bbox list to COCO-style annotations\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        coco_annotations.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": bbox[\"class\"],\n",
    "            \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "            \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "            \"id\": len(coco_annotations) + 1,  # Unique annotation ID\n",
    "        })\n",
    "\n",
    "# Create pseudo-COCO structure\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(raw_annotations[\"categories\"])],\n",
    "}\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:train_size]\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "train_images = [img for img in annotations[\"images\"] if img[\"id\"] in train_ids]\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "train_annotations = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "train_dataset = AUAIRDataset(train_annotations, img_dir, processor, split=\"train\")\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "# ðŸ§  Training Loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "            labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "            # Move all tensors in labels to the correct device\n",
    "            labels = [\n",
    "                {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                for label in labels\n",
    "            ]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": avg_train_loss})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "                labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "                # Move all tensors in labels to the correct device\n",
    "                labels = [\n",
    "                    {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                    for label in labels\n",
    "                ]\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"val_loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "    processor.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "\n",
    "# ðŸ§ª Run Training\n",
    "# Save ground truth annotations for evaluation\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01253cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d68dc7",
   "metadata": {},
   "source": [
    "INFERENCE PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ”§ Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ðŸŸ£ Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-inference\")\n",
    "\n",
    "# âš™ï¸ Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"yolos-tiny-finetuned\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"yolos-tiny-finetuned\")\n",
    "model.to(device)\n",
    "\n",
    "# âš™ï¸ Category ID mapping (COCO to AUAIR for evaluation)\n",
    "reverse_map = {1: 0, 3: 1, 8: 2, 7: 3, 4: 4, 2: 5, 6: 6, 10: 7}  # COCO to AUAIR\n",
    "\n",
    "# âš™ï¸ Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"val\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image (not used for inference, but kept for compatibility)\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Convert to tensors (not used for inference, but kept for compatibility)\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image\n",
    "        encoding = self.processor(images=image, return_tensors=\"pt\")\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)  # Remove batch dimension\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "# ðŸ“‚ Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}  # Map image_id to image metadata\n",
    "coco_annotations = []  # COCO-style annotations\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1  # Assign unique image_id (1-based indexing)\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        coco_annotations.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": bbox[\"class\"],\n",
    "            \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "            \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "            \"id\": len(coco_annotations) + 1,\n",
    "        })\n",
    "\n",
    "# Create pseudo-COCO structure\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(raw_annotations[\"categories\"])],\n",
    "}\n",
    "\n",
    "# Split dataset (use validation set for inference)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# ðŸ§  Inference\n",
    "def run_yolos_inference(model, dataset, output_path=\"yolos_pred.json\", log_images=False):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        inputs, image, img_id, image_name = dataset[idx]\n",
    "        if inputs is None:\n",
    "            continue\n",
    "        inputs = {k: v.unsqueeze(0).to(device) for k, v in inputs.items() if k == \"pixel_values\"}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        width, height = image.size\n",
    "        target_sizes = torch.tensor([[height, width]]).to(device)\n",
    "        \n",
    "        result = processor.post_process_object_detection(\n",
    "            outputs,\n",
    "            target_sizes=target_sizes,\n",
    "            threshold=0.5\n",
    "        )[0]\n",
    "\n",
    "        if log_images and idx % 50 == 0 and len(result[\"boxes\"]) > 0:\n",
    "            boxes = result[\"boxes\"].cpu().tolist()\n",
    "            scores = result[\"scores\"].cpu().tolist()\n",
    "            labels = result[\"labels\"].cpu().tolist()\n",
    "\n",
    "            wandb.log({\n",
    "                \"prediction\": wandb.Image(image, boxes={\n",
    "                    \"predictions\": {\n",
    "                        \"box_data\": [\n",
    "                            {\n",
    "                                \"position\": {\n",
    "                                    \"minX\": b[0] / width,\n",
    "                                    \"minY\": b[1] / height,\n",
    "                                    \"maxX\": b[2] / width,\n",
    "                                    \"maxY\": b[3] / height,\n",
    "                                },\n",
    "                                \"score\": s,\n",
    "                                \"class_id\": reverse_map.get(l, l)  # Map back to AUAIR IDs for logging\n",
    "                            }\n",
    "                            for b, s, l in zip(boxes, scores, labels)\n",
    "                        ],\n",
    "                        \"class_labels\": {i: name for i, name in enumerate(raw_annotations[\"categories\"])}\n",
    "                    }\n",
    "                }),\n",
    "                \"step\": idx\n",
    "            })\n",
    "\n",
    "        for box, label, score in zip(result[\"boxes\"], result[\"labels\"], result[\"scores\"]):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": reverse_map.get(int(label), int(label)),  # Map COCO to AUAIR IDs\n",
    "                \"bbox\": [float(xmin), float(ymin), float(xmax - xmin), float(ymax - ymin)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    return output_path\n",
    "\n",
    "# ðŸ§ª Run Inference\n",
    "pred_json = run_yolos_inference(model, val_dataset, output_path=\"yolos_pred.json\", log_images=True)\n",
    "print(f\"Predictions saved to {pred_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91085e2",
   "metadata": {},
   "source": [
    "EVALUATION PART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ”§ Paths\n",
    "gt_path = \"gt.json\"\n",
    "pred_path = \"yolos_pred.json\"\n",
    "\n",
    "# ðŸŸ£ Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-evaluate\")\n",
    "\n",
    "# ðŸ“Š mAP Evaluation\n",
    "def evaluate_map(gt_path, pred_path):\n",
    "    coco_gt = COCO(gt_path)\n",
    "    coco_dt = coco_gt.loadRes(pred_path)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"mAP@[0.5:0.95]\": coco_eval.stats[0],\n",
    "        \"AP50\": coco_eval.stats[1],\n",
    "        \"AP75\": coco_eval.stats[2],\n",
    "        \"AP_small\": coco_eval.stats[3],\n",
    "        \"AP_medium\": coco_eval.stats[4],\n",
    "        \"AP_large\": coco_eval.stats[5]\n",
    "    }\n",
    "\n",
    "    precisions = coco_eval.eval[\"precision\"]\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    categories = coco_gt.loadCats(cat_ids)\n",
    "\n",
    "    print(\"\\nðŸ“Š Per-category AP (IoU=0.50:0.95):\")\n",
    "    for idx, cat in enumerate(categories):\n",
    "        precision = precisions[:, :, idx, 0, 0]\n",
    "        precision = precision[precision > -1]\n",
    "        ap = precision.mean() if precision.size > 0 else float(\"nan\")\n",
    "        metrics[f\"AP_{cat['name']}\"] = ap\n",
    "        print(f\"  {cat['name']:20s}: {ap:.4f}\")\n",
    "\n",
    "    wandb.log(metrics)\n",
    "    print(\"âœ… mAP + per-class AP metrics logged to W&B.\")\n",
    "    return metrics\n",
    "\n",
    "# ðŸ§ª Run Evaluation\n",
    "metrics = evaluate_map(gt_path, pred_path)\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d30290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
