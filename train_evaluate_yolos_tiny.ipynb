{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d7be84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'annotations'])\n",
      "{'image_name': 'frame_20190829091111_x_0001973.jpg', 'image_width:': 1920.0, 'image_height': 1080.0, 'platform': 'Parrot Bebop 2', 'time': {'year': 2019, 'month': 8, 'day': 29, 'hour': 9, 'min': 11, 'sec': 11, 'ms': 394400.0}, 'longtitude': 10.18798203255313, 'latitude': 56.20630134795274, 'altitude': 19921.6, 'linear_x': 0.03130074199289083, 'linear_y': 0.028357808757573367, 'linear_z': 0.0744575835764408, 'angle_phi': -0.06713105738162994, 'angle_theta': 0.06894744634628296, 'angle_psi': 1.1161083340644837, 'bbox': [{'top': 163, 'left': 1098, 'height': 185, 'width': 420, 'class': 1}, {'top': 421, 'left': 1128, 'height': 176, 'width': 393, 'class': 1}, {'top': 927, 'left': 1703, 'height': 153, 'width': 183, 'class': 0}]}\n"
     ]
    }
   ],
   "source": [
    "print(annotations.keys())  # Check top-level keys\n",
    "print(annotations['annotations'][0])  # Check the first annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba437b7",
   "metadata": {},
   "source": [
    " YOLOS-Tiny Object Detection Script (Train + Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3c219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cj9ujv8c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-train</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/cj9ujv8c' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/cj9ujv8c</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250417_162642-cj9ujv8c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cj9ujv8c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250417_162820-btni46im</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/btni46im' target=\"_blank\">yolos-tiny-train</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/btni46im' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/btni46im</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6565 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 291\u001b[0m\n\u001b[0;32m    288\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(val_annotations, f)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Run inference on validation set\u001b[39;00m\n\u001b[0;32m    294\u001b[0m pred_json \u001b[38;5;241m=\u001b[39m run_yolos_inference(model, val_dataset, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolos_pred.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[15], line 153\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    151\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    152\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 153\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[15], line 62\u001b[0m, in \u001b[0;36mAUAIRDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m anns:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 62\u001b[0m         x, y, w, h \u001b[38;5;241m=\u001b[39m \u001b[43mbbox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, bbox[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m\"\u001b[39m], bbox[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m], bbox[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;66;03m# Convert to [x_min, y_min, x_max, y_max] for YOLOS\u001b[39;00m\n\u001b[0;32m     64\u001b[0m         boxes\u001b[38;5;241m.\u001b[39mappend([x, y, x \u001b[38;5;241m+\u001b[39m w, y \u001b[38;5;241m+\u001b[39m h])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "\n",
    "# 🔧 Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 🟣 Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-train\")\n",
    "\n",
    "# ⚙️ Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model.to(device)\n",
    "\n",
    "# ⚙️ Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"train\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            # COCO-style bbox: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to [x_min, y_min, x_max, y_max] for YOLOS\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Convert to tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image and annotations\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": [{\"bbox\": b, \"category_id\": l} for b, l in zip(boxes, labels)]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)  # Remove batch dimension\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "# 📂 Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}  # Map image_id to image metadata\n",
    "coco_annotations = []  # COCO-style annotations\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1  # Assign unique image_id (1-based indexing)\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    # Convert bbox list to COCO-style annotations\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        coco_annotations.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": bbox[\"class\"],\n",
    "            \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "            \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "            \"id\": len(coco_annotations) + 1,  # Unique annotation ID\n",
    "        })\n",
    "\n",
    "# Create pseudo-COCO structure\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(raw_annotations[\"categories\"])],\n",
    "}\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:train_size]\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "train_images = [img for img in annotations[\"images\"] if img[\"id\"] in train_ids]\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "train_annotations = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "train_dataset = AUAIRDataset(train_annotations, img_dir, processor, split=\"train\")\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "# 🧠 Training Loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "            labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": avg_train_loss})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "                labels = [item[0][\"labels\"] for item in batch]\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"val_loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "    processor.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "\n",
    "# 🧠 Inference\n",
    "def run_yolos_inference(model, dataset, output_path=\"yolos_pred.json\", log_images=False):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        inputs, image, img_id, image_name = dataset[idx]\n",
    "        if inputs is None:\n",
    "            continue\n",
    "        inputs = {k: v.unsqueeze(0).to(device) for k, v in inputs.items() if k == \"pixel_values\"}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        width, height = image.size\n",
    "        target_sizes = torch.tensor([[height, width]]).to(device)\n",
    "        \n",
    "        result = processor.post_process_object_detection(\n",
    "            outputs,\n",
    "            target_sizes=target_sizes,\n",
    "            threshold=0.5\n",
    "        )[0]\n",
    "\n",
    "        if log_images and idx % 50 == 0 and len(result[\"boxes\"]) > 0:\n",
    "            boxes = result[\"boxes\"].cpu().tolist()\n",
    "            scores = result[\"scores\"].cpu().tolist()\n",
    "            labels = result[\"labels\"].cpu().tolist()\n",
    "\n",
    "            wandb.log({\n",
    "                \"prediction\": wandb.Image(image, boxes={\n",
    "                    \"predictions\": {\n",
    "                        \"box_data\": [\n",
    "                            {\n",
    "                                \"position\": {\n",
    "                                    \"minX\": b[0] / width,\n",
    "                                    \"minY\": b[1] / height,\n",
    "                                    \"maxX\": b[2] / width,\n",
    "                                    \"maxY\": b[3] / height,\n",
    "                                },\n",
    "                                \"score\": s,\n",
    "                                \"class_id\": l\n",
    "                            }\n",
    "                            for b, s, l in zip(boxes, scores, labels)\n",
    "                        ],\n",
    "                        \"class_labels\": {i: name for i, name in enumerate(raw_annotations[\"categories\"])}\n",
    "                    }\n",
    "                }),\n",
    "                \"step\": idx\n",
    "            })\n",
    "\n",
    "        for box, label, score in zip(result[\"boxes\"], result[\"labels\"], result[\"scores\"]):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": int(label),\n",
    "                \"bbox\": [float(xmin), float(ymin), float(xmax - xmin), float(ymax - ymin)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    return output_path\n",
    "\n",
    "# 📊 mAP Evaluation\n",
    "def evaluate_map(gt_path, pred_path):\n",
    "    coco_gt = COCO(gt_path)\n",
    "    coco_dt = coco_gt.loadRes(pred_path)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"mAP@[0.5:0.95]\": coco_eval.stats[0],\n",
    "        \"AP50\": coco_eval.stats[1],\n",
    "        \"AP75\": coco_eval.stats[2],\n",
    "        \"AP_small\": coco_eval.stats[3],\n",
    "        \"AP_medium\": coco_eval.stats[4],\n",
    "        \"AP_large\": coco_eval.stats[5]\n",
    "    }\n",
    "\n",
    "    precisions = coco_eval.eval[\"precision\"]\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    categories = coco_gt.loadCats(cat_ids)\n",
    "\n",
    "    print(\"\\n📊 Per-category AP (IoU=0.50:0.95):\")\n",
    "    for idx, cat in enumerate(categories):\n",
    "        precision = precisions[:, :, idx, 0, 0]\n",
    "        precision = precision[precision > -1]\n",
    "        ap = precision.mean() if precision.size > 0 else float(\"nan\")\n",
    "        metrics[f\"AP_{cat['name']}\"] = ap\n",
    "        print(f\"  {cat['name']:20s}: {ap:.4f}\")\n",
    "\n",
    "    wandb.log(metrics)\n",
    "    print(\"✅ mAP + per-class AP metrics logged to W&B.\")\n",
    "    return metrics\n",
    "\n",
    "# 🧪 Run Training and Evaluation\n",
    "# Save ground truth annotations to gt.json\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5)\n",
    "\n",
    "# Run inference on validation set\n",
    "pred_json = run_yolos_inference(model, val_dataset, output_path=\"yolos_pred.json\", log_images=True)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_map(\"gt.json\", pred_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb73cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01253cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
