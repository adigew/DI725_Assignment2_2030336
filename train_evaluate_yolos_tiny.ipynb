{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d7be84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'annotations'])\n",
      "{'image_name': 'frame_20190829091111_x_0001973.jpg', 'image_width:': 1920.0, 'image_height': 1080.0, 'platform': 'Parrot Bebop 2', 'time': {'year': 2019, 'month': 8, 'day': 29, 'hour': 9, 'min': 11, 'sec': 11, 'ms': 394400.0}, 'longtitude': 10.18798203255313, 'latitude': 56.20630134795274, 'altitude': 19921.6, 'linear_x': 0.03130074199289083, 'linear_y': 0.028357808757573367, 'linear_z': 0.0744575835764408, 'angle_phi': -0.06713105738162994, 'angle_theta': 0.06894744634628296, 'angle_psi': 1.1161083340644837, 'bbox': [{'top': 163, 'left': 1098, 'height': 185, 'width': 420, 'class': 1}, {'top': 421, 'left': 1128, 'height': 176, 'width': 393, 'class': 1}, {'top': 927, 'left': 1703, 'height': 153, 'width': 183, 'class': 0}]}\n"
     ]
    }
   ],
   "source": [
    "print(annotations.keys())  # Check top-level keys\n",
    "print(annotations['annotations'][0])  # Check the first annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba437b7",
   "metadata": {},
   "source": [
    " YOLOS-Tiny Object Detection Script (Train + Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8104f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fw8xmppq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-train</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fw8xmppq' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fw8xmppq</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250417_164957-fw8xmppq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fw8xmppq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250417_165554-fynpym4j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fynpym4j' target=\"_blank\">yolos-tiny-train</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fynpym4j' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/fynpym4j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6565/6565 [58:53<00:00,  1.86it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [10:40<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 13666.2197, Val Loss: 13905.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6565/6565 [57:54<00:00,  1.89it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [10:39<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 13660.2704, Val Loss: 13905.3763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6565/6565 [58:05<00:00,  1.88it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [10:37<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 13667.5493, Val Loss: 13905.3745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6565/6565 [58:17<00:00,  1.88it/s] \n",
      "Validation: 100%|██████████| 1642/1642 [10:51<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 13667.5583, Val Loss: 13905.3575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6565/6565 [58:00<00:00,  1.89it/s] \n",
      "Validation: 100%|██████████| 1642/1642 [10:18<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 13661.5152, Val Loss: 13905.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6565/6565 [57:31<00:00,  1.90it/s]  \n",
      "Validation: 100%|██████████| 1642/1642 [09:04<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 13672.2140, Val Loss: 13905.3667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 11/6565 [00:05<51:31,  2.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 217\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    215\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(val_annotations, f)\n\u001b[1;32m--> 217\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 180\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    177\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    178\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 180\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m    183\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_train_loss})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "#  Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#  Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-train\")\n",
    "\n",
    "#  Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model.to(device)\n",
    "\n",
    "#  Category ID mapping (AUAIR to COCO)\n",
    "label_map = {0: 1, 1: 3, 2: 8, 3: 7, 4: 4, 5: 2, 6: 6, 7: 10}  # AUAIR to COCO\n",
    "\n",
    "#  Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"train\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            # COCO-style bbox: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to [x_min, y_min, x_max, y_max] for YOLOS\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(label_map[ann[\"category_id\"]])  # Map AUAIR to COCO IDs\n",
    "\n",
    "        # Convert to tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image and annotations\n",
    "        # Include \"area\" and \"iscrowd\" for COCO compatibility\n",
    "        processor_annotations = [\n",
    "            {\n",
    "                \"bbox\": [x, y, w, h],  # COCO format [x, y, width, height]\n",
    "                \"category_id\": label_map[l],  # Map AUAIR to COCO IDs\n",
    "                \"area\": float(w * h),  # Compute area\n",
    "                \"iscrowd\": 0  # Default to 0 (no crowd)\n",
    "            }\n",
    "            for (x, y, w, h), l in zip((ann[\"bbox\"] for ann in anns), [ann[\"category_id\"] for ann in anns])\n",
    "        ]\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": processor_annotations},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)  # Remove batch dimension\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "#  Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}  # Map image_id to image metadata\n",
    "coco_annotations = []  # COCO-style annotations\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1  # Assign unique image_id (1-based indexing)\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    # Convert bbox list to COCO-style annotations\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        coco_annotations.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": bbox[\"class\"],\n",
    "            \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "            \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "            \"id\": len(coco_annotations) + 1,  # Unique annotation ID\n",
    "        })\n",
    "\n",
    "# Create pseudo-COCO structure\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(raw_annotations[\"categories\"])],\n",
    "}\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:train_size]\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "train_images = [img for img in annotations[\"images\"] if img[\"id\"] in train_ids]\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "train_annotations = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "train_dataset = AUAIRDataset(train_annotations, img_dir, processor, split=\"train\")\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "#  Training Loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "            labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "            # Move all tensors in labels to the correct device\n",
    "            labels = [\n",
    "                {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                for label in labels\n",
    "            ]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": avg_train_loss})\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "                labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "                # Move all tensors in labels to the correct device\n",
    "                labels = [\n",
    "                    {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                    for label in labels\n",
    "                ]\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        wandb.log({\"epoch\": epoch, \"val_loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "    processor.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "\n",
    "# Run Training\n",
    "# Save ground truth annotations for evaluation\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb7d2b",
   "metadata": {},
   "source": [
    "Fixed Classifier Head Adjustment:\n",
    "Replaced model.config.d_model with model.config.hidden_size to get the correct input dimension (768 for yolos-tiny).\n",
    "Set class_labels_classifier to a torch.nn.Linear layer with hidden_size input and num_classes (9) output.\n",
    "Added Gradient Clipping:\n",
    "Added torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) before optimizer.step() to stabilize training and address the high loss (~13,660) observed previously.\n",
    "Preserved Improvements:\n",
    "Kept bounding box normalization ([x_min, y_min, x_max, y_max] in [0, 1]).\n",
    "Retained loss component logging (loss_ce, loss_bbox, loss_giou).\n",
    "Maintained debugging for category IDs and sample counts.\n",
    "Kept lr=1e-4 for faster convergence.\n",
    "Category IDs:\n",
    "Used AUAIR IDs (0–7) directly, as the classifier head is adjusted to match AUAIR categories (0–7 for classes, 8 for background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01253cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:84um06jj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-train</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/84um06jj' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/84um06jj</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250419_162654-84um06jj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:84um06jj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250419_163135-bnaqmrkw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/bnaqmrkw' target=\"_blank\">yolos-tiny-train</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/bnaqmrkw' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/bnaqmrkw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model id2label: {0: 'N/A', 1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 12: 'N/A', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 26: 'N/A', 27: 'backpack', 28: 'umbrella', 29: 'N/A', 30: 'N/A', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 45: 'N/A', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 66: 'N/A', 67: 'dining table', 68: 'N/A', 69: 'N/A', 70: 'toilet', 71: 'N/A', 72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 83: 'N/A', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n",
      "AUAIR categories: ['Human', 'Car', 'Truck', 'Van', 'Motorbike', 'Bicycle', 'Bus', 'Trailer']\n",
      "Unique category IDs in raw annotations: {0, 1, 2, 3, 4, 5, 6, 7}\n",
      "invalid_category_count: 0\n",
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n",
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n",
      "Sample 1, Image ID: 2, Labels: [3, 7]\n",
      "Valid training samples: 26258, Valid validation samples: 6565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  41%|████      | 2687/6565 [20:35<30:05,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  49%|████▉     | 3220/6565 [24:42<25:34,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  74%|███████▍  | 4848/6565 [37:08<12:48,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6565/6565 [50:08<00:00,  2.18it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:18<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.2621 (CE: 0.4123, BBox: 0.0864, GIoU: 0.2088), Val Loss: 0.9616 (CE: 0.3023, BBox: 0.0650, GIoU: 0.1672)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  23%|██▎       | 1539/6565 [11:37<38:12,  2.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  27%|██▋       | 1788/6565 [13:30<36:28,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  77%|███████▋  | 5050/6565 [38:14<11:29,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6565/6565 [49:40<00:00,  2.20it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:18<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 1.0084 (CE: 0.3170, BBox: 0.0684, GIoU: 0.1747), Val Loss: 0.9990 (CE: 0.3101, BBox: 0.0689, GIoU: 0.1723)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   5%|▍         | 311/6565 [02:18<46:41,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  49%|████▉     | 3241/6565 [24:22<25:13,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  61%|██████    | 3995/6565 [30:04<19:37,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6565/6565 [49:29<00:00,  2.21it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:18<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.9619 (CE: 0.3142, BBox: 0.0635, GIoU: 0.1650), Val Loss: 0.9217 (CE: 0.2860, BBox: 0.0624, GIoU: 0.1618)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   8%|▊         | 499/6565 [03:43<45:19,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  17%|█▋        | 1117/6565 [08:23<40:51,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  17%|█▋        | 1126/6565 [08:27<40:59,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6565/6565 [49:36<00:00,  2.21it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:19<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.7676 (CE: 0.2482, BBox: 0.0500, GIoU: 0.1347), Val Loss: 0.7334 (CE: 0.2406, BBox: 0.0480, GIoU: 0.1264)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  27%|██▋       | 1768/6565 [13:07<36:28,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  67%|██████▋   | 4384/6565 [32:37<16:18,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  77%|███████▋  | 5058/6565 [37:38<11:16,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6565/6565 [48:51<00:00,  2.24it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:08<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.7223 (CE: 0.2391, BBox: 0.0465, GIoU: 0.1253), Val Loss: 0.7158 (CE: 0.2379, BBox: 0.0464, GIoU: 0.1230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  32%|███▏      | 2095/6565 [15:30<33:22,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  51%|█████▏    | 3369/6565 [24:59<23:39,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  59%|█████▉    | 3903/6565 [28:57<19:58,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6565/6565 [48:48<00:00,  2.24it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:06<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.7026 (CE: 0.2342, BBox: 0.0450, GIoU: 0.1217), Val Loss: 0.7045 (CE: 0.2338, BBox: 0.0455, GIoU: 0.1217)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  18%|█▊        | 1170/6565 [08:40<40:42,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  33%|███▎      | 2156/6565 [15:59<32:43,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  67%|██████▋   | 4366/6565 [32:25<16:32,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6565/6565 [48:49<00:00,  2.24it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:06<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.6801 (CE: 0.2276, BBox: 0.0434, GIoU: 0.1176), Val Loss: 0.6918 (CE: 0.2324, BBox: 0.0444, GIoU: 0.1188)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  15%|█▍        | 977/6565 [07:14<41:22,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  21%|██        | 1376/6565 [10:13<38:45,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  77%|███████▋  | 5080/6565 [38:17<11:06,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6565/6565 [49:31<00:00,  2.21it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:10<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.6727 (CE: 0.2259, BBox: 0.0429, GIoU: 0.1162), Val Loss: 0.6905 (CE: 0.2325, BBox: 0.0443, GIoU: 0.1184)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  72%|███████▏  | 4747/6565 [35:51<13:35,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  76%|███████▌  | 5002/6565 [37:47<11:47,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  98%|█████████▊| 6428/6565 [48:33<01:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6565/6565 [49:35<00:00,  2.21it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:14<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.6677 (CE: 0.2251, BBox: 0.0424, GIoU: 0.1153), Val Loss: 0.6888 (CE: 0.2318, BBox: 0.0442, GIoU: 0.1180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  30%|███       | 1991/6565 [14:53<34:20,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2, Image ID: 3, Labels: [7, 7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  36%|███▌      | 2371/6565 [17:45<31:08,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1, Image ID: 2, Labels: [3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  96%|█████████▌| 6310/6565 [47:23<01:53,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Image ID: 1, Labels: [7, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6565/6565 [49:18<00:00,  2.22it/s]\n",
      "Validation: 100%|██████████| 1642/1642 [08:15<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.6644 (CE: 0.2239, BBox: 0.0422, GIoU: 0.1147), Val Loss: 0.6879 (CE: 0.2311, BBox: 0.0442, GIoU: 0.1180)\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "#  Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#  Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-train\")\n",
    "\n",
    "#  Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model.to(device)\n",
    "\n",
    "#  Verify category mapping\n",
    "print(\"Model id2label:\", model.config.id2label)\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "    print(\"AUAIR categories:\", raw_annotations[\"categories\"])\n",
    "\n",
    "#  Category ID mapping (AUAIR to COCO)\n",
    "label_map = {0: 1, 1: 3, 2: 8, 3: 7, 4: 4, 5: 2, 6: 6, 7: 10}  # AUAIR to COCO\n",
    "\n",
    "#  Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"train\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.split = split\n",
    "        # Map image_id to annotations for efficient lookup\n",
    "        self.ann_by_image_id = {}\n",
    "        self.invalid_category_count = 0  # Track invalid category IDs\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        img_width, img_height = img_info[\"width\"], img_info[\"height\"]\n",
    "        for ann in anns:\n",
    "            # COCO-style bbox: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to [x_min, y_min, x_max, y_max] and normalize\n",
    "            x_min, y_min = x / img_width, y / img_height\n",
    "            x_max, y_max = (x + w) / img_width, (y + h) / img_height\n",
    "            if ann[\"category_id\"] in label_map:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(label_map[ann[\"category_id\"]])  # Map AUAIR IDs to COCO IDs\n",
    "            else:\n",
    "                self.invalid_category_count += 1\n",
    "                #print(f\"Invalid category_id {ann['category_id']} in image_id {img_id}, annotation: {ann}\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        # Process image and annotations\n",
    "        processor_annotations = [\n",
    "            {\n",
    "                \"bbox\": [x, y, w, h],  # COCO format [x, y, width, height]\n",
    "                \"category_id\": label_map[l],  # Map AUAIR IDs to COCO IDs\n",
    "                \"area\": float(w * h),\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            for (x, y, w, h), l in zip((ann[\"bbox\"] for ann in anns if ann[\"category_id\"] in label_map), \n",
    "                                     [ann[\"category_id\"] for ann in anns if ann[\"category_id\"] in label_map])\n",
    "        ]\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": processor_annotations},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)\n",
    "        encoding[\"labels\"] = target\n",
    "\n",
    "        # Debug: Log category IDs for first few samples\n",
    "        if idx < 3 and self.split == \"train\":\n",
    "            print(f\"Sample {idx}, Image ID: {img_id}, Labels: {labels}\")\n",
    "\n",
    "        return encoding, image, img_id, img_info[\"file_name\"]\n",
    "\n",
    "#  Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    raw_annotations = json.load(f)\n",
    "\n",
    "# Debug: Check unique category IDs in raw annotations\n",
    "unique_category_ids = set()\n",
    "for ann in raw_annotations[\"annotations\"]:\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        unique_category_ids.add(bbox[\"class\"])\n",
    "print(f\"Unique category IDs in raw annotations: {unique_category_ids}\")\n",
    "\n",
    "# Create pseudo-COCO format\n",
    "image_map = {}\n",
    "coco_annotations = []\n",
    "invalid_annotation_count = 0  # Track invalid annotations during processing\n",
    "for idx, ann in enumerate(raw_annotations[\"annotations\"]):\n",
    "    img_name = ann[\"image_name\"]\n",
    "    img_id = idx + 1\n",
    "    image_map[img_id] = {\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": ann[\"image_width:\"],\n",
    "        \"height\": ann[\"image_height\"],\n",
    "    }\n",
    "    for bbox in ann[\"bbox\"]:\n",
    "        if bbox[\"class\"] in label_map:  # Only include valid category IDs\n",
    "            coco_annotations.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": label_map[bbox[\"class\"]],  # Map AUAIR IDs to COCO IDs\n",
    "                \"bbox\": [bbox[\"left\"], bbox[\"top\"], bbox[\"width\"], bbox[\"height\"]],\n",
    "                \"area\": bbox[\"width\"] * bbox[\"height\"],\n",
    "                \"id\": len(coco_annotations) + 1,\n",
    "            })\n",
    "        else:\n",
    "            #print(f\"Skipping annotation with invalid category_id {bbox['class']} in image {img_name}\")\n",
    "            invalid_annotation_count += 1\n",
    "\n",
    "\n",
    "# Print invalid annotation count from annotation processing\n",
    "print(f\"invalid_category_count: {invalid_annotation_count}\")\n",
    "\n",
    "annotations = {\n",
    "    \"images\": [{\"id\": img_id, \"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]} for img_id, img in image_map.items()],\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": [{\"id\": i, \"name\": model.config.id2label[i]} for i in label_map.values()],\n",
    "}\n",
    "\n",
    "# Split dataset (80% train, 20% val)\n",
    "np.random.seed(42)\n",
    "img_ids = [img[\"id\"] for img in annotations[\"images\"]]\n",
    "np.random.shuffle(img_ids)\n",
    "train_size = int(0.8 * len(img_ids))\n",
    "train_ids = img_ids[:train_size]\n",
    "val_ids = img_ids[train_size:]\n",
    "\n",
    "train_images = [img for img in annotations[\"images\"] if img[\"id\"] in train_ids]\n",
    "val_images = [img for img in annotations[\"images\"] if img[\"id\"] in val_ids]\n",
    "train_annotations = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in train_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "val_annotations = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": [ann for ann in annotations[\"annotations\"] if ann[\"image_id\"] in val_ids],\n",
    "    \"categories\": annotations[\"categories\"],\n",
    "}\n",
    "\n",
    "train_dataset = AUAIRDataset(train_annotations, img_dir, processor, split=\"train\")\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "#  Training Loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Count valid samples\n",
    "    valid_train_samples = 0\n",
    "    for batch in train_loader:\n",
    "        valid_train_samples += len(batch)\n",
    "    valid_val_samples = 0\n",
    "    for batch in val_loader:\n",
    "        valid_val_samples += len(batch)\n",
    "    print(f\"Valid training samples: {valid_train_samples}, Valid validation samples: {valid_val_samples}\")\n",
    "    #print(f\"Invalid category IDs encountered (train): {train_dataset.invalid_category_count}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss_ce = 0.0\n",
    "        train_loss_bbox = 0.0\n",
    "        train_loss_giou = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "            labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "            # Move all tensors in labels to the correct device\n",
    "            labels = [\n",
    "                {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                for label in labels\n",
    "            ]\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss_dict = outputs.loss_dict  # Contains loss_ce, loss_bbox, loss_giou\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_loss_ce += loss_dict.get(\"loss_ce\", 0.0).item()\n",
    "            train_loss_bbox += loss_dict.get(\"loss_bbox\", 0.0).item()\n",
    "            train_loss_giou += loss_dict.get(\"loss_giou\", 0.0).item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_loss_ce = train_loss_ce / len(train_loader)\n",
    "        avg_train_loss_bbox = train_loss_bbox / len(train_loader)\n",
    "        avg_train_loss_giou = train_loss_giou / len(train_loader)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_loss_ce\": avg_train_loss_ce,\n",
    "            \"train_loss_bbox\": avg_train_loss_bbox,\n",
    "            \"train_loss_giou\": avg_train_loss_giou\n",
    "        })\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loss_ce = 0.0\n",
    "        val_loss_bbox = 0.0\n",
    "        val_loss_giou = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                pixel_values = torch.stack([item[0][\"pixel_values\"] for item in batch]).to(device)\n",
    "                labels = [item[0][\"labels\"] for item in batch]\n",
    "\n",
    "                labels = [\n",
    "                    {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in label.items()}\n",
    "                    for label in labels\n",
    "                ]\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                val_loss_ce += outputs.loss_dict.get(\"loss_ce\", 0.0).item()\n",
    "                val_loss_bbox += outputs.loss_dict.get(\"loss_bbox\", 0.0).item()\n",
    "                val_loss_giou += outputs.loss_dict.get(\"loss_giou\", 0.0).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_loss_ce = val_loss_ce / len(val_loader)\n",
    "        avg_val_loss_bbox = val_loss_bbox / len(val_loader)\n",
    "        avg_val_loss_giou = val_loss_giou / len(val_loader)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_loss_ce\": avg_val_loss_ce,\n",
    "            \"val_loss_bbox\": avg_val_loss_bbox,\n",
    "            \"val_loss_giou\": avg_val_loss_giou,\n",
    "            \"invalid_category_count_train\": train_dataset.invalid_category_count,\n",
    "            \"invalid_category_count_val\": val_dataset.invalid_category_count\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f} (CE: {avg_train_loss_ce:.4f}, BBox: {avg_train_loss_bbox:.4f}, GIoU: {avg_train_loss_giou:.4f}), Val Loss: {avg_val_loss:.4f} (CE: {avg_val_loss_ce:.4f}, BBox: {avg_val_loss_bbox:.4f}, GIoU: {avg_val_loss_giou:.4f})\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "    processor.save_pretrained(\"yolos-tiny-finetuned\")\n",
    "\n",
    "# Run Training\n",
    "# Save ground truth annotations for evaluation\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91085e2",
   "metadata": {},
   "source": [
    "EVALUATION PART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1b8c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated gt.json: Added iscrowd=0 to 26249 annotations.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load existing gt.json\n",
    "with open(\"gt.json\", \"r\") as f:\n",
    "    gt = json.load(f)\n",
    "\n",
    "# Check if annotations exist\n",
    "if \"annotations\" not in gt or not gt[\"annotations\"]:\n",
    "    print(\"Error: No annotations found in gt.json\")\n",
    "    exit(1)\n",
    "\n",
    "# Add iscrowd=0 to all annotations\n",
    "updated_count = 0\n",
    "for ann in gt[\"annotations\"]:\n",
    "    if \"iscrowd\" not in ann:\n",
    "        ann[\"iscrowd\"] = 0\n",
    "        updated_count += 1\n",
    "\n",
    "# Save updated gt.json\n",
    "with open(\"gt.json\", \"w\") as f:\n",
    "    json.dump(gt, f)\n",
    "\n",
    "print(f\"Updated gt.json: Added iscrowd=0 to {updated_count} annotations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f68de4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\wandb\\run-20250420_145125-r3echm6i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/r3echm6i' target=\"_blank\">yolos-tiny-eval-no-threshold</a></strong> to <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/r3echm6i' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/r3echm6i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6565 images and 26249 annotations from gt.json\n",
      "Ground truth category IDs: {1, 2, 3, 4, 6, 7, 8, 10}\n",
      "Sample ground truth annotation: {'image_id': 10, 'category_id': 3, 'bbox': [650, 97, 267, 130], 'area': 34710, 'id': 21, 'iscrowd': 0}\n",
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/6565 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image frame_20190829091111_x_0000439.jpg:\n",
      "  Raw boxes (first 5): [[4.9900422e-01 8.2237434e-01 5.5540246e-01 8.6553842e-01]\n",
      " [3.5807058e-01 1.1779785e-02 4.9871388e-01 1.6985920e-01]\n",
      " [5.9637088e-02 1.4046554e-01 1.8891297e-01 4.2536828e-01]\n",
      " [3.3015913e-01 9.6900563e-04 4.6406829e-01 6.7342333e-02]\n",
      " [7.6115686e-01 9.4988990e-01 8.8177949e-01 9.9914026e-01]]\n",
      "  Scores (first 5): [0.99950814 0.99001884 0.9997652  0.9867486  0.9993869 ]\n",
      "  Labels (first 5): [91 91 91 91 91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 1/6565 [00:00<30:40,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image frame_20190829091111_x_0002548.jpg:\n",
      "  Raw boxes (first 5): [[5.9575731e-01 6.4183706e-01 6.6055983e-01 6.9977951e-01]\n",
      " [5.2310991e-01 1.7092696e-03 7.1097124e-01 1.3746108e-01]\n",
      " [6.2775530e-04 1.6733815e-01 3.8005363e-02 4.3429664e-01]\n",
      " [3.4745291e-01 5.0322211e-05 7.1502107e-01 1.2297649e-01]\n",
      " [8.0105513e-01 5.9278989e-01 9.9468517e-01 9.9999666e-01]]\n",
      "  Scores (first 5): [0.9987809  0.97448754 0.999767   0.98993415 0.99966204]\n",
      "  Labels (first 5): [91  7 91 91 91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 2/6565 [00:00<24:50,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image frame_20190829091111_x_0002441.jpg:\n",
      "  Raw boxes (first 5): [[7.9676777e-01 6.5321463e-01 8.4460920e-01 7.2880989e-01]\n",
      " [5.9239441e-01 2.7583567e-03 7.7684468e-01 1.3892907e-01]\n",
      " [5.3198780e-03 1.9549999e-01 7.6272286e-02 3.5741156e-01]\n",
      " [7.7813409e-02 2.7163458e-04 5.7412887e-01 7.9040593e-01]\n",
      " [7.9881448e-01 4.4483274e-01 9.9560744e-01 9.9959236e-01]]\n",
      "  Scores (first 5): [0.9980191  0.97573245 0.99981004 0.99369264 0.99949336]\n",
      "  Labels (first 5): [91  7 91 91 91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6565/6565 [08:17<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 42845 predictions\n",
      "Predicted category IDs: {3, 4, 6, 7, 8, 10}\n",
      "Sample prediction: {'image_id': 10, 'category_id': 7, 'bbox': [194.6468963623047, 0.0, 908.5993041992188, 85.07330322265625], 'score': 0.6986420750617981}\n",
      "Loading and preparing results...\n",
      "DONE (t=0.69s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=2.66s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.54s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.19s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.23s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.19s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.57s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.10s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.09s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.31s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.10s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "\n",
      "Per-class AP:\n",
      "person: 0.0000\n",
      "car: 0.0000\n",
      "truck: 0.0000\n",
      "train: 0.0000\n",
      "motorcycle: 0.0000\n",
      "bicycle: 0.0000\n",
      "bus: 0.0000\n",
      "traffic light: 0.0000\n",
      "Overall mAP: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AP/bicycle</td><td>▁</td></tr><tr><td>AP/bus</td><td>▁</td></tr><tr><td>AP/car</td><td>▁</td></tr><tr><td>AP/motorcycle</td><td>▁</td></tr><tr><td>AP/person</td><td>▁</td></tr><tr><td>AP/traffic light</td><td>▁</td></tr><tr><td>AP/train</td><td>▁</td></tr><tr><td>AP/truck</td><td>▁</td></tr><tr><td>mAP</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AP/bicycle</td><td>0</td></tr><tr><td>AP/bus</td><td>0</td></tr><tr><td>AP/car</td><td>0.0</td></tr><tr><td>AP/motorcycle</td><td>0</td></tr><tr><td>AP/person</td><td>0</td></tr><tr><td>AP/traffic light</td><td>0</td></tr><tr><td>AP/train</td><td>0.0</td></tr><tr><td>AP/truck</td><td>0</td></tr><tr><td>mAP</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">yolos-tiny-eval-no-threshold</strong> at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/r3echm6i' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2/runs/r3echm6i</a><br/> View project at: <a href='https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2' target=\"_blank\">https://wandb.ai/adigew-middle-east-technical-university/di725-assignment2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250420_145125-r3echm6i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Paths\n",
    "root_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\"\n",
    "annotation_path = os.path.join(root_dir, \"annotations.json\")\n",
    "img_dir = r\"C:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725_Assignment2_2030336\\data\\auair2019\\images\"\n",
    "gt_path = \"gt.json\"  # Ground truth annotations saved during training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#  Init W&B\n",
    "wandb.init(project=\"di725-assignment2\", name=\"yolos-tiny-eval-no-threshold\")\n",
    "\n",
    "#  Load model + processor\n",
    "processor = YolosImageProcessor.from_pretrained(\"yolos-tiny-finetuned\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"yolos-tiny-finetuned\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#  Category ID mapping (AUAIR to COCO)\n",
    "label_map = {0: 1, 1: 3, 2: 8, 3: 7, 4: 4, 5: 2, 6: 6, 7: 10}  # AUAIR to COCO\n",
    "coco_to_auair = {v: k for k, v in label_map.items()}  # Reverse mapping for evaluation\n",
    "\n",
    "#  Custom Dataset\n",
    "class AUAIRDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, processor, split=\"val\"):\n",
    "        self.annotations = annotations[\"annotations\"]\n",
    "        self.images = annotations[\"images\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.split = split\n",
    "        self.ann_by_image_id = {}\n",
    "        self.invalid_category_count = 0\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image_id:\n",
    "                self.ann_by_image_id[img_id] = []\n",
    "            self.ann_by_image_id[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_path = os.path.join(self.img_dir, img_info[\"file_name\"])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Get annotations for this image\n",
    "        anns = self.ann_by_image_id.get(img_id, [])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        img_width, img_height = img_info[\"width\"], img_info[\"height\"]\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            x_min, y_min = x / img_width, y / img_height\n",
    "            x_max, y_max = (x + w) / img_width, (y + h) / img_height\n",
    "            if ann[\"category_id\"] in label_map:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(label_map[ann[\"category_id\"]])\n",
    "            else:\n",
    "                self.invalid_category_count += 1\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.empty((0,), dtype=torch.long),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        encoding = self.processor(images=image, return_tensors=\"pt\")\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)\n",
    "        encoding[\"labels\"] = target\n",
    "        return encoding, image, img_id, img_info[\"file_name\"], img_width, img_height\n",
    "\n",
    "#  Load ground truth annotations\n",
    "with open(gt_path) as f:\n",
    "    val_annotations = json.load(f)\n",
    "\n",
    "# Log annotation count and category IDs\n",
    "print(f\"Loaded {len(val_annotations['images'])} images and {len(val_annotations['annotations'])} annotations from gt.json\")\n",
    "gt_category_ids = set(ann[\"category_id\"] for ann in val_annotations[\"annotations\"])\n",
    "print(f\"Ground truth category IDs: {gt_category_ids}\")\n",
    "if val_annotations[\"annotations\"]:\n",
    "    print(f\"Sample ground truth annotation: {val_annotations['annotations'][0]}\")\n",
    "\n",
    "# Verify and fix annotations\n",
    "if not val_annotations[\"annotations\"]:\n",
    "    raise ValueError(\"No annotations found in gt.json. Cannot perform evaluation.\")\n",
    "updated_count = 0\n",
    "for ann in val_annotations[\"annotations\"]:\n",
    "    if \"iscrowd\" not in ann:\n",
    "        ann[\"iscrowd\"] = 0\n",
    "        updated_count += 1\n",
    "if updated_count > 0:\n",
    "    print(f\"Warning: Added iscrowd=0 to {updated_count} annotations.\")\n",
    "    with open(\"gt_updated.json\", \"w\") as f:\n",
    "        json.dump(val_annotations, f)\n",
    "\n",
    "# Create validation dataset and DataLoader\n",
    "val_dataset = AUAIRDataset(val_annotations, img_dir, processor, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=lambda x: [xi for xi in x if xi is not None])\n",
    "\n",
    "#  Evaluation Function\n",
    "def evaluate_model(model, val_loader, processor, device):\n",
    "    coco_gt = COCO(gt_path)\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Evaluating\")):\n",
    "            encoding, image, img_id, img_name, img_width, img_height = batch[0]\n",
    "            pixel_values = encoding[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate predictions\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            boxes = outputs.pred_boxes\n",
    "\n",
    "            # Process predictions\n",
    "            scores = torch.softmax(logits, dim=-1).max(dim=-1)[0].cpu().numpy()\n",
    "            labels = logits.argmax(dim=-1).cpu().numpy()\n",
    "            boxes = boxes.cpu().numpy()  # [x_center, y_center, w, h]\n",
    "\n",
    "            # Debug: Log raw model outputs for first few batches\n",
    "            if batch_idx < 3:\n",
    "                print(f\"Image {img_name}:\")\n",
    "                print(f\"  Raw boxes (first 5): {boxes[0][:5]}\")\n",
    "                print(f\"  Scores (first 5): {scores[0][:5]}\")\n",
    "                print(f\"  Labels (first 5): {labels[0][:5]}\")\n",
    "\n",
    "            for score, label, box in zip(scores[0], labels[0], boxes[0]):\n",
    "                if label in coco_to_auair:  # No confidence threshold\n",
    "                    # Convert [x_center, y_center, w, h] to [x_min, y_min, w, h]\n",
    "                    x_center, y_center, w, h = box\n",
    "                    x_min = (x_center - w / 2) * img_width\n",
    "                    y_min = (y_center - h / 2) * img_height\n",
    "                    w *= img_width\n",
    "                    h *= img_height\n",
    "\n",
    "                    # Clamp coordinates to image bounds\n",
    "                    x_min = max(0, min(x_min, img_width))\n",
    "                    y_min = max(0, min(y_min, img_height))\n",
    "                    w = max(0, min(w, img_width - x_min))\n",
    "                    h = max(0, min(h, img_height - y_min))\n",
    "\n",
    "                    predictions.append({\n",
    "                        \"image_id\": int(img_id),\n",
    "                        \"category_id\": int(label),\n",
    "                        \"bbox\": [float(x_min), float(y_min), float(w), float(h)],\n",
    "                        \"score\": float(score),\n",
    "                    })\n",
    "\n",
    "    # Log prediction details\n",
    "    print(f\"Generated {len(predictions)} predictions\")\n",
    "    predicted_category_ids = set(pred[\"category_id\"] for pred in predictions)\n",
    "    print(f\"Predicted category IDs: {predicted_category_ids}\")\n",
    "    if predictions:\n",
    "        print(f\"Sample prediction: {predictions[0]}\")\n",
    "\n",
    "    # Save predictions to a temporary JSON file\n",
    "    pred_json_path = \"predictions.json\"\n",
    "    with open(pred_json_path, \"w\") as f:\n",
    "        json.dump(predictions, f)\n",
    "\n",
    "    # Load predictions into COCO format\n",
    "    coco_dt = coco_gt.loadRes(pred_json_path)\n",
    "\n",
    "    # Initialize COCO evaluation\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    # Extract per-class AP\n",
    "    per_class_ap = {}\n",
    "    for idx, cat_id in enumerate(coco_gt.getCatIds()):\n",
    "        coco_eval.params.catIds = [cat_id]\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "        ap = coco_eval.stats[0]  # AP at IoU=0.50:0.95\n",
    "        cat_name = model.config.id2label.get(cat_id, f\"Category {cat_id}\")\n",
    "        per_class_ap[cat_name] = ap\n",
    "        wandb.log({f\"AP/{cat_name}\": ap})\n",
    "\n",
    "    # Log overall mAP\n",
    "    overall_map = coco_eval.stats[0]\n",
    "    wandb.log({\"mAP\": overall_map})\n",
    "\n",
    "    # Print per-class AP and overall mAP\n",
    "    print(\"\\nPer-class AP:\")\n",
    "    for cat_name, ap in per_class_ap.items():\n",
    "        print(f\"{cat_name}: {ap:.4f}\")\n",
    "    print(f\"Overall mAP: {overall_map:.4f}\")\n",
    "\n",
    "    return per_class_ap, overall_map\n",
    "\n",
    "#  Run Evaluation\n",
    "per_class_ap, overall_map = evaluate_model(model, val_loader, processor, device)\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d30290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
